# üöÄ –ë–ª–æ–∫ 6: –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ Python

---

## üìã –û–±–∑–æ—Ä –±–ª–æ–∫–∞

**‚è± –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** 8-12 –Ω–µ–¥–µ–ª—å  
**üéØ –¶–µ–ª—å:** –ü–æ–ª—É—á–∏—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –æ–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è Python  
**üìä –§–æ—Ä–º–∞—Ç:** –¢–µ–æ—Ä–∏—è + –ø—Ä–∞–∫—Ç–∏–∫–∞ + —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã

---

## üéØ –í—ã–±–æ—Ä —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    –í–´–ë–ï–†–ò–¢–ï –°–í–û–ô –¢–†–ï–ö                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   üìä DATA       ‚îÇ   üõ† DEVOPS     ‚îÇ   üéÆ GAMEDEV    ‚îÇ üñ• DESKTOP‚îÇ
‚îÇ   SCIENCE       ‚îÇ                 ‚îÇ                 ‚îÇ           ‚îÇ
‚îÇ                 ‚îÇ                 ‚îÇ                 ‚îÇ           ‚îÇ
‚îÇ ‚Ä¢ ML/AI         ‚îÇ ‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è ‚îÇ ‚Ä¢ Pygame        ‚îÇ ‚Ä¢ GUI     ‚îÇ
‚îÇ ‚Ä¢ –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö ‚îÇ ‚Ä¢ –û–±–ª–∞–∫–∞        ‚îÇ ‚Ä¢ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞   ‚îÇ ‚Ä¢ –°–∏—Å—Ç–µ–º—ã ‚îÇ
‚îÇ ‚Ä¢ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è  ‚îÇ ‚Ä¢ CI/CD         ‚îÇ ‚Ä¢ AI –¥–ª—è –∏–≥—Ä    ‚îÇ ‚Ä¢ –ö—Ä–æ—Å—Å-  ‚îÇ
‚îÇ ‚Ä¢ BigData       ‚îÇ ‚Ä¢ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥    ‚îÇ ‚Ä¢ –ú—É–ª—å—Ç–∏–ø–ª–µ–µ—Ä   ‚îÇ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞ ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

# üìä DATA SCIENCE –¢–†–ï–ö

---

## üìà 6.1 –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö

### üîç Pandas: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏

#### –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã (MultiIndex)
```python
# –°–æ–∑–¥–∞–Ω–∏–µ MultiIndex
import pandas as pd
import numpy as np

# –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–º –∏–Ω–¥–µ–∫—Å–æ–º
arrays = [
    ['–ú–æ—Å–∫–≤–∞', '–ú–æ—Å–∫–≤–∞', '–°–ü–±', '–°–ü–±'],
    ['2023', '2024', '2023', '2024']
]
index = pd.MultiIndex.from_arrays(arrays, names=['–ì–æ—Ä–æ–¥', '–ì–æ–¥'])
df = pd.DataFrame({'–ü—Ä–æ–¥–∞–∂–∏': [100, 120, 80, 95]}, index=index)

# –û–ø–µ—Ä–∞—Ü–∏–∏ —Å MultiIndex
df.loc['–ú–æ—Å–∫–≤–∞']  # –í—ã–±–æ—Ä –ø–æ –ø–µ—Ä–≤–æ–º—É —É—Ä–æ–≤–Ω—é
df.xs('2023', level='–ì–æ–¥')  # –í—ã–±–æ—Ä –ø–æ –≤—Ç–æ—Ä–æ–º—É —É—Ä–æ–≤–Ω—é
```

#### –û–ø–µ—Ä–∞—Ü–∏–∏ —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π
```python
# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞
grouped = df.groupby(['region', 'product'])
result = grouped.agg({
    'sales': ['sum', 'mean', 'std'],
    'profit': 'max',
    'customers': 'count'
})

# –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –∫ –≥—Ä—É–ø–ø–∞–º
def custom_metric(group):
    return group['profit'].sum() / group['sales'].sum()

metrics = df.groupby('category').apply(custom_metric)
```

---

### üìÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤

#### –†–∞–±–æ—Ç–∞ —Å –¥–∞—Ç–∞–º–∏
```python
# –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
dates = pd.date_range('2023-01-01', periods=365, freq='D')
ts = pd.Series(np.random.randn(365), index=dates)

# –†–µ—Å–µ–º–ø–ª–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö
monthly = ts.resample('M').mean()  # –ü–æ –º–µ—Å—è—Ü–∞–º
weekly = ts.resample('W').sum()    # –ü–æ –Ω–µ–¥–µ–ª—è–º

# –°–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞
rolling_mean = ts.rolling(window=30).mean()
expanding_mean = ts.expanding().mean()
```

#### –ê–Ω–∞–ª–∏–∑ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏
```python
from statsmodels.tsa.seasonal import seasonal_decompose

# –î–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞
decomposition = seasonal_decompose(ts, model='additive', period=365)
trend = decomposition.trend
seasonal = decomposition.seasonal
residual = decomposition.resid
```

---

### üìä –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å SciPy

#### –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑
```python
from scipy import stats

# t-—Ç–µ—Å—Ç –¥–ª—è –¥–≤—É—Ö –≤—ã–±–æ—Ä–æ–∫
group1 = np.random.normal(10, 2, 100)
group2 = np.random.normal(12, 2, 100)
t_stat, p_value = stats.ttest_ind(group1, group2)

# –ö—Ä–∏—Ç–µ—Ä–∏–π –®–∞–ø–∏—Ä–æ-–£–∏–ª–∫–∞ –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏
shapiro_stat, shapiro_p = stats.shapiro(group1)

# –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
correlation, p_val = stats.pearsonr(x, y)
```

#### ANOVA –∏ –¥—Ä—É–≥–∏–µ —Ç–µ—Å—Ç—ã
```python
# –û–¥–Ω–æ—Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –¥–∏—Å–ø–µ—Ä—Å–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
f_stat, p_value = stats.f_oneway(group1, group2, group3)

# –ö—Ä–∏—Ç–µ—Ä–∏–π –ö—Ä–∞—Å–∫–µ–ª–∞-–£–æ–ª–ª–∏—Å–∞ (–Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–π)
h_stat, p_value = stats.kruskal(group1, group2, group3)

# –ö—Ä–∏—Ç–µ—Ä–∏–π —Ö–∏-–∫–≤–∞–¥—Ä–∞—Ç
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)
```

---

### üß™ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

#### –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
```python
def calculate_sample_size(p1, p2, alpha=0.05, power=0.8):
    """–†–∞—Å—á–µ—Ç —Ä–∞–∑–º–µ—Ä–∞ –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è A/B —Ç–µ—Å—Ç–∞"""
    effect_size = abs(p1 - p2)
    z_alpha = stats.norm.ppf(1 - alpha/2)
    z_beta = stats.norm.ppf(power)
    
    pooled_p = (p1 + p2) / 2
    variance = 2 * pooled_p * (1 - pooled_p)
    
    n = ((z_alpha + z_beta) ** 2 * variance) / (effect_size ** 2)
    return int(np.ceil(n))

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
sample_size = calculate_sample_size(0.10, 0.12)  # 10% vs 12% –∫–æ–Ω–≤–µ—Ä—Å–∏—è
```

#### –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
```python
def ab_test_analysis(control, treatment):
    """–ê–Ω–∞–ª–∏–∑ A/B —Ç–µ—Å—Ç–∞"""
    n_control = len(control)
    n_treatment = len(treatment)
    
    p_control = control.mean()
    p_treatment = treatment.mean()
    
    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è –ø—Ä–æ–ø–æ—Ä—Ü–∏—è
    p_pooled = (control.sum() + treatment.sum()) / (n_control + n_treatment)
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ—à–∏–±–∫–∞
    se = np.sqrt(p_pooled * (1 - p_pooled) * (1/n_control + 1/n_treatment))
    
    # Z-—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    z = (p_treatment - p_control) / se
    p_value = 2 * (1 - stats.norm.cdf(abs(z)))
    
    return {
        'conversion_control': p_control,
        'conversion_treatment': p_treatment,
        'lift': (p_treatment - p_control) / p_control,
        'z_score': z,
        'p_value': p_value,
        'significant': p_value < 0.05
    }
```

---

### üíæ –†–∞–±–æ—Ç–∞ —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (Dask)

#### –û—Å–Ω–æ–≤—ã Dask
```python
import dask.dataframe as dd

# –ß—Ç–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
df = dd.read_csv('large_file_*.csv')

# –õ–µ–Ω–∏–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
result = df.groupby('category').sales.sum()
computed_result = result.compute()  # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π

# –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
df_processed = df.map_partitions(lambda x: x[x.sales > 1000])
```

---

## ü§ñ 6.2 –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

### üîß Scikit-learn: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏—è

#### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train_scaled)
```

#### –ú–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# –û–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
probabilities = pipeline.predict_proba(X_test)
```

---

### üìä –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π

#### –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)

# –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
accuracy = accuracy_score(y_test, predictions)
precision = precision_score(y_test, predictions, average='weighted')
recall = recall_score(y_test, predictions, average='weighted')
f1 = f1_score(y_test, predictions, average='weighted')

# ROC-AUC –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
if len(np.unique(y)) == 2:
    auc = roc_auc_score(y_test, probabilities[:, 1])

# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
cm = confusion_matrix(y_test, predictions)
```

#### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
```python
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, precision_recall_curve

# ROC –∫—Ä–∏–≤–∞—è
fpr, tpr, _ = roc_curve(y_test, probabilities[:, 1])
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()
```

---

### üéõ Feature Engineering

#### –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
```python
# –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# –ë–∏–Ω–Ω–∏–Ω–≥ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
from sklearn.preprocessing import KBinsDiscretizer

discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
X_binned = discretizer.fit_transform(X[['continuous_feature']])

# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
X_encoded = encoder.fit_transform(X[['category']])
```

#### –û—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
```python
from sklearn.feature_selection import (
    SelectKBest, f_classif, RFE, SelectFromModel
)

# Univariate Feature Selection
selector = SelectKBest(score_func=f_classif, k=10)
X_selected = selector.fit_transform(X, y)

# Recursive Feature Elimination
rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=10)
X_rfe = rfe.fit_transform(X, y)

# Feature importance –æ—Ç –º–æ–¥–µ–ª–∏
sfm = SelectFromModel(RandomForestClassifier(), threshold='mean')
X_important = sfm.fit_transform(X, y)
```

---

### üîÑ –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã

#### –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
```python
from sklearn.model_selection import (
    cross_val_score, StratifiedKFold, TimeSeriesSplit
)

# –û–±—ã—á–Ω–∞—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
cv_scores = cross_val_score(
    pipeline, X, y, cv=5, scoring='accuracy'
)

# –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(pipeline, X, y, cv=skf)

# –î–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
tscv = TimeSeriesSplit(n_splits=5)
cv_scores = cross_val_score(pipeline, X, y, cv=tscv)
```

#### –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Grid Search
param_grid = {
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [3, 5, 7, None],
    'classifier__min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(
    pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1
)
grid_search.fit(X_train, y_train)

# Random Search (–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π)
from scipy.stats import randint

param_dist = {
    'classifier__n_estimators': randint(50, 500),
    'classifier__max_depth': [3, 5, 7, 10, None],
    'classifier__min_samples_split': randint(2, 20)
}

random_search = RandomizedSearchCV(
    pipeline, param_dist, n_iter=100, cv=5, n_jobs=-1
)
random_search.fit(X_train, y_train)
```

---

### üéØ –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç–æ–¥—ã

#### Voting –∏ Stacking
```python
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import StackingClassifier

# Voting Classifier
clf1 = LogisticRegression()
clf2 = RandomForestClassifier()
clf3 = SVC(probability=True)

voting_clf = VotingClassifier(
    estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)],
    voting='soft'  # 'hard' –¥–ª—è –º–∞–∂–æ—Ä–∏—Ç–∞—Ä–Ω–æ–≥–æ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è
)

# Stacking Classifier
stacking_clf = StackingClassifier(
    estimators=[('lr', clf1), ('rf', clf2), ('svc', clf3)],
    final_estimator=LogisticRegression(),
    cv=5
)
```

#### Blending
```python
def create_blend_predictions(models, X_blend, X_test):
    """–°–æ–∑–¥–∞–Ω–∏–µ blend –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    blend_train = np.zeros((X_blend.shape[0], len(models)))
    blend_test = np.zeros((X_test.shape[0], len(models)))
    
    for i, model in enumerate(models):
        blend_train[:, i] = model.predict_proba(X_blend)[:, 1]
        blend_test[:, i] = model.predict_proba(X_test)[:, 1]
    
    return blend_train, blend_test

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
blend_train, blend_test = create_blend_predictions(
    [clf1, clf2, clf3], X_blend, X_test
)

# –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å
meta_model = LogisticRegression()
meta_model.fit(blend_train, y_blend)
final_predictions = meta_model.predict(blend_test)
```

---

## üß† 6.3 –ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ

### üî• TensorFlow –∏ Keras

#### –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization

# –ü—Ä–æ—Å—Ç–∞—è –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω–∞—è —Å–µ—Ç—å
model = Sequential([
    Dense(128, activation='relu', input_shape=(input_dim,)),
    BatchNormalization(),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(num_classes, activation='softmax')
])

# –ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    batch_size=32,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=10),
        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
    ]
)
```

#### –ö–∞—Å—Ç–æ–º–Ω—ã–µ —Å–ª–æ–∏ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
```python
class CustomLayer(tf.keras.layers.Layer):
    def __init__(self, units=32):
        super(CustomLayer, self).__init__()
        self.units = units

    def build(self, input_shape):
        self.w = self.add_weight(
            shape=(input_shape[-1], self.units),
            initializer='random_normal',
            trainable=True
        )
        self.b = self.add_weight(
            shape=(self.units,),
            initializer='zeros',
            trainable=True
        )

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b

# –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
def custom_loss(y_true, y_pred):
    """Focal Loss –¥–ª—è –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    alpha = 0.25
    gamma = 2.0
    
    ce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
    p_t = tf.exp(-ce_loss)
    focal_loss = alpha * (1 - p_t) ** gamma * ce_loss
    
    return focal_loss
```

---

### üñº CNN –¥–ª—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è

#### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ CNN
```python
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten

# CNN –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
cnn_model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

# Data Augmentation
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    zoom_range=0.2
)

# –û–±—É—á–µ–Ω–∏–µ —Å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π
model.fit(
    datagen.flow(X_train, y_train, batch_size=32),
    validation_data=(X_val, y_val),
    epochs=50
)
```

#### Transfer Learning
```python
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import GlobalAveragePooling2D

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
base_model = VGG16(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# –ó–∞–º–æ—Ä–æ–∑–∫–∞ –±–∞–∑–æ–≤—ã—Ö —Å–ª–æ–µ–≤
base_model.trainable = False

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
model = Sequential([
    base_model,
    GlobalAveragePooling2D(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(num_classes, activation='softmax')
])

# Fine-tuning
base_model.trainable = True
for layer in base_model.layers[:-4]:
    layer.trainable = False

model.compile(
    optimizer=tf.keras.optimizers.Adam(lr=0.0001/10),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)
```

---

### üìù RNN –∏ LSTM –¥–ª—è —Ç–µ–∫—Å—Ç–∞

#### –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(texts)

sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences, maxlen=100, padding='post')

# –°–æ–∑–¥–∞–Ω–∏–µ word embeddings
vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 100

embedding_matrix = np.zeros((vocab_size, embedding_dim))
# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö embeddings (Word2Vec, GloVe)
```

#### LSTM –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
```python
from tensorflow.keras.layers import LSTM, Embedding, Bidirectional

# –ú–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
text_model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3)),
    Dense(32, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

# Attention –º–µ—Ö–∞–Ω–∏–∑–º
from tensorflow.keras.layers import Attention

class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self):
        super(AttentionLayer, self).__init__()

    def call(self, inputs):
        # inputs: (batch_size, time_steps, features)
        attention_weights = tf.nn.softmax(
            tf.reduce_sum(inputs, axis=-1, keepdims=True), axis=1
        )
        context_vector = tf.reduce_sum(
            inputs * attention_weights, axis=1
        )
        return context_vector
```

---

## üîß 6.4 MLOps –∏ –ø—Ä–æ–¥–∞–∫—à–Ω ML

### üì¶ –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–µ–π

#### DVC (Data Version Control)
```bash
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DVC
dvc init

# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥ –≤–µ—Ä—Å–∏–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å
dvc add data/raw_data.csv
git add data/raw_data.csv.dvc .gitignore
git commit -m "Add raw data"

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
dvc run -f prepare.dvc \
    -d data/raw_data.csv \
    -o data/processed_data.csv \
    python src/prepare.py

# –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞
dvc repro
```

#### MLflow –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
```python
import mlflow
import mlflow.sklearn

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
with mlflow.start_run():
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    mlflow.log_param("n_estimators", 100)
    mlflow.log_param("max_depth", 5)
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = RandomForestClassifier(n_estimators=100, max_depth=5)
    model.fit(X_train, y_train)
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    accuracy = model.score(X_test, y_test)
    mlflow.log_metric("accuracy", accuracy)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    mlflow.sklearn.log_model(model, "model")
```

---

### üê≥ Docker –¥–ª—è ML –ø—Ä–æ–µ–∫—Ç–æ–≤

#### Dockerfile –¥–ª—è ML
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞
COPY src/ ./src/
COPY models/ ./models/

# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
RUN useradd -m mluser
USER mluser

EXPOSE 8000

CMD ["python", "src/api.py"]
```

#### Docker Compose –¥–ª—è ML —Å—Ç–µ–∫–∞
```yaml
version: '3.8'

services:
  ml-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=/app/models/model.pkl
    volumes:
      - ./models:/app/models
    depends_on:
      - redis
      - postgres

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"

  postgres:
    image: postgres:13
    environment:
      POSTGRES_DB: mldb
      POSTGRES_USER: mluser
      POSTGRES_PASSWORD: mlpass
    volumes:
      - postgres_data:/var/lib/postgresql/data

volumes:
  postgres_data:
```

---

### üöÄ API –¥–ª—è ML –º–æ–¥–µ–ª–µ–π

#### FastAPI –¥–ª—è ML —Å–µ—Ä–≤–∏—Å–∞
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
import numpy as np

app = FastAPI(title="ML Model API", version="1.0.0")

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
model = joblib.load("models/trained_model.pkl")
scaler = joblib.load("models/scaler.pkl")

class PredictionRequest(BaseModel):
    features: list[float]

class PredictionResponse(BaseModel):
    prediction: float
    probability: float

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    try:
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        features = np.array(request.features).reshape(1, -1)
        features_scaled = scaler.transform(features)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        prediction = model.predict(features_scaled)[0]
        probability = model.predict_proba(features_scaled)[0].max()
        
        return PredictionResponse(
            prediction=float(prediction),
            probability=float(probability)
        )
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

@app.get("/health")
async def health_check():
    return {"status": "healthy"}

@app.get("/model/info")
async def model_info():
    return {
        "model_type": type(model).__name__,
        "features": model.n_features_in_
    }
```

---

### üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π

#### –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥—Ä–∏—Ñ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö
```python
import numpy as np
from scipy import stats
from typing import Dict, Any

class DataDriftDetector:
    def __init__(self, reference_data: np.ndarray):
        self.reference_data = reference_data
        self.reference_stats = self._calculate_stats(reference_data)
    
    def _calculate_stats(self, data: np.ndarray) -> Dict[str, Any]:
        return {
            'mean': np.mean(data, axis=0),
            'std': np.std(data, axis=0),
            'min': np.min(data, axis=0),
            'max': np.max(data, axis=0)
        }
    
    def detect_drift(self, new_data: np.ndarray, threshold: float = 0.05) -> Dict[str, bool]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –¥—Ä–∏—Ñ—Ç–∞ —Å –ø–æ–º–æ—â—å—é KS-—Ç–µ—Å—Ç–∞"""
        drift_detected = {}
        
        for i in range(new_data.shape[1]):
            ks_statistic, p_value = stats.ks_2samp(
                self.reference_data[:, i], 
                new_data[:, i]
            )
            drift_detected[f'feature_{i}'] = p_value < threshold
            
        return drift_detected
    
    def performance_degradation(self, predictions: np.ndarray, 
                              actuals: np.ndarray) -> float:
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        from sklearn.metrics import accuracy_score
        return accuracy_score(actuals, predictions)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
detector = DataDriftDetector(X_train)
drift_results = detector.detect_drift(X_new)
```

---

### üîÑ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

#### –°–∏—Å—Ç–µ–º–∞ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
```python
import hashlib
import random
from typing import Optional

class ModelABTester:
    def __init__(self, model_a, model_b, traffic_split: float = 0.5):
        self.model_a = model_a
        self.model_b = model_b
        self.traffic_split = traffic_split
        self.results = {'a': [], 'b': []}
    
    def get_model_assignment(self, user_id: str) -> str:
        """–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
        return 'a' if (hash_value % 100) < (self.traffic_split * 100) else 'b'
    
    def predict(self, user_id: str, features: np.ndarray):
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å —É—á–µ—Ç–æ–º A/B —Ç–µ—Å—Ç–∞"""
        assignment = self.get_model_assignment(user_id)
        
        if assignment == 'a':
            prediction = self.model_a.predict(features)
            self.results['a'].append({
                'user_id': user_id,
                'prediction': prediction,
                'timestamp': pd.Timestamp.now()
            })
        else:
            prediction = self.model_b.predict(features)
            self.results['b'].append({
                'user_id': user_id,
                'prediction': prediction,
                'timestamp': pd.Timestamp.now()
            })
        
        return prediction, assignment
    
    def analyze_results(self):
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ A/B —Ç–µ—Å—Ç–∞"""
        metrics_a = self._calculate_metrics(self.results['a'])
        metrics_b = self._calculate_metrics(self.results['b'])
        
        return {
            'model_a': metrics_a,
            'model_b': metrics_b,
            'statistical_significance': self._statistical_test(
                self.results['a'], self.results['b']
            )
        }
```

---

## üéØ –ò—Ç–æ–≥–æ–≤—ã–µ –ø—Ä–æ–µ–∫—Ç—ã Data Science —Ç—Ä–µ–∫–∞

### 1. üè≠ –ü–æ–ª–Ω—ã–π ML –ø–∞–π–ø–ª–∞–π–Ω: –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—Ç–æ–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ü–†–û–ï–ö–¢–ê                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   üìä DATA       ‚îÇ   ü§ñ MODEL      ‚îÇ   üöÄ DEPLOY     ‚îÇ üìà MONITOR‚îÇ
‚îÇ   PIPELINE      ‚îÇ   TRAINING      ‚îÇ                 ‚îÇ           ‚îÇ
‚îÇ                 ‚îÇ                 ‚îÇ                 ‚îÇ           ‚îÇ
‚îÇ ‚Ä¢ ETL –ø—Ä–æ—Ü–µ—Å—Å—ã  ‚îÇ ‚Ä¢ Feature Eng   ‚îÇ ‚Ä¢ FastAPI       ‚îÇ ‚Ä¢ Drift   ‚îÇ
‚îÇ ‚Ä¢ –í–∞–ª–∏–¥–∞—Ü–∏—è     ‚îÇ ‚Ä¢ AutoML        ‚îÇ ‚Ä¢ Docker        ‚îÇ ‚Ä¢ A/B     ‚îÇ
‚îÇ ‚Ä¢ –í–µ—Ä—Å–∏–æ–Ω–∏—Ä.    ‚îÇ ‚Ä¢ –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º.   ‚îÇ ‚Ä¢ Kubernetes    ‚îÇ ‚Ä¢ Alerts  ‚îÇ
‚îÇ ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ      ‚îÇ ‚Ä¢ –ê–Ω—Å–∞–º–±–ª–∏     ‚îÇ ‚Ä¢ CI/CD         ‚îÇ ‚Ä¢ –î–∞—à–±–æ—Ä–¥ ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
```
churn_prediction/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îî‚îÄ‚îÄ external/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extract.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transform.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validate.py
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ predict.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/
‚îÇ       ‚îú‚îÄ‚îÄ drift.py
‚îÇ       ‚îî‚îÄ‚îÄ performance.py
‚îú‚îÄ‚îÄ tests/
‚îú‚îÄ‚îÄ configs/
‚îú‚îÄ‚îÄ notebooks/
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ dvc.yaml
```

---

# üéØ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ –±–ª–æ–∫–∞

## üìä –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    –ù–ê–í–´–ö–ò –≠–ö–°–ü–ï–†–¢–ê                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   üß† TECHNICAL  ‚îÇ   üèó ARCHITECTURE‚îÇ   üë• SOFT      ‚îÇ üöÄ BUSINESS‚îÇ
‚îÇ                 ‚îÇ                 ‚îÇ                 ‚îÇ           ‚îÇ
‚îÇ ‚Ä¢ –ê–ª–≥–æ—Ä–∏—Ç–º—ã     ‚îÇ ‚Ä¢ –°–∏—Å—Ç–µ–º–Ω—ã–π     ‚îÇ ‚Ä¢ –ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è ‚îÇ ‚Ä¢ ROI     ‚îÇ
‚îÇ ‚Ä¢ –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞    ‚îÇ   –¥–∏–∑–∞–π–Ω        ‚îÇ ‚Ä¢ –ú–µ–Ω—Ç–æ—Ä—Å—Ç–≤–æ    ‚îÇ ‚Ä¢ –ü—Ä–æ–¥—É–∫—Ç ‚îÇ
‚îÇ ‚Ä¢ –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä.   ‚îÇ ‚Ä¢ –ú–∞—Å—à—Ç–∞–±–∏—Ä.    ‚îÇ ‚Ä¢ –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏  ‚îÇ ‚Ä¢ –°—Ç—Ä–∞—Ç–µ–≥–∏—è‚îÇ
‚îÇ ‚Ä¢ –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã   ‚îÇ ‚Ä¢ –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å    ‚îÇ ‚Ä¢ –¢–∏–º–≤–æ—Ä–∫       ‚îÇ ‚Ä¢ –ú–µ—Ç—Ä–∏–∫–∏ ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìö –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã

### –ö–Ω–∏–≥–∏
- **"Hands-On Machine Learning"** - Aur√©lien G√©ron
- **"The Elements of Statistical Learning"** - Hastie, Tibshirani, Friedman
- **"Deep Learning"** - Ian Goodfellow
- **"Building Machine Learning Pipelines"** - Hannes Hapke

### –ö—É—Ä—Å—ã –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
- **Kaggle Learn** - –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–∏–∫—Ä–æ–∫—É—Ä—Å—ã
- **Fast.ai** - –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- **Coursera ML Specializations** - Andrew Ng
- **Papers With Code** - –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è

### –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
- **MLflow** - —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º–∏
- **DVC** - –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
- **Weights & Biases** - —Ç—Ä–µ–∫–∏–Ω–≥ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤
- **Optuna** - –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

---

## üéØ Roadmap –∫–∞—Ä—å–µ—Ä–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞

```
Junior DS (0-2 –≥–æ–¥–∞)     ‚Üí  Middle DS (2-4 –≥–æ–¥–∞)     ‚Üí  Senior DS (4+ –ª–µ—Ç)
‚îú‚îÄ –û—Å–Ω–æ–≤—ã ML             ‚îú‚îÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã   ‚îú‚îÄ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º
‚îú‚îÄ Python/SQL            ‚îú‚îÄ Feature Engineering     ‚îú‚îÄ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –ª–∏–¥–µ—Ä—Å—Ç–≤–æ
‚îú‚îÄ Pandas/Scikit-learn   ‚îú‚îÄ Deep Learning          ‚îú‚îÄ –ë–∏–∑–Ω–µ—Å-—Å—Ç—Ä–∞—Ç–µ–≥–∏—è
‚îî‚îÄ Jupyter notebooks     ‚îú‚îÄ MLOps                  ‚îú‚îÄ –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
                        ‚îî‚îÄ –ü—Ä–æ–¥—É–∫—Ç–æ–≤–æ–µ –º—ã—à–ª–µ–Ω–∏–µ    ‚îî‚îÄ –ú–µ–Ω—Ç–æ—Ä—Å—Ç–≤–æ –∫–æ–º–∞–Ω–¥—ã
```

---

*–°–ª–µ–¥—É—é—â–∏–π –º–∞—Ç–µ—Ä–∏–∞–ª: –¥—Ä—É–≥–∏–µ —Ç—Ä–µ–∫–∏ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ (DevOps, GameDev, Desktop) üöÄ*