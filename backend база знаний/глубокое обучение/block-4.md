# 🔄 Блок 4: Рекуррентные сети и обработка последовательностей

---

**⏱ Длительность:** 5-7 недель  
**🎯 Цель:** Изучить RNN, LSTM, GRU и их применение для работы с временными рядами и текстом  
**📊 Уровень сложности:** ⭐⭐⭐⭐☆

---

## 📋 Содержание блока

```
📖 Глава 4.1: Vanilla RNN ────────────────────── 1 неделя
📖 Глава 4.2: LSTM и GRU ───────────────────── 1-2 недели  
📖 Глава 4.3: Bidirectional RNN и Encoder-Decoder ─── 1 неделя
📖 Глава 4.4: Механизм внимания ──────────────── 1 неделя
📖 Глава 4.5: Transformer архитектура ───────── 1-2 недели
📖 Глава 4.6: Обработка естественного языка ──── 1 неделя
```

---

## 🔍 Глава 4.1: Vanilla RNN
**⏱ Длительность:** 1 неделя

### 🎯 Цели главы:
- Понять концепцию рекуррентных связей
- Изучить архитектуру простых RNN
- Освоить проблемы затухающих/взрывающихся градиентов

### 📊 Концепция рекуррентных связей

```
Традиционная нейронная сеть:
Input → Hidden → Output

RNN:
Input ──→ Hidden ──→ Output
         ↙     ↘
    Previous    Next
     State     State
```

### 🔧 Архитектура Vanilla RNN

```
┌─────────────────────────────────────────────────────┐
│                  RNN CELL                           │
│                                                     │
│  h(t-1) ──→ [●] ──→ h(t) ──→ [●] ──→ h(t+1)        │
│             ↑               ↑                       │
│           x(t-1)          x(t)                      │
│             │               │                       │
│             ↓               ↓                       │
│           y(t-1)          y(t)                      │
└─────────────────────────────────────────────────────┘
```

### 📐 Математические основы

**Формула RNN:**
```
h(t) = tanh(W_hh * h(t-1) + W_xh * x(t) + b_h)
y(t) = W_hy * h(t) + b_y
```

**Где:**
- `h(t)` - скрытое состояние в момент времени t
- `x(t)` - входные данные в момент времени t
- `y(t)` - выход в момент времени t
- `W_hh`, `W_xh`, `W_hy` - весовые матрицы
- `b_h`, `b_y` - векторы смещения

### ⚠️ Проблемы Vanilla RNN

```
────────────────────────────────────────────────────────
                ПРОБЛЕМЫ RNN
────────────────────────────────────────────────────────

1. ЗАТУХАЮЩИЕ ГРАДИЕНТЫ
   Градиент ────→ ○ ────→ ○ ────→ ○ ────→ ○
   Сильный      Слабый  Очень   Почти    Ноль
                        слабый   ноль

2. ВЗРЫВАЮЩИЕСЯ ГРАДИЕНТЫ  
   Градиент ────→ ○ ────→ ● ────→ ●● ────→ ●●●
   Малый        Норма   Большой  Огромный  Infinity

3. КРАТКОВРЕМЕННАЯ ПАМЯТЬ
   Информация с начала последовательности теряется
   
────────────────────────────────────────────────────────
```

### 💻 Практическое задание

**Задача:** Реализация простой RNN для предсказания временных рядов

```python
# Пример структуры кода
class VanillaRNN:
    def __init__(self, input_size, hidden_size, output_size):
        # Инициализация весов
        pass
    
    def forward(self, input_sequence):
        # Прямой проход
        pass
    
    def backward(self, loss):
        # Обратное распространение
        pass
```

### 📈 Визуализация развертывания RNN

```
Временная последовательность:
t=0    t=1    t=2    t=3    t=4
│      │      │      │      │
▼      ▼      ▼      ▼      ▼
┌─┐    ┌─┐    ┌─┐    ┌─┐    ┌─┐
│x│    │x│    │x│    │x│    │x│
└─┘    └─┘    └─┘    └─┘    └─┘
│      │      │      │      │
▼      ▼      ▼      ▼      ▼
┌─┐ ──→┌─┐ ──→┌─┐ ──→┌─┐ ──→┌─┐
│h│    │h│    │h│    │h│    │h│
└─┘    └─┘    └─┘    └─┘    └─┘
│      │      │      │      │
▼      ▼      ▼      ▼      ▼
┌─┐    ┌─┐    ┌─┐    ┌─┐    ┌─┐
│y│    │y│    │y│    │y│    │y│
└─┘    └─┘    └─┘    └─┘    └─┘
```

---

## 🔒 Глава 4.2: LSTM и GRU
**⏱ Длительность:** 1-2 недели

### 🎯 Цели главы:
- Изучить архитектуру LSTM и принцип работы гейтов
- Понять GRU как упрощенную версию LSTM
- Сравнить производительность разных архитектур

### 🚪 Архитектура LSTM

```
┌─────────────────────────────────────────────────────┐
│                  LSTM CELL                          │
│                                                     │
│    C(t-1) ──────────────────────────────────→ C(t)  │
│      │                                    ↗    │    │
│      │           ┌─────────────────────┐ ╱      │    │
│      │           │   FORGET GATE       │╱       │    │
│      │           │   f(t) = σ(...)    │        │    │
│      │           └─────────────────────┘        │    │
│      │                     │                   │    │
│      ↓                     ↓                   │    │
│    ┌─────┐               ┌─────┐               │    │
│    │  ×  │               │  ×  │               │    │
│    └─────┘               └─────┘               │    │
│      │                     ↑                   │    │
│      │           ┌─────────────────────┐       │    │
│      │           │   INPUT GATE        │       │    │
│      │           │   i(t) = σ(...)    │       │    │
│      │           └─────────────────────┘       │    │
│      │                     │                   │    │
│      │           ┌─────────────────────┐       │    │
│      │           │   CANDIDATE VALUES  │       │    │
│      │           │   g(t) = tanh(...)  │       │    │
│      │           └─────────────────────┘       │    │
│      │                     │                   │    │
│      ↓                     ↓                   ↓    │
│    ┌─────────────────────────────────────────────┐  │
│    │                  +                          │  │
│    └─────────────────────────────────────────────┘  │
│                        │                            │
│                        ↓                            │
│              ┌─────────────────────┐                │
│              │   OUTPUT GATE       │                │
│              │   o(t) = σ(...)    │                │
│              └─────────────────────┘                │
│                        │                            │
│                        ↓                            │
│                    ┌─────┐                          │
│                    │  ×  │                          │
│                    └─────┘                          │
│                        │                            │
│                        ↓                            │
│                      h(t) ──────────────────────────┘
└─────────────────────────────────────────────────────┘
```

### 🔑 Гейты LSTM

```
────────────────────────────────────────────────────────
                    ГЕЙТЫ LSTM
────────────────────────────────────────────────────────

🚪 FORGET GATE (Гейт забывания)
   f(t) = σ(W_f · [h(t-1), x(t)] + b_f)
   ├─ Определяет, какую информацию забыть
   └─ Выход: [0, 1] для каждого элемента

🚪 INPUT GATE (Входной гейт)
   i(t) = σ(W_i · [h(t-1), x(t)] + b_i)
   ├─ Определяет, какую новую информацию запомнить
   └─ Работает в паре с candidate values

🚪 CANDIDATE VALUES (Кандидаты)
   g(t) = tanh(W_g · [h(t-1), x(t)] + b_g)
   ├─ Новая информация для добавления
   └─ Значения в диапазоне [-1, 1]

🚪 OUTPUT GATE (Выходной гейт)
   o(t) = σ(W_o · [h(t-1), x(t)] + b_o)
   ├─ Определяет, какую часть состояния выводить
   └─ Контролирует финальный выход

────────────────────────────────────────────────────────
```

### 🔄 Архитектура GRU

```
┌─────────────────────────────────────────────────────┐
│                   GRU CELL                          │
│                                                     │
│   h(t-1) ──────────────────────────────────→ h(t)   │
│     │                                    ↗     │    │
│     │                                   ╱      │    │
│     │         ┌─────────────────────┐  ╱       │    │
│     │         │   RESET GATE        │ ╱        │    │
│     │         │   r(t) = σ(...)    │╱         │    │
│     │         └─────────────────────┘          │    │
│     │                   │                      │    │
│     ↓                   ↓                      │    │
│   ┌─────┐             ┌─────┐                  │    │
│   │  ×  │             │  ×  │                  │    │
│   └─────┘             └─────┘                  │    │
│     │                   │                      │    │
│     │         ┌─────────────────────┐          │    │
│     │         │   UPDATE GATE       │          │    │
│     │         │   z(t) = σ(...)    │          │    │
│     │         └─────────────────────┘          │    │
│     │                   │                      │    │
│     │                   ↓                      │    │
│     │         ┌─────────────────────┐          │    │
│     │         │   CANDIDATE         │          │    │
│     │         │   h̃(t) = tanh(...)  │          │    │
│     │         └─────────────────────┘          │    │
│     │                   │                      │    │
│     ↓                   ↓                      ↓    │
│   ┌─────────────────────────────────────────────┐   │
│   │            (1-z) * h(t-1) + z * h̃(t)      │   │
│   └─────────────────────────────────────────────┘   │
│                         │                           │
│                         ↓                           │
│                       h(t) ─────────────────────────┘
└─────────────────────────────────────────────────────┘
```

### 📊 Сравнение архитектур

```
┌─────────────┬─────────────┬─────────────┬─────────────┐
│   Модель    │ Параметры   │ Сложность   │ Память      │
├─────────────┼─────────────┼─────────────┼─────────────┤
│ Vanilla RNN │     3n²     │   Низкая    │   Низкая    │
├─────────────┼─────────────┼─────────────┼─────────────┤
│    LSTM     │    12n²     │   Высокая   │   Высокая   │
├─────────────┼─────────────┼─────────────┼─────────────┤
│     GRU     │     9n²     │   Средняя   │   Средняя   │
└─────────────┴─────────────┴─────────────┴─────────────┘

где n - размер скрытого состояния
```

### 💻 Практическое задание

**Задача:** Генерация текста с помощью LSTM

```python
# Пример архитектуры
class TextLSTM:
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        self.embedding = EmbeddingLayer(vocab_size, embedding_dim)
        self.lstm = LSTMLayer(embedding_dim, hidden_dim)
        self.output = LinearLayer(hidden_dim, vocab_size)
    
    def generate_text(self, seed_text, length=100):
        # Генерация текста
        pass
```

### 📈 Эффективность работы с градиентами

```
                 ГРАДИЕНТНЫЙ ПОТОК
                 
RNN:     ○ ──→ ○ ──→ ○ ──→ ○ ──→ ○
         ↓     ↓     ↓     ↓     ↓
        💥    💥    💥    💥    💥
        
LSTM:    ● ══→ ● ══→ ● ══→ ● ══→ ●
         ↓     ↓     ↓     ↓     ↓
        ✅    ✅    ✅    ✅    ✅
        
GRU:     ◆ ──→ ◆ ──→ ◆ ──→ ◆ ──→ ◆
         ↓     ↓     ↓     ↓     ↓
        ✅    ✅    ✅    ✅    ✅

Легенда:
○ - RNN (проблемы с градиентами)
● - LSTM (стабильный градиентный поток)
◆ - GRU (хороший баланс)
💥 - Градиент затухает/взрывается
✅ - Стабильные градиенты
```

---

## 🔄 Глава 4.3: Bidirectional RNN и Encoder-Decoder
**⏱ Длительность:** 1 неделя

### 🎯 Цели главы:
- Изучить двунаправленные RNN
- Понять архитектуру encoder-decoder
- Освоить sequence-to-sequence модели

### ↔️ Bidirectional RNN

```
     ПРЯМОЕ НАПРАВЛЕНИЕ
     ──────────────────→
┌─┐    ┌─┐    ┌─┐    ┌─┐    ┌─┐
│h│ ──→│h│ ──→│h│ ──→│h│ ──→│h│
└─┘    └─┘    └─┘    └─┘    └─┘
│      │      │      │      │
▼      ▼      ▼      ▼      ▼
┌─┐    ┌─┐    ┌─┐    ┌─┐    ┌─┐
│x│    │x│    │x│    │x│    │x│
└─┘    └─┘    └─┘    └─┘    └─┘
│      │      │      │      │
▲      ▲      ▲      ▲      ▲
┌─┐    ┌─┐    ┌─┐    ┌─┐    ┌─┐
│h│ ←──│h│ ←──│h│ ←──│h│ ←──│h│
└─┘    └─┘    └─┘    └─┘    └─┘
     ←──────────────────
     ОБРАТНОЕ НАПРАВЛЕНИЕ
```

### 🔄 Архитектура Encoder-Decoder

```
┌─────────────────────────────────────────────────────┐
│                    ENCODER                          │
│                                                     │
│  "Hello" → "world" → "!" → <EOS>                   │
│     ↓        ↓       ↓      ↓                       │
│   ┌─┐      ┌─┐     ┌─┐    ┌─┐                      │
│   │E│ ──→  │E│ ──→ │E│ ──→│E│                      │
│   └─┘      └─┘     └─┘    └─┘                      │
│                             │                       │
│                          Context                    │
│                          Vector                     │
│                             ↓                       │
└─────────────────────────────────────────────────────┘
                              │
                              ↓
┌─────────────────────────────────────────────────────┐
│                    DECODER                          │
│                                                     │
│                           ┌─┐                      │
│                       ┌── │D│ ──→ "Привет"          │
│                       │   └─┘                      │
│                       │   ┌─┐                      │
│                       ├── │D│ ──→ "мир"             │
│                       │   └─┘                      │
│                       │   ┌─┐                      │
│                       ├── │D│ ──→ "!"               │
│                       │   └─┘                      │
│                       │   ┌─┐                      │
│                       └── │D│ ──→ <EOS>             │
│                           └─┘                      │
└─────────────────────────────────────────────────────┘
```

### 🔄 Sequence-to-Sequence модели

```
ТИПЫ ПОСЛЕДОВАТЕЛЬНОСТЕЙ:

1. ONE-TO-ONE (Обычная классификация)
   Input: x ──→ Network ──→ Output: y

2. ONE-TO-MANY (Генерация текста)
   Input: x ──→ Network ──→ Output: y₁, y₂, y₃, ...

3. MANY-TO-ONE (Анализ тональности)
   Input: x₁, x₂, x₃, ... ──→ Network ──→ Output: y

4. MANY-TO-MANY (Машинный перевод)
   Input: x₁, x₂, x₃, ... ──→ Network ──→ Output: y₁, y₂, y₃, ...

────────────────────────────────────────────────────────
```

### 🎯 Применения Seq2Seq

```
┌─────────────────────────────────────────────────────┐
│                ПРИМЕНЕНИЯ SEQ2SEQ                   │
├─────────────────────────────────────────────────────┤
│                                                     │
│ 🌐 МАШИННЫЙ ПЕРЕВОД                                │
│    EN: "Hello world"  →  RU: "Привет мир"          │
│                                                     │
│ 💬 ЧАТБОТЫ                                         │
│    Q: "Как дела?"  →  A: "Отлично, спасибо!"       │
│                                                     │
│ 📝 СУММАРИЗАЦИЯ                                    │
│    Long text  →  Short summary                      │
│                                                     │
│ 🔍 ПОИСК ОТВЕТОВ                                   │
│    Question  →  Answer                              │
│                                                     │
│ 🎨 ГЕНЕРАЦИЯ КОДА                                  │
│    Description  →  Code                             │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 💻 Практическое задание

**Задача:** Создание простого переводчика

```python
class Seq2SeqTranslator:
    def __init__(self, vocab_size_src, vocab_size_tgt, hidden_size):
        self.encoder = EncoderRNN(vocab_size_src, hidden_size)
        self.decoder = DecoderRNN(vocab_size_tgt, hidden_size)
    
    def translate(self, source_sentence):
        # Encoding
        context = self.encoder(source_sentence)
        
        # Decoding
        translation = self.decoder(context)
        
        return translation
```

### 📊 Проблемы базовой архитектуры

```
ПРОБЛЕМЫ ENCODER-DECODER:

1. ИНФОРМАЦИОННОЕ УЗКОЕ МЕСТО
   Long sentence ──→ [Fixed Vector] ──→ Long translation
                     ↑
                   Bottleneck!

2. ПОТЕРЯ ИНФОРМАЦИИ
   Начало ──→ Середина ──→ Конец
     ↓         ↓         ↓
   Забыто   Частично   Помню
            помню

3. СЛОЖНОСТЬ ОБУЧЕНИЯ
   Gradient ──→ Encoder ──→ Decoder
             ↓
        Затухает в длинных последовательностях
```

---

## 👁️ Глава 4.4: Механизм внимания
**⏱ Длительность:** 1 неделя

### 🎯 Цели главы:
- Понять проблему bottleneck в seq2seq
- Изучить механизм внимания Bahdanau и Luong
- Освоить Self-attention

### 🔍 Концепция внимания

```
БЕЗ ВНИМАНИЯ:
Encoder ──→ [Context] ──→ Decoder
           (Одно значение)

С ВНИМАНИЕМ:
Encoder ──→ [h₁, h₂, h₃, h₄] ──→ Decoder
            ↑    ↑    ↑    ↑
            │    │    │    │
          Внимание к каждому состоянию
```

### 🎯 Архитектура внимания

```
┌─────────────────────────────────────────────────────┐
│                 МЕХАНИЗМ ВНИМАНИЯ                   │
│                                                     │
│   ENCODER STATES                                    │
│   ┌─┐   ┌─┐   ┌─┐   ┌─┐   ┌─┐                     │
│   │h₁│  │h₂│  │h₃│  │h₄│  │h₅│                     │
│   └─┘   └─┘   └─┘   └─┘   └─┘                     │
│    ↑     ↑     ↑     ↑     ↑                       │
│    │     │     │     │     │                       │
│  ┌─────────────────────────────┐                   │
│  │      ATTENTION WEIGHTS      │                   │
│  │    α₁   α₂   α₃   α₄   α₅   │                   │
│  │   0.1  0.05  0.7  0.1  0.05 │                   │
│  └─────────────────────────────┘                   │
│              │                                     │
│              ↓                                     │
│    ┌─────────────────────┐                         │
│    │   CONTEXT VECTOR    │                         │
│    │   c = Σ αᵢ × hᵢ    │                         │
│    └─────────────────────┘                         │
│              │                                     │
│              ↓                                     │
│         ┌─────────┐                                │
│         │ DECODER │                                │
│         │  STATE  │                                │
│         └─────────┘                                │
└─────────────────────────────────────────────────────┘
```

### 🔢 Математика внимания

```
ВЫЧИСЛЕНИЕ ВНИМАНИЯ:

1. ЭНЕРГИЯ (Energy)
   eᵢⱼ = a(sᵢ₋₁, hⱼ)
   
2. ВЕСА ВНИМАНИЯ (Attention weights)
   αᵢⱼ = exp(eᵢⱼ) / Σₖ exp(eᵢₖ)
   
3. КОНТЕКСТНЫЙ ВЕКТОР (Context vector)
   cᵢ = Σⱼ αᵢⱼ × hⱼ
   
4. ВЫХОД ДЕКОДЕРА (Decoder output)
   sᵢ = f(sᵢ₋₁, yᵢ₋₁, cᵢ)
```

### 🔄 Типы внимания

```
┌─────────────────────────────────────────────────────┐
│                  ТИПЫ ВНИМАНИЯ                      │
├─────────────────────────────────────────────────────┤
│                                                     │
│ 🎯 BAHDANAU ATTENTION (Additive)                   │
│    e = v^T tanh(W₁h + W₂s)                         │
│    └─ Используется в seq2seq моделях                │
│                                                     │
│ 🎯 LUONG ATTENTION (Multiplicative)                │
│    e = h^T W s                                      │
│    └─ Более эффективен вычислительно                │
│                                                     │
│ 🎯 SELF-ATTENTION                                   │
│    Внимание к самой себе                            │
│    └─ Основа для Transformer                        │
│                                                     │
│ 🎯 MULTI-HEAD ATTENTION                            │
│    Несколько типов внимания параллельно             │
│    └─ Позволяет фокусироваться на разных аспектах   │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 🔍 Self-Attention

```
SELF-ATTENTION MECHANISM:

Input: X = [x₁, x₂, x₃, x₄]
       │   │   │   │
       ↓   ↓   ↓   ↓
Query: Q = [q₁, q₂, q₃, q₄]
Key:   K = [k₁, k₂, k₃, k₄]
Value: V = [v₁, v₂, v₃, v₄]

Attention(Q, K, V) = softmax(QK^T / √d_k)V

┌─────────────────────────────────────────────────────┐
│     Q₁ × K₁  Q₁ × K₂  Q₁ × K₃  Q₁ × K₄            │
│     Q₂ × K₁  Q₂ × K₂  Q₂ × K₃  Q₂ × K₄            │
│     Q₃ × K₁  Q₃ × K₂  Q₃ × K₃  Q₃ × K₄            │
│     Q₄ × K₁  Q₄ × K₂  Q₄ × K₃  Q₄ × K₄            │
└─────────────────────────────────────────────────────┘
                    │
                    ↓ (softmax)
┌─────────────────────────────────────────────────────┐
│     α₁₁     α₁₂     α₁₃     α₁₄                     │
│     α₂₁     α₂₂     α₂₃     α₂₄                     │
│     α₃₁     α₃₂     α₃₃     α₃₄                     │
│     α₄₁     α₄₂     α₄₃     α₄₄                     │
└─────────────────────────────────────────────────────┘
```

### 💻 Практическое задание

**Задача:** Добавление attention к переводчику

```python
class AttentionTranslator:
    def __init__(self, vocab_size_src, vocab_size_tgt, hidden_size):
        self.encoder = EncoderRNN(vocab_size_src, hidden_size)
        self.attention = AttentionLayer(hidden_size)
        self.decoder = DecoderRNN(vocab_size_tgt, hidden_size)
    
    def translate_with_attention(self, source_sentence):
        # Encoding
        encoder_outputs = self.encoder(source_sentence)
        
        # Decoding with attention
        translation = []
        decoder_hidden = encoder_outputs[-1]
        
        for step in range(max_length):
            # Attention
            context = self.attention(decoder_hidden, encoder_outputs)
            
            # Decoding step
            output, decoder_hidden = self.decoder(context, decoder_hidden)
            translation.append(output)
            
        return translation
```

### 📊 Визуализация внимания

```
ПРИМЕР ВНИМАНИЯ ПРИ ПЕРЕВОДЕ:

EN: "The  cat  sat  on  the  mat"
RU: "Кот  сидел  на  коврике"

    The  cat  sat  on  the  mat
Кот  ▓▓▓ ███  ░░░ ░░░  ░░░  ░░░
сидел░░░ ░░░  ███ ░░░  ░░░  ░░░
на   ░░░ ░░░  ░░░ ███  ░░░  ░░░
коврике ░░░ ░░░  ░░░ ░░░  ▓▓▓ ███

Легенда:
███ - Сильное внимание
▓▓▓ - Среднее внимание
░░░ - Слабое внимание
```

---

## 🤖 Глава 4.5: Transformer архитектура
**⏱ Длительность:** 1-2 недели

### 🎯 Цели главы:
- Изучить архитектуру "Attention is All You Need"
- Понять Multi-head attention
- Освоить Positional encoding
- Изучить семейства BERT и GPT

### 🏗️ Архитектура Transformer

```
┌─────────────────────────────────────────────────────┐
│                   TRANSFORMER                       │
│                                                     │
│  INPUT EMBEDDINGS + POSITIONAL ENCODING             │
│            ↓                                        │
│  ┌─────────────────────────────────────────────┐    │
│  │              ENCODER                        │    │
│  │  ┌─────────────────────────────────────┐    │    │
│  │  │        MULTI-HEAD ATTENTION         │    │    │
│  │  └─────────────────────────────────────┘    │    │
│  │                    ↓                        │    │
│  │  ┌─────────────────────────────────────┐    │    │
│  │  │           ADD & NORM               │    │    │
│  │  └─────────────────────────────────────┘    │    │
│  │                    ↓                        │    │
│  │  ┌─────────────────────────────────────┐    │    │
│  │  │        FEED FORWARD                │    │    │
│  │  └─────────────────────────────────────┘    │    │
│  │                    ↓                        │    │
│  │  ┌─────────────────────────────────────┐    │    │
│  │  │           ADD & NORM               │    │    │
│  │  └─────────────────────────────────────┘    │    │
│  └─────────────────────────────────────────────┘    │
│                    ↓                                │
│  ┌─────────────────────────────────────────────┐    │
│  │              DECODER                        │    │
│  │  ┌─────────────────────────────────────┐    │    │
│  │  │    MASKED MULTI-HEAD ATTENTION      │    │    │
│  │  └─────────────────────────────────────┘    │    │
│  │                    ↓                        │    │
│  │  ┌─────────────────────────────────────┐    │    │
│  │  │           ADD & NORM               │    │    │
│  │  └─────────────────────────────────────┘    │    │
│  │                    ↓                        │    │
│  │  ┌─────────────────────────────────────┐    │    │
│  │  │    ENCODER-DECODER ATTENTION        │    │    │
│  │  └─────────────────────────────────────┘    │    │
│  │                    ↓                        │    │
│  │  ┌─────────────────────────────────────┐    │    │
│  │  │           ADD & NORM               │    │    │
│  │  └─────────────────────────────────────┘    │    │
│  │                    ↓                        │    │
│  │  ┌─────────────────────────────────────┐    │    │
│  │  │        FEED FORWARD                │    │    │
│  │  └─────────────────────────────────────┘    │    │
│  │                    ↓                        │    │
│  │  ┌─────────────────────────────────────┐    │    │
│  │  │           ADD & NORM               │    │    │
│  │  └─────────────────────────────────────┘    │    │
│  └─────────────────────────────────────────────┘    │
│                    ↓                                │
│               LINEAR + SOFTMAX                       │
│                    ↓                                │
│                 OUTPUT                               │
└─────────────────────────────────────────────────────┘
```

### 🎯 Multi-Head Attention

```
MULTI-HEAD ATTENTION:

       Input X
         │
    ┌────┼────┐
    │    │    │
    ▼    ▼    ▼
   Q₁   K₁   V₁     Head 1
   Q₂   K₂   V₂     Head 2
   ...  ...  ...    ...
   Qₕ   Kₕ   Vₕ     Head h
    │    │    │
    └────┼────┘
         │
    ┌────────────┐
    │ Attention  │
    │  Outputs   │
    └────────────┘
         │
    ┌────────────┐
    │  Concat &  │
    │  Linear    │
    └────────────┘
         │
        ▼
     Output
```

### 🔢 Positional Encoding

```
ПОЗИЦИОННОЕ КОДИРОВАНИЕ:

PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))

где:
- pos: позиция в последовательности
- i: индекс размерности
- d_model: размерность модели

ВИЗУАЛИЗАЦИЯ:
Position 0: [sin(0), cos(0), sin(0), cos(0), ...]
Position 1: [sin(1), cos(1), sin(1/100), cos(1/100), ...]
Position 2: [sin(2), cos(2), sin(2/100), cos(2/100), ...]
...

┌─────────────────────────────────────────────────────┐
│ Позиция │ Dim 0 │ Dim 1 │ Dim 2 │ Dim 3 │ ... │    │
├─────────────────────────────────────────────────────┤
│    0    │  0.0  │  1.0  │  0.0  │  1.0  │ ... │    │
│    1    │  0.8  │  0.5  │  0.01 │  1.0  │ ... │    │
│    2    │  0.9  │ -0.4  │  0.02 │  1.0  │ ... │    │
│   ...   │  ...  │  ...  │  ...  │  ...  │ ... │    │
└─────────────────────────────────────────────────────┘
```

### 🔄 Типы Transformer моделей

```
┌─────────────────────────────────────────────────────┐
│                СЕМЕЙСТВА МОДЕЛЕЙ                    │
├─────────────────────────────────────────────────────┤
│                                                     │
│ 🤖 BERT (Bidirectional Encoder Representations)    │
│    ├─ Encoder-only архитектура                     │
│    ├─ Маскированное языковое моделирование          │
│    └─ Задачи: классификация, NER, Q&A              │
│                                                     │
│ 🤖 GPT (Generative Pre-trained Transformer)       │
│    ├─ Decoder-only архитектура                     │
│    ├─ Авторегрессивная генерация                   │
│    └─ Задачи: генерация текста, диалоги            │
│                                                     │
│ 🤖 T5 (Text-to-Text Transfer Transformer)         │
│    ├─ Encoder-decoder архитектура                  │
│    ├─ Все задачи как text-to-text                  │
│    └─ Универсальный подход к NLP                   │
│                                                     │
│ 🤖 BART (Bidirectional and Auto-Regressive)       │
│    ├─ Encoder-decoder архитектура                  │
│    ├─ Denoising autoencoder                        │
│    └─ Задачи: суммаризация, перевод               │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 📊 Сравнение архитектур

```
┌─────────────────────────────────────────────────────┐
│          RNN vs CNN vs TRANSFORMER                  │
├─────────────────────────────────────────────────────┤
│                                                     │
│ 🔄 RNN                                             │
│    ├─ Последовательная обработка                   │
│    ├─ Сложность: O(n)                             │
│    └─ Проблемы с длинными последовательностями     │
│                                                     │
│ 🔲 CNN                                             │
│    ├─ Параллельная обработка                       │
│    ├─ Сложность: O(n/k)                           │
│    └─ Локальные зависимости                        │
│                                                     │
│ 🤖 TRANSFORMER                                     │
│    ├─ Полностью параллельная обработка             │
│    ├─ Сложность: O(n²)                            │
│    └─ Глобальные зависимости                       │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 💻 Практическое задание

**Задача:** Реализация мини-Transformer

```python
class MiniTransformer:
    def __init__(self, vocab_size, d_model, nhead, num_layers):
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model)
        self.transformer = nn.Transformer(d_model, nhead, num_layers)
        self.fc_out = nn.Linear(d_model, vocab_size)
    
    def forward(self, src, tgt):
        # Embeddings and positional encoding
        src_emb = self.pos_encoding(self.embedding(src))
        tgt_emb = self.pos_encoding(self.embedding(tgt))
        
        # Transformer
        output = self.transformer(src_emb, tgt_emb)
        
        # Final linear layer
        return self.fc_out(output)
```

### 📈 Преимущества Transformer

```
ПОЧЕМУ TRANSFORMER ЭФФЕКТИВЕН:

1. ПАРАЛЛЕЛИЗАЦИЯ
   RNN: x₁ → x₂ → x₃ → x₄  (последовательно)
   TRF: x₁ ┐ x₂ ┐ x₃ ┐ x₄  (параллельно)
          └────┴────┴────┘

2. ГЛОБАЛЬНЫЕ ЗАВИСИМОСТИ
   Каждый элемент может "видеть" все остальные
   
3. МАСШТАБИРУЕМОСТЬ
   Легко увеличивать количество параметров
   
4. TRANSFER LEARNING
   Предобученные модели отлично дообучаются
```

---

## 📝 Глава 4.6: Обработка естественного языка
**⏱ Длительность:** 1 неделя

### 🎯 Цели главы:
- Изучить токенизацию и препроцессинг текста
- Понять Word embeddings (Word2Vec, GloVe)
- Освоить работу с различными языками
- Реализовать анализ тональности

### 🔤 Токенизация

```
УРОВНИ ТОКЕНИЗАЦИИ:

1. СИМВОЛЬНЫЙ УРОВЕНЬ
   "Hello" → ['H', 'e', 'l', 'l', 'o']
   
2. УРОВЕНЬ СЛОВ
   "Hello world!" → ['Hello', 'world', '!']
   
3. ПОДСЛОВА (SUBWORD)
   "unbelievable" → ['un', 'believ', 'able']
   
4. BYTE-PAIR ENCODING (BPE)
   Статистический подход к разделению

┌─────────────────────────────────────────────────────┐
│                   ТОКЕНИЗАЦИЯ                       │
│                                                     │
│  "Привет, как дела?" → ['Привет', ',', 'как',      │
│                        'дела', '?']                 │
│                                                     │
│  Проблемы:                                          │
│  ├─ Неизвестные слова (OOV)                        │
│  ├─ Различные языки                                │
│  └─ Пунктуация и специальные символы               │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 🎯 Word Embeddings

```
ТРАДИЦИОННЫЕ ПОДХОДЫ:

1. ONE-HOT ENCODING
   "кот" → [0, 0, 1, 0, 0, ..., 0]
   "собака" → [0, 0, 0, 1, 0, ..., 0]
   
   Проблемы:
   ├─ Большая размерность
   ├─ Нет семантической близости
   └─ Разреженные вектора

2. WORD2VEC
   "кот" → [0.2, -0.1, 0.5, 0.3, ...]
   "собака" → [0.1, -0.2, 0.4, 0.2, ...]
   
   Преимущества:
   ├─ Компактные вектора
   ├─ Семантическая близость
   └─ Арифметические операции
```

### 🔢 Архитектуры Word2Vec

```
┌─────────────────────────────────────────────────────┐
│                 SKIP-GRAM                           │
│                                                     │
│      контекст ← слово → контекст                   │
│                                                     │
│  Input: "кот"                                      │
│  Output: ["сидит", "на", "коврике", "дома"]       │
│                                                     │
│  ┌─────┐     ┌─────────┐     ┌─────┐              │
│  │Input│ ──→ │Embedding│ ──→ │Output│              │
│  │Word │     │ Layer   │     │Context│             │
│  └─────┘     └─────────┘     └─────┘              │
│                                                     │
└─────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────┐
│                   CBOW                              │
│                                                     │
│      контекст → слово                              │
│                                                     │
│  Input: ["сидит", "на", "коврике", "дома"]        │
│  Output: "кот"                                     │
│                                                     │
│  ┌─────┐     ┌─────────┐     ┌─────┐              │
│  │Context│ ──→ │Embedding│ ──→ │Target│             │
│  │Words  │     │ Layer   │     │Word │             │
│  └─────┘     └─────────┘     └─────┘              │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 🌐 Многоязычность

```
РАБОТА С РАЗНЫМИ ЯЗЫКАМИ:

┌─────────────────────────────────────────────────────┐
│                  ПРОБЛЕМЫ                           │
├─────────────────────────────────────────────────────┤
│                                                     │
│ 🔤 КОДИРОВКИ                                       │
│    UTF-8, UTF-16, ASCII                            │
│                                                     │
│ 🔤 МОРФОЛОГИЯ                                      │
│    Русский: "кот-коты-котами-о котах"              │
│    Английский: "cat-cats"                          │
│                                                     │
│ 🔤 НАПРАВЛЕНИЕ ПИСЬМА                              │
│    Латиница: слева направо                         │
│    Арабский: справа налево                         │
│                                                     │
│ 🔤 СЕГМЕНТАЦИЯ                                     │
│    Китайский: нет пробелов между словами           │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 📊 Анализ тональности

```
PIPELINE АНАЛИЗА ТОНАЛЬНОСТИ:

1. ПРЕДОБРАБОТКА
   "Отличный фильм!!!" → "отличный фильм"
   
2. ТОКЕНИЗАЦИЯ
   "отличный фильм" → ["отличный", "фильм"]
   
3. ВЕКТОРИЗАЦИЯ
   ["отличный", "фильм"] → [0.5, 0.3, -0.1, 0.8, ...]
   
4. МОДЕЛЬ
   [0.5, 0.3, -0.1, 0.8, ...] → [0.1, 0.9] (негативная, позитивная)
   
5. РЕЗУЛЬТАТ
   ПОЗИТИВНАЯ (90% уверенности)

┌─────────────────────────────────────────────────────┐
│              КЛАССЫ ТОНАЛЬНОСТИ                     │
├─────────────────────────────────────────────────────┤
│                                                     │
│ 😢 НЕГАТИВНАЯ    │ 😐 НЕЙТРАЛЬНАЯ  │ 😊 ПОЗИТИВНАЯ │
│                  │                  │                │
│ ├─ Плохо         │ ├─ Нормально     │ ├─ Хорошо      │
│ ├─ Ужасно        │ ├─ Обычно        │ ├─ Отлично     │
│ └─ Разочарование │ └─ Информация    │ └─ Восторг     │
│                  │                  │                │
└─────────────────────────────────────────────────────┘
```

### 💻 Практическое задание

**Задача:** Анализ тональности текста

```python
class SentimentAnalyzer:
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.classifier = nn.Linear(hidden_dim, 3)  # neg, neu, pos
        
    def forward(self, text):
        # Embedding
        embedded = self.embedding(text)
        
        # LSTM
        lstm_out, (hidden, cell) = self.lstm(embedded)
        
        # Classification
        sentiment = self.classifier(hidden[-1])
        
        return sentiment
    
    def predict_sentiment(self, text):
        # Preprocessing
        tokens = self.tokenize(text)
        
        # Prediction
        scores = self.forward(tokens)
        sentiment = torch.argmax(scores, dim=1)
        
        return sentiment
```

### 📈 Метрики качества NLP

```
МЕТРИКИ ДЛЯ АНАЛИЗА ТОНАЛЬНОСТИ:

┌─────────────────────────────────────────────────────┐
│                CONFUSION MATRIX                     │
├─────────────────────────────────────────────────────┤
│                │ Predicted │           │            │
│   Actual       │    Neg    │   Neu     │    Pos     │
├─────────────────────────────────────────────────────┤
│   Negative     │    85     │    5      │    10      │
│   Neutral      │    10     │    90     │    0       │
│   Positive     │    5      │    0      │    95      │
└─────────────────────────────────────────────────────┘

МЕТРИКИ:
├─ Accuracy: (85+90+95)/(85+5+10+10+90+0+5+0+95) = 90%
├─ Precision (Pos): 95/(10+0+95) = 90.5%
├─ Recall (Pos): 95/(5+0+95) = 95%
└─ F1-Score (Pos): 2*(90.5*95)/(90.5+95) = 92.7%
```

### 🔧 Препроцессинг текста

```python
def preprocess_text(text):
    """
    Полный pipeline предобработки текста
    """
    
    # 1. Приведение к нижнему регистру
    text = text.lower()
    
    # 2. Удаление HTML тегов
    text = re.sub(r'<[^>]+>', '', text)
    
    # 3. Удаление URL
    text = re.sub(r'http\S+', '', text)
    
    # 4. Удаление пунктуации (кроме важной)
    text = re.sub(r'[^\w\s!?.]', '', text)
    
    # 5. Удаление лишних пробелов
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 6. Токенизация
    tokens = text.split()
    
    # 7. Удаление стоп-слов
    stop_words = set(['и', 'в', 'на', 'с', 'по', 'для', 'не'])
    tokens = [token for token in tokens if token not in stop_words]
    
    return tokens
```

---

## 🎯 Итоговый результат блока

### 📊 Что вы изучили:

```
┌─────────────────────────────────────────────────────┐
│                ОСВОЕННЫЕ НАВЫКИ                     │
├─────────────────────────────────────────────────────┤
│                                                     │
│ 🧠 АРХИТЕКТУРЫ                                     │
│    ├─ Vanilla RNN                                  │
│    ├─ LSTM/GRU                                     │
│    ├─ Bidirectional RNN                            │
│    ├─ Encoder-Decoder                              │
│    └─ Transformer                                  │
│                                                     │
│ 🔍 МЕХАНИЗМЫ                                       │
│    ├─ Attention mechanisms                         │
│    ├─ Self-attention                               │
│    ├─ Multi-head attention                         │
│    └─ Positional encoding                          │
│                                                     │
│ 📝 NLP ЗАДАЧИ                                      │
│    ├─ Анализ тональности                           │
│    ├─ Машинный перевод                             │
│    ├─ Генерация текста                             │
│    └─ Классификация текста                         │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 🏆 Практические результаты:

```
✅ Реализация чат-бота или системы машинного перевода
✅ Понимание архитектуры Transformer
✅ Проект по анализу тональности текста
✅ Готовность к изучению больших языковых моделей
✅ Навыки работы с последовательными данными
```

### 🚀 Готовность к следующему блоку:

Вы готовы к изучению **генеративных моделей** и работе с современными архитектурами типа GPT, BERT и других продвинутых моделей!