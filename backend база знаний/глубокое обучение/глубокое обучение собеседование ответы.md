# Глубокое обучение для Backend разработчика: Теория + PyTorch

## 1. Основы нейросетей

### Вопрос 1: Что такое нейрон и как он работает?

**Теория:** Нейрон = линейная комбинация входов + нелинейная активация

**Данные в Python:**
```python
import torch
import torch.nn as nn

# Входные данные: batch_size x input_features
x = torch.tensor([[1.0, 2.0, 3.0]])  # shape: (1, 3)
# Веса: input_features x output_features  
weights = torch.tensor([[0.5], [0.3], [-0.2]])  # shape: (3, 1)
bias = torch.tensor([0.1])  # shape: (1,)
```

**Код:**
```python
# Математически: y = activation(x @ weights + bias)
linear_output = x @ weights + bias  # (1, 1)
output = torch.relu(linear_output)  # ReLU активация

# Через PyTorch слой
neuron = nn.Linear(3, 1)  # 3 входа, 1 выход
output = neuron(x)
```

**Углубления:**
1. **Математическая запись:** `y = σ(w₁x₁ + w₂x₂ + w₃x₃ + b)`
2. **Bias нужен** для сдвига функции активации (без него нейрон всегда проходит через 0)
3. **Инициализация весов:** `nn.init.xavier_uniform_(neuron.weight)`

---

### Вопрос 2: Объясните forward pass в нейронной сети

**Теория:** Последовательное применение слоев к данным

**Данные:**
```python
# Batch данных: (batch_size, features)
x = torch.randn(32, 10)  # 32 примера, 10 признаков

# Простая сеть
net = nn.Sequential(
    nn.Linear(10, 5),
    nn.ReLU(),
    nn.Linear(5, 1)
)
```

**Код:**
```python
# Forward pass
output = net(x)  # shape: (32, 1)

# Пошагово:
hidden = torch.relu(net[0](x))  # (32, 5)
output = net[2](hidden)         # (32, 1)
```

**Углубления:**
1. **Batch обработка:** Все 32 примера обрабатываются параллельно матричными операциями
2. **Computational graph:** PyTorch автоматически строит граф для backprop
3. **Оптимизация:** Используйте `torch.no_grad()` для inference

---

### Вопрос 3: Что такое функции активации и зачем они нужны?

**Теория:** Добавляют нелинейность, без них сеть = одна линейная функция

**Данные:**
```python
x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])
```

**Код:**
```python
# Популярные активации
relu = torch.relu(x)        # [0, 0, 0, 1, 2]
sigmoid = torch.sigmoid(x)  # [0.12, 0.27, 0.5, 0.73, 0.88]
tanh = torch.tanh(x)        # [-0.96, -0.76, 0, 0.76, 0.96]
gelu = torch.nn.functional.gelu(x)  # [-0.05, -0.16, 0, 0.84, 1.95]
```

**Углубления:**
1. **Без активации:** Стек линейных слоев = одна линейная функция
2. **ReLU vs Sigmoid:** ReLU быстрее, нет vanishing gradients; Sigmoid для вероятностей
3. **GELU/Swish:** Более гладкие, хорошо для Transformers

---

### Вопрос 4: Объясните backpropagation простыми словами

**Теория:** Вычисление градиентов от выхода к входу через цепное правило

**Данные:**
```python
x = torch.tensor([1.0], requires_grad=True)
w = torch.tensor([2.0], requires_grad=True)
b = torch.tensor([0.5], requires_grad=True)
```

**Код:**
```python
# Forward
y = w * x + b  # y = 2.5
loss = (y - 3.0) ** 2  # target = 3.0, loss = 0.25

# Backward
loss.backward()  # Вычисляет все градиенты

print(f"∂loss/∂w = {w.grad}")  # -1.0
print(f"∂loss/∂b = {b.grad}")  # -1.0
print(f"∂loss/∂x = {x.grad}")  # -2.0
```

**Углубления:**
1. **Цепное правило:** `∂loss/∂w = ∂loss/∂y * ∂y/∂w`
2. **Эффективность:** O(n) vs O(n²) для численного дифференцирования
3. **Vanishing gradients:** Градиенты затухают в глубоких сетях

---

### Вопрос 5: Что такое функция потерь?

**Теория:** Измеряет разность между предсказанием и целевым значением

**Данные:**
```python
# Классификация: предсказания и истинные классы
predictions = torch.tensor([[2.0, 1.0, 0.1]])  # логиты для 3 классов
targets = torch.tensor([0])  # истинный класс 0

# Регрессия
pred_values = torch.tensor([2.5, 3.1, 1.8])
true_values = torch.tensor([2.0, 3.0, 2.0])
```

**Код:**
```python
# Cross-Entropy для классификации
ce_loss = nn.CrossEntropyLoss()
loss = ce_loss(predictions, targets)  # 0.41

# MSE для регрессии  
mse_loss = nn.MSELoss()
loss = mse_loss(pred_values, true_values)  # 0.08

# Focal Loss (для несбалансированных данных)
# focal_loss = alpha * (1-p)^gamma * ce_loss
```

**Углубления:**
1. **Выбор функции:** CE для классификации, MSE для регрессии, MAE для outliers
2. **CE vs MSE:** CE лучше для probability distributions
3. **Focal Loss:** Фокусируется на сложных примерах: `FL = -α(1-p)^γ log(p)`

---

### Вопрос 6: Объясните переобучение (overfitting)

**Теория:** Модель запоминает тренировочные данные, но плохо обобщает

**Данные:**
```python
# Простой пример: полином высокой степени
x_train = torch.linspace(0, 1, 10).unsqueeze(1)
y_train = torch.sin(2 * 3.14159 * x_train) + 0.1 * torch.randn_like(x_train)

# Большая модель (может переобучиться)
big_model = nn.Sequential(
    nn.Linear(1, 100),
    nn.ReLU(),
    nn.Linear(100, 100),
    nn.ReLU(),
    nn.Linear(100, 1)
)
```

**Код:**
```python
# Мониторинг переобучения
train_losses = []
val_losses = []

for epoch in range(1000):
    # Обучение
    train_pred = big_model(x_train)
    train_loss = nn.MSELoss()(train_pred, y_train)
    
    # Валидация (на других данных)
    with torch.no_grad():
        val_pred = big_model(x_val)
        val_loss = nn.MSELoss()(val_pred, y_val)
    
    # Переобучение: train_loss падает, val_loss растет
    if val_loss > min(val_losses[-10:]):  # Early stopping
        break
```

**Углубления:**
1. **Признаки:** train_loss ↓, val_loss ↑ или плато
2. **Bias-variance:** High variance (модель слишком сложная для данных)
3. **Методы борьбы:** dropout, regularization, early stopping, data augmentation

---

### Вопрос 7: Зачем нужны train/validation/test выборки?

**Теория:** Train - обучение, Val - настройка гиперпараметров, Test - финальная оценка

**Данные:**
```python
from sklearn.model_selection import train_test_split

# Данные
X = torch.randn(1000, 10)
y = torch.randint(0, 2, (1000,))

# Разделение: 60% train, 20% val, 20% test
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)
```

**Код:**
```python
# Использование
model = nn.Linear(10, 2)
optimizer = torch.optim.Adam(model.parameters())

# 1. Обучение на train
for epoch in range(100):
    pred = model(X_train)
    loss = nn.CrossEntropyLoss()(pred, y_train)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

# 2. Подбор гиперпараметров на val
best_lr = 0.001
for lr in [0.1, 0.01, 0.001]:
    # Обучаем с lr, проверяем на val
    pass

# 3. Финальная оценка на test (только один раз!)
final_accuracy = evaluate(model, X_test, y_test)
```

**Углубления:**
1. **Пропорции:** 70/15/15 или 80/10/10 в зависимости от размера данных
2. **Data leakage:** Информация из test/val не должна попасть в train (scaling, feature selection)
3. **Cross-validation:** При малых данных вместо фиксированного val

---

### Вопрос 8: Что такое batch normalization?

**Теория:** Нормализация активаций по батчу для стабилизации обучения

**Данные:**
```python
# Активации слоя (batch_size, features)
x = torch.randn(32, 100)  # 32 примера, 100 нейронов
print(f"Mean: {x.mean():.2f}, Std: {x.std():.2f}")  # ~0.0, ~1.0
```

**Код:**
```python
# Batch Norm слой
bn = nn.BatchNorm1d(100)

# Во время обучения
bn.train()
x_normalized = bn(x)  # Нормализует по батчу
print(f"After BN - Mean: {x_normalized.mean():.2f}, Std: {x_normalized.std():.2f}")

# Во время inference
bn.eval()
x_single = torch.randn(1, 100)  # Один пример
x_normalized = bn(x_single)  # Использует running statistics
```

**Углубления:**
1. **Проблема:** Internal covariate shift - распределение активаций меняется
2. **Inference:** Использует накопленные статистики `running_mean`, `running_var`
3. **Варианты:** LayerNorm (для NLP), GroupNorm (для малых батчей)

---

### Вопрос 9: Объясните dropout

**Теория:** Случайно "выключает" нейроны во время обучения

**Данные:**
```python
x = torch.ones(4, 5)  # Простой тензор из единиц
dropout = nn.Dropout(p=0.5)  # 50% нейронов выключается
```

**Код:**
```python
# Обучение: случайно обнуляет элементы
dropout.train()
x_dropped = dropout(x)
print("Training mode:")
print(x_dropped)  # Примерно половина нулей, остальные масштабированы

# Inference: ничего не обнуляет
dropout.eval() 
x_inference = dropout(x)
print("Inference mode:")
print(x_inference)  # Все единицы остались
```

**Углубления:**
1. **Почему помогает:** Предотвращает co-adaptation нейронов
2. **Scaling:** Во время обучения активации умножаются на `1/(1-p)`
3. **Где применять:** Обычно к полносвязным слоям, не к Conv слоям

---

### Вопрос 10: Что такое learning rate?

**Теория:** Размер шага при обновлении весов

**Данные:**
```python
model = nn.Linear(10, 1)
criterion = nn.MSELoss()

# Разные learning rates
optimizers = {
    'too_high': torch.optim.SGD(model.parameters(), lr=10.0),
    'good': torch.optim.SGD(model.parameters(), lr=0.01),
    'too_low': torch.optim.SGD(model.parameters(), lr=0.00001)
}
```

**Код:**
```python
# Влияние learning rate
x = torch.randn(100, 10)
y = torch.randn(100, 1)

for name, opt in optimizers.items():
    model_copy = nn.Linear(10, 1)
    losses = []
    
    for epoch in range(100):
        pred = model_copy(x)
        loss = criterion(pred, y)
        loss.backward()
        opt.step()
        opt.zero_grad()
        losses.append(loss.item())
    
    print(f"{name}: final loss {losses[-1]:.4f}")

# Learning rate scheduling
scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=30, gamma=0.1)
```

**Углубления:**
1. **Слишком большой:** Loss взрывается или осциллирует
2. **Слишком маленький:** Очень медленная сходимость
3. **Scheduling:** Cosine annealing, exponential decay, cyclic LR

---

### Вопрос 11: Что такое transfer learning?

**Теория:** Использование предобученной модели для новой задачи

**Данные:**
```python
import torchvision.models as models

# Предобученная ResNet
pretrained_model = models.resnet18(pretrained=True)
print(pretrained_model.fc)  # Последний слой: Linear(512, 1000)

# Наша задача: 10 классов вместо 1000
num_classes = 10
```

**Код:**
```python
# Заморозить все слои кроме последнего
for param in pretrained_model.parameters():
    param.requires_grad = False

# Заменить последний слой
pretrained_model.fc = nn.Linear(512, num_classes)

# Только последний слой будет обучаться
optimizer = torch.optim.Adam(pretrained_model.fc.parameters(), lr=0.001)

# Fine-tuning: разморозить все и обучать с малым LR
for param in pretrained_model.parameters():
    param.requires_grad = True
optimizer = torch.optim.Adam(pretrained_model.parameters(), lr=0.0001)
```

**Углубления:**
1. **Какие слои:** Ранние слои (edges, textures) переносятся лучше
2. **Fine-tuning vs Feature extraction:** Обучать всю сеть vs только classifier
3. **Эффективность:** Особенно при малых данных и похожих доменах

---

### Вопрос 12: Объясните метрики качества

**Теория:** Способы измерения качества модели

**Данные:**
```python
# Бинарная классификация
y_true = torch.tensor([1, 0, 1, 1, 0, 1, 0, 0])
y_pred = torch.tensor([1, 0, 1, 0, 0, 1, 1, 0])  # Предсказания
y_proba = torch.tensor([0.9, 0.1, 0.8, 0.6, 0.3, 0.7, 0.55, 0.2])  # Вероятности
```

**Код:**
```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score

# Основные метрики
accuracy = (y_pred == y_true).float().mean()  # 0.75
precision = precision_score(y_true, y_pred)   # 0.67
recall = recall_score(y_true, y_pred)         # 0.75  
f1 = 2 * precision * recall / (precision + recall)  # 0.71

# ROC-AUC (нужны вероятности)
auc = roc_auc_score(y_true, y_proba)  # 0.84

# Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true, y_pred)
# [[2 1]   TN FP
#  [1 3]]  FN TP
```

**Углубления:**
1. **Accuracy обманывает:** При несбалансированных классах (99% vs 1%)
2. **Precision vs Recall:** Precision = TP/(TP+FP), Recall = TP/(TP+FN)
3. **ROC-AUC неинформативен:** При сильно несбалансированных данных, лучше PR-AUC

---

## 2. Обучение и оптимизация

### Вопрос 13: Как работает градиентный спуск?

**Теория:** Итеративно движется в направлении антиградиента

**Данные:**
```python
# Простая функция: f(x) = (x-3)² + 2
x = torch.tensor([0.0], requires_grad=True)
target = 3.0
```

**Код:**
```python
learning_rate = 0.1
history = []

for step in range(20):
    # Forward: вычисляем функцию
    y = (x - target) ** 2 + 2
    
    # Backward: вычисляем градиент
    y.backward()
    
    # Update: шаг в направлении антиградиента
    with torch.no_grad():
        x -= learning_rate * x.grad
        x.grad.zero_()
    
    history.append((x.item(), y.item()))

print(f"Converged to x={x.item():.3f}")  # Должно быть ~3.0
```

**Углубления:**
1. **Варианты:** SGD (один пример), Mini-batch (подмножество), Batch (все данные)
2. **Momentum:** Накапливает "скорость" для преодоления локальных минимумов
3. **Поиск минимума:** Градиент указывает направление наибольшего роста

---

### Вопрос 14: Что такое Adam optimizer?

**Теория:** Адаптивный оптимизатор, подстраивает LR для каждого параметра

**Данные:**
```python
model = nn.Linear(5, 1)
data = torch.randn(100, 5)
targets = torch.randn(100, 1)

# Сравнение оптимизаторов
optimizers = {
    'SGD': torch.optim.SGD(model.parameters(), lr=0.01),
    'Adam': torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))
}
```

**Код:**
```python
# Adam внутри:
# m_t = β₁ * m_{t-1} + (1-β₁) * grad  # 1-й момент (momentum)
# v_t = β₂ * v_{t-1} + (1-β₂) * grad²  # 2-й момент (RMSprop)
# m̂_t = m_t / (1-β₁ᵗ)  # bias correction
# v̂_t = v_t / (1-β₂ᵗ)  # bias correction  
# param -= lr * m̂_t / (√v̂_t + ε)

losses_comparison = {}
for name, opt in optimizers.items():
    model_copy = nn.Linear(5, 1)
    opt_copy = type(opt)(model_copy.parameters(), **opt.defaults)
    losses = []
    
    for epoch in range(100):
        pred = model_copy(data)
        loss = nn.MSELoss()(pred, targets)
        loss.backward()
        opt_copy.step()
        opt_copy.zero_grad()
        losses.append(loss.item())
    
    losses_comparison[name] = losses
```

**Углубления:**
1. **Адаптивность:** Автоматически подстраивает effective learning rate
2. **Моменты:** β₁=0.9 (momentum), β₂=0.999 (squared gradients)
3. **Когда SGD лучше:** При хорошо подобранном LR и scheduling

---

### Вопрос 15: Объясните gradient clipping

**Теория:** Ограничивает норму градиентов для стабильности

**Данные:**
```python
# RNN может иметь взрывающиеся градиенты
rnn = nn.LSTM(10, 20, num_layers=3)
x = torch.randn(50, 32, 10)  # sequence_len, batch_size, features
target = torch.randn(50, 32, 20)
```

**Код:**
```python
optimizer = torch.optim.Adam(rnn.parameters())
max_norm = 1.0  # Максимальная норма градиента

for epoch in range(100):
    output, _ = rnn(x)
    loss = nn.MSELoss()(output, target)
    loss.backward()
    
    # Клиппинг градиентов
    grad_norm = torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm)
    
    if grad_norm > max_norm:
        print(f"Clipped gradient norm from {grad_norm:.2f} to {max_norm}")
    
    optimizer.step()
    optimizer.zero_grad()

# Альтернатива - клиппинг по значению
# torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)
```

**Углубления:**
1. **Проблема:** Exploding gradients в RNN/LSTM/очень глубоких сетях
2. **Clipping by norm:** Масштабирует все градиенты пропорционально
3. **Важность:** Особенно критично для sequence modeling

---

### Вопрос 16: Что такое регуляризация?

**Теория:** Добавление ограничений для предотвращения переобучения

**Данные:**
```python
model = nn.Sequential(
    nn.Linear(10, 100),
    nn.ReLU(),
    nn.Linear(100, 1)
)

x = torch.randn(100, 10)
y = torch.randn(100, 1)
```

**Код:**
```python
# L2 регуляризация (weight decay)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)

# L1 регуляризация (вручную)
def l1_loss(model, lambda_l1=0.01):
    l1_penalty = sum(p.abs().sum() for p in model.parameters())
    return lambda_l1 * l1_penalty

# Обучение с регуляризацией
for epoch in range(100):
    pred = model(x)
    loss = nn.MSELoss()(pred, y)
    
    # Добавляем L1 регуляризацию
    loss += l1_loss(model)
    
    loss.backward()
    optimizer.step()  # weight_decay автоматически добавляет L2
    optimizer.zero_grad()

# Data augmentation как регуляризация
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10)
])
```

**Углубления:**
1. **L1 vs L2:** L1 создает sparse weights, L2 просто уменьшает
2. **Weight decay:** L2 регуляризация встроена в оптимизатор
3. **Data augmentation:** Увеличивает размер данных искусственно

---

### Вопрос 17: Объясните early stopping

**Теория:** Останавливает обучение когда validation loss перестает улучшаться

**Данные:**
```python
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)
model = nn.Sequential(nn.Linear(10, 50), nn.ReLU(), nn.Linear(50, 1))
```

**Код:**
```python
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float('inf')
        self.counter = 0
        
    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
        return self.counter >= self.patience

early_stopping = EarlyStopping(patience=10)
optimizer = torch.optim.Adam(model.parameters())

for epoch in range(1000):
    # Обучение
    model.train()
    train_loss = train_epoch(model, train_loader, optimizer)
    
    # Валидация
    model.eval()
    val_loss = validate(model, val_loader)
    
    if early_stopping(val_loss):
        print(f"Early stopping at epoch {epoch}")
        break
```

**Углубления:**
1. **Patience:** Сколько эпох ждать улучшения
2. **Критерий:** Обычно validation loss, иногда accuracy
3. **Воспроизводимость:** Сохранять лучшую модель, а не последнюю

---

### Вопрос 18: Что такое learning rate finder?

**Теория:** Автоматический поиск оптимального learning rate

**Данные:**
```python
model = nn.Linear(10, 1)
criterion = nn.MSELoss()
x = torch.randn(1000, 10)
y = torch.randn(1000, 1)
```

**Код:**
```python
def lr_finder(model, data, targets, start_lr=1e-6, end_lr=1, num_iter=100):
    optimizer = torch.optim.SGD(model.parameters(), lr=start_lr)
    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 
                                                          gamma=(end_lr/start_lr)**(1/num_iter))
    
    lrs = []
    losses = []
    
    for i in range(num_iter):
        # Forward pass
        pred = model(data)
        loss = criterion(pred, targets)
        
        # Record
        lrs.append(optimizer.param_groups[0]['lr'])
        losses.append(loss.item())
        
        # Backward pass
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        lr_scheduler.step()
        
        # Stop if loss explodes
        if i > 10 and loss.item() > 2 * min(losses):
            break
    
    return lrs, losses

# Использование
lrs, losses = lr_finder(model, x, y)
# Optimal LR обычно там, где loss падает быстрее всего
```

**Углубления:**
1. **Алгоритм:** Экспоненциально увеличивает LR и записывает loss
2. **Интерпретация:** Оптимальный LR там, где градиент наибольший
3. **Практика:** Обычно берут на порядок меньше найденного максимума

---

### Вопрос 19: Объясните mixup/cutmix

**Теория:** Data augmentation через смешивание примеров

**Данные:**
```python
# Batch изображений
images = torch.randn(32, 3, 224, 224)  # batch_size, channels, height, width
labels = torch.randint(0, 10, (32,))   # класы 0-9
```

**Код:**
```python
def mixup(images, labels, alpha=0.2):
    batch_size = images.size(0)
    
    # Случайная перестановка
    indices = torch.randperm(batch_size)
    
    # Коэффициент смешивания из Beta распределения
    lam = torch.distributions.Beta(alpha, alpha).sample()
    
    # Смешивание изображений
    mixed_images = lam * images + (1 - lam) * images[indices]
    
    # Смешивание меток
    mixed_labels = lam * labels + (1 - lam) * labels[indices]
    
    return mixed_images, mixed_labels

def cutmix(images, labels, alpha=1.0):
    batch_size = images.size(0)
    indices = torch.randperm(batch_size)
    
    lam = torch.distributions.Beta(alpha, alpha).sample()
    
    # Вырезаем прямоугольник
    H, W = images.size(2), images.size(3)
    cut_h, cut_w = int(H * (1 - lam)), int(W * (1 - lam))
    cx, cy = torch.randint(H, (1,)), torch.randint(W, (1,))
    
    x1, y1 = max(0, cx - cut_h // 2), max(0, cy - cut_w // 2)
    x2, y2 = min(H, cx + cut_h // 2), min(W, cy + cut_w // 2)
    
    mixed_images = images.clone()
    mixed_images[:, :, x1:x2, y1:y2] = images[indices, :, x1:x2, y1:y2]
    
    return mixed_images, labels, indices, lam

# Использование в обучении
mixed_images, mixed_labels = mixup(images, labels)
```

**Углубления:**
1. **Mixup:** Линейная интерполяция изображений и меток
2. **CutMix:** Вырезает патч из одного изображения в другое
3. **Параметры:** alpha контролирует интенсивность смешивания

---

### Вопрос 20: Что такое gradient accumulation?

**Теория:** Накопление градиентов для эмуляции больших батчей

**Данные:**
```python
# Большая модель, не помещается батч 128
model = nn.Linear(1000, 1000)  
real_batch_size = 128
mini_batch_size = 32  # Помещается в память
accumulation_steps = real_batch_size // mini_batch_size  # 4
```

**Код:**
```python
optimizer = torch.optim.Adam(model.parameters())
data_loader = torch.utils.data.DataLoader(dataset, batch_size=mini_batch_size)

for batch_idx, (data, target) in enumerate(data_loader):
    # Forward pass
    output = model(data)
    loss = criterion(output, target) / accumulation_steps  # Нормализуем
    
    # Backward pass (накапливаем градиенты)
    loss.backward()
    
    # Обновляем веса каждые accumulation_steps
    if (batch_idx + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# Эквивалентно обучению с батчом размера real_batch_size
```

**Углубления:**
1. **Зачем:** Большие батчи не помещаются в GPU память
2. **Эффективный batch size:** mini_batch_size × accumulation_steps
3. **BatchNorm:** Работает на mini-batch, не на эффективном batch

---

### Вопрос 21: Объясните knowledge distillation

**Теория:** Обучение маленькой модели на "мягких" предсказаниях большой

**Данные:**
```python
# Большая модель (teacher)
teacher = nn.Sequential(
    nn.Linear(10, 1000), nn.ReLU(),
    nn.Linear(1000, 1000), nn.ReLU(), 
    nn.Linear(1000, 10)
)

# Маленькая модель (student)  
student = nn.Sequential(
    nn.Linear(10, 50), nn.ReLU(),
    nn.Linear(50, 10)
)

x = torch.randn(100, 10)
y = torch.randint(0, 10, (100,))
```

**Код:**
```python
def distillation_loss(student_logits, teacher_logits, true_labels, 
                     temperature=3, alpha=0.7):
    # Soft targets от teacher
    soft_teacher = torch.softmax(teacher_logits / temperature, dim=1)
    soft_student = torch.log_softmax(student_logits / temperature, dim=1)
    
    # KL divergence между teacher и student
    distill_loss = torch.nn.KLDivLoss(reduction='batchmean')(
        soft_student, soft_teacher) * (temperature ** 2)
    
    # Обычная loss с истинными метками  
    student_loss = torch.nn.CrossEntropyLoss()(student_logits, true_labels)
    
    return alpha * distill_loss + (1 - alpha) * student_loss

# Обучение student
teacher.eval()  # Заморожен
optimizer = torch.optim.Adam(student.parameters())

for epoch in range(100):
    with torch.no_grad():
        teacher_logits = teacher(x)
    
    student_logits = student(x)
    loss = distillation_loss(student_logits, teacher_logits, y)
    
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
```

**Углубления:**
1. **Soft targets:** teacher дает распределение вероятностей, не просто класс
2. **Temperature:** Делает распределение более "мягким"
3. **Self-distillation:** Модель обучает сама себя на следующей итерации

---

### Вопрос 22: Что такое ensemble методы?

**Теория:** Объединение предсказаний нескольких моделей

**Данные:**
```python
# Несколько разных архитектур
models = [
    nn.Sequential(nn.Linear(10, 50), nn.ReLU(), nn.Linear(50, 2)),
    nn.Sequential(nn.Linear(10, 100), nn.ReLU(), nn.Linear(100, 2)),
    nn.Sequential(nn.Linear(10, 20), nn.ReLU(), nn.Linear(20, 20), nn.ReLU(), nn.Linear(20, 2))
]

x_test = torch.randn(100, 10)
```

**Код:**
```python
# Voting ensemble
def ensemble_predict(models, x, method='soft'):
    predictions = []
    
    for model in models:
        model.eval()
        with torch.no_grad():
            pred = model(x)
            if method == 'soft':
                pred = torch.softmax(pred, dim=1)  # Вероятности
            else:
                pred = torch.argmax(pred, dim=1)   # Hard vote
            predictions.append(pred)
    
    if method == 'soft':
        # Среднее вероятностей
        ensemble_pred = torch.stack(predictions).mean(dim=0)
        return torch.argmax(ensemble_pred, dim=1)
    else:
        # Голосование
        predictions = torch.stack(predictions)
        return torch.mode(predictions, dim=0)[0]

# Weighted ensemble
weights = torch.tensor([0.5, 0.3, 0.2])  # Веса по важности моделей
weighted_pred = sum(w * torch.softmax(model(x_test), dim=1) 
                   for w, model in zip(weights, models))
```

**Углубления:**
1. **Типы:** Voting, averaging, stacking, bagging
2. **Dropout как ensemble:** Каждый forward pass = новая подсеть
3. **Затраты:** Увеличивает время inference в N раз

---

## 3. Архитектуры

### Вопрос 23: Что такое сверточный слой?

**Теория:** Применяет фильтр (kernel) к локальным областям изображения

**Данные:**
```python
# Изображение: batch_size, channels, height, width
image = torch.randn(1, 3, 32, 32)  # RGB изображение 32x32

# Сверточный слой
conv = nn.Conv2d(
    in_channels=3,   # RGB входы
    out_channels=64, # 64 фильтра  
    kernel_size=3,   # 3x3 фильтр
    stride=1,        # Шаг 1
    padding=1        # Дополнение краев
)
```

**Код:**
```python
# Forward pass
output = conv(image)  # shape: (1, 64, 32, 32)

# Размер выхода: (H + 2*padding - kernel_size) / stride + 1
def output_size(input_size, kernel_size, stride, padding):
    return (input_size + 2*padding - kernel_size) // stride + 1

h_out = output_size(32, 3, 1, 1)  # 32
w_out = output_size(32, 3, 1, 1)  # 32

# Визуализация весов
conv_weights = conv.weight  # shape: (64, 3, 3, 3)
print(f"Kernel shape: {conv_weights.shape}")
# 64 фильтра, каждый 3x3 для 3 каналов
```

**Углубления:**
1. **Параметры:** kernel_size × in_channels × out_channels + bias
2. **Вычисления:** Скользящее окно с поэлементным умножением
3. **Эффективность:** Локальность, разделение весов, трансляционная инвариантность

---

### Вопрос 24: Объясните pooling слои

**Теория:** Уменьшают пространственные размеры, сохраняя важную информацию

**Данные:**
```python
# Feature maps после свертки
feature_maps = torch.randn(1, 64, 32, 32)

# Разные виды pooling
max_pool = nn.MaxPool2d(kernel_size=2, stride=2)
avg_pool = nn.AvgPool2d(kernel_size=2, stride=2) 
adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling
```

**Код:**
```python
# Max pooling: берет максимум в окне 2x2
max_output = max_pool(feature_maps)  # (1, 64, 16, 16)

# Average pooling: берет среднее
avg_output = avg_pool(feature_maps)  # (1, 64, 16, 16)

# Global average pooling: одно число на канал
global_output = adaptive_pool(feature_maps)  # (1, 64, 1, 1)

# Пример работы max pooling на 2x2 окне:
x = torch.tensor([[1., 2.], [3., 4.]])
print(torch.max(x))  # 4.0 - максимум из окна

# Альтернативы pooling
stride_conv = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)
strided_output = stride_conv(feature_maps)  # Тоже уменьшает размер
```

**Углубления:**
1. **Max vs Average:** Max сохраняет яркие features, Average - общую структуру
2. **Global Average Pooling:** Заменяет полносвязные слои, меньше параметров
3. **Альтернативы:** Strided convolutions, dilated convolutions

---

### Вопрос 25: Что такое ResNet и residual connections?

**Теория:** Skip connections позволяют обучать очень глубокие сети

**Данные:**
```python
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # Skip connection для изменения размерности
        self.downsample = None
        if stride != 1 or in_channels != out_channels:
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride),
                nn.BatchNorm2d(out_channels)
            )
```

**Код:**
```python
    def forward(self, x):
        residual = x  # Сохраняем вход
        
        # Основной путь
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        
        # Skip connection
        if self.downsample:
            residual = self.downsample(x)
        
        out += residual  # Главное: сложение!
        return torch.relu(out)

# Использование
x = torch.randn(1, 64, 32, 32)
block = ResidualBlock(64, 64)
output = block(x)  # Тот же размер

# Без residual connections глубокие сети деградируют:
deep_net = nn.Sequential(*[nn.Conv2d(64, 64, 3, 1, 1) for _ in range(50)])
# Такая сеть плохо обучается из-за vanishing gradients
```

**Углубления:**
1. **Проблема:** Очень глубокие сети хуже мелких даже на train
2. **Решение:** Skip connections позволяют градиентам "обходить" слои
3. **Identity mapping:** F(x) = H(x) - x, где H(x) - целевая функция

---

### Вопрос 26: Объясните 1x1 convolutions

**Теория:** Изменяют количество каналов без изменения пространственных размеров

**Данные:**
```python
# Feature maps с большим количеством каналов
x = torch.randn(1, 512, 32, 32)  # Много каналов

# 1x1 свертка для уменьшения каналов (bottleneck)
bottleneck = nn.Conv2d(512, 64, kernel_size=1)
reduced = bottleneck(x)  # (1, 64, 32, 32)

# Потом можно применить дорогую операцию
expensive_conv = nn.Conv2d(64, 64, kernel_size=3, padding=1)
processed = expensive_conv(reduced)

# И восстановить каналы
expand = nn.Conv2d(64, 512, kernel_size=1)  
output = expand(processed)  # (1, 512, 32, 32)
```

**Код:**
```python
# Bottleneck block (как в ResNet)
class BottleneckBlock(nn.Module):
    def __init__(self, in_channels, bottleneck_channels, out_channels):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, bottleneck_channels, 1)  # Reduce
        self.conv2 = nn.Conv2d(bottleneck_channels, bottleneck_channels, 3, padding=1)  # Process  
        self.conv3 = nn.Conv2d(bottleneck_channels, out_channels, 1)  # Expand
        
    def forward(self, x):
        out = torch.relu(self.conv1(x))      # 512 -> 64
        out = torch.relu(self.conv2(out))    # 64 -> 64  
        out = self.conv3(out)                # 64 -> 512
        return out + x  # Residual connection

# Параметры: 512*64 + 64*64*9 + 64*512 = 102,400
# Без bottleneck: 512*512*9 = 2,359,296 (в 23 раза больше!)
```

**Углубления:**
1. **Назначение:** Изменение количества каналов, добавление нелинейности
2. **Bottleneck:** Сначала уменьшаем каналы, потом увеличиваем
3. **Нелинейность:** Даже 1x1 conv + ReLU добавляет нелинейность

---

### Вопрос 27: Что такое рекуррентные сети (RNN)?

**Теория:** Обрабатывают последовательности, имеют "память" о прошлом

**Данные:**
```python
# Последовательность слов (например, предложение)
sequence_length = 10
batch_size = 32
vocab_size = 1000
hidden_size = 128

# Входная последовательность
input_seq = torch.randint(0, vocab_size, (sequence_length, batch_size))
embedding = nn.Embedding(vocab_size, hidden_size)
x = embedding(input_seq)  # (seq_len, batch, hidden_size)
```

**Код:**
```python
# Простая RNN ячейка
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.Wxh = nn.Linear(input_size, hidden_size)   # input to hidden
        self.Whh = nn.Linear(hidden_size, hidden_size)  # hidden to hidden
        
    def forward(self, x, hidden=None):
        batch_size = x.size(1)
        
        if hidden is None:
            hidden = torch.zeros(batch_size, self.hidden_size)
            
        outputs = []
        for t in range(x.size(0)):  # По времени
            hidden = torch.tanh(self.Wxh(x[t]) + self.Whh(hidden))
            outputs.append(hidden)
            
        return torch.stack(outputs), hidden

# Использование
rnn = SimpleRNN(hidden_size, hidden_size)
outputs, final_hidden = rnn(x)  # outputs: (seq_len, batch, hidden)

# PyTorch RNN
builtin_rnn = nn.RNN(hidden_size, hidden_size, batch_first=False)
outputs, hidden = builtin_rnn(x)
```

**Углубления:**
1. **Hidden state:** "Память" сети, передается от шага к шагу
2. **Формула:** h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b)
3. **Проблема:** Vanishing gradients для длинных последовательностей

---

### Вопрос 28: Объясните LSTM

**Теория:** Решает проблему long-term dependencies через систему "ворот"

**Данные:**
```python
# LSTM для обработки текста
seq_len, batch_size, input_size = 20, 32, 100
hidden_size = 256

lstm = nn.LSTM(input_size, hidden_size, batch_first=False)
x = torch.randn(seq_len, batch_size, input_size)
```

**Код:**
```python
# LSTM forward pass
output, (h_n, c_n) = lstm(x)
# output: (seq_len, batch, hidden_size) - все выходы
# h_n: (1, batch, hidden_size) - последний hidden state  
# c_n: (1, batch, hidden_size) - последний cell state

# Внутренняя логика LSTM (упрощенно):
class LSTMCell(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        # Все 4 gate в одной матрице (эффективнее)
        self.weight_ih = nn.Linear(input_size, 4 * hidden_size)
        self.weight_hh = nn.Linear(hidden_size, 4 * hidden_size)
        
    def forward(self, x, state):
        h, c = state
        
        # Все gates сразу
        gates = self.weight_ih(x) + self.weight_hh(h)
        i, f, g, o = gates.chunk(4, 1)  # input, forget, gate, output
        
        i = torch.sigmoid(i)  # Input gate
        f = torch.sigmoid(f)  # Forget gate  
        g = torch.tanh(g)     # New memory
        o = torch.sigmoid(o)  # Output gate
        
        # Обновление cell state и hidden state
        c_new = f * c + i * g
        h_new = o * torch.tanh(c_new)
        
        return h_new, (h_new, c_new)
```

**Углубления:**
1. **Gates:** Forget (что забыть), Input (что запомнить), Output (что вывести)
2. **Cell state vs Hidden state:** Cell = долгосрочная память, Hidden = краткосрочная
3. **Преимущества:** Может запоминать на сотни шагов

---

### Вопрос 29: Что такое attention mechanism?

**Теория:** Позволяет модели "обращать внимание" на разные части входа

**Данные:**
```python
# Последовательность для перевода
seq_len, batch_size, hidden_size = 10, 32, 256

# Encoder outputs (все скрытые состояния)
encoder_outputs = torch.randn(seq_len, batch_size, hidden_size)

# Decoder hidden state (текущее состояние декодера)  
decoder_hidden = torch.randn(batch_size, hidden_size)
```

**Код:**
```python
class AttentionMechanism(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.attn = nn.Linear(hidden_size * 2, hidden_size)
        self.v = nn.Linear(hidden_size, 1, bias=False)
        
    def forward(self, decoder_hidden, encoder_outputs):
        seq_len = encoder_outputs.size(0)
        batch_size = encoder_outputs.size(1)
        
        # Повторяем decoder_hidden для каждого encoder шага
        decoder_hidden = decoder_hidden.unsqueeze(0).repeat(seq_len, 1, 1)
        
        # Concatenate и вычисляем energy
        energy = torch.tanh(self.attn(torch.cat([decoder_hidden, encoder_outputs], dim=2)))
        attention_weights = torch.softmax(self.v(energy).squeeze(2), dim=0)
        
        # Weighted sum encoder outputs
        context = torch.sum(attention_weights.unsqueeze(2) * encoder_outputs, dim=0)
        
        return context, attention_weights

# Использование
attention = AttentionMechanism(hidden_size)
context, weights = attention(decoder_hidden, encoder_outputs)
print(f"Context shape: {context.shape}")  # (batch_size, hidden_size)
print(f"Attention weights: {weights.shape}")  # (seq_len, batch_size)
```

**Углубления:**
1. **Проблема:** Encoder "забывает" начало длинной последовательности
2. **Решение:** Decoder смотрит на все encoder states, а не только последний
3. **Веса внимания:** Показывают, на какие слова модель "смотрит"

---

### Вопрос 30: Объясните архитектуру Transformer

**Теория:** Полностью основан на attention, без recurrence

**Данные:**
```python
seq_len, batch_size, d_model = 100, 32, 512
vocab_size = 10000

# Входная последовательность токенов
input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))

# Embeddings + positional encoding
embedding = nn.Embedding(vocab_size, d_model)
pos_encoding = torch.randn(seq_len, d_model)  # Обычно синусоидальное
```

**Код:**
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)  
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        
        # Linear transformations and split into heads
        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)
        
        # Scaled dot-product attention
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)
        
        if mask is not None:
            scores.masked_fill_(mask == 0, -1e9)
            
        attention_weights = torch.softmax(scores, dim=-1)
        context = torch.matmul(attention_weights, V)
        
        # Concatenate heads
        context = context.transpose(1, 2).contiguous().view(
            batch_size, -1, self.d_model)
        
        return self.W_o(context)

# Positional encoding (синусоидальное)
def positional_encoding(seq_len, d_model):
    pe = torch.zeros(seq_len, d_model)
    position = torch.arange(0, seq_len).unsqueeze(1).float()
    
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                        -(torch.log(torch.tensor(10000.0)) / d_model))
    
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    
    return pe
```

**Углубления:**
1. **Без рекуррентности:** Все позиции обрабатываются параллельно
2. **Multi-head attention:** Несколько "взглядов" на данные одновременно
3. **Positional encoding:** Информация о позиции, так как attention не учитывает порядок

---

### Вопрос 31: В чем разница между BERT и GPT?

**Теория:** BERT - bidirectional encoder, GPT - autoregressive decoder

**Данные:**
```python
from transformers import BertModel, GPT2Model, BertTokenizer, GPT2Tokenizer

# BERT: понимает контекст с обеих сторон
bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')

# GPT: генерирует следующий токен
gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
gpt_model = GPT2Model.from_pretrained('gpt2')

text = "The cat sat on the [MASK]."
```

**Код:**
```python
# BERT: Masked Language Modeling
bert_tokens = bert_tokenizer(text.replace('[MASK]', '[MASK]'), 
                            return_tensors='pt', padding=True)
with torch.no_grad():
    bert_outputs = bert_model(**bert_tokens)
    # Может предсказать [MASK] используя весь контекст

# GPT: Autoregressive generation  
gpt_tokens = gpt_tokenizer("The cat sat on", return_tensors='pt')
with torch.no_grad():
    gpt_outputs = gpt_model(**gpt_tokens)
    # Предсказывает следующий токен только по левому контексту

# Главные различия:
print("BERT использует:")
print("- Bidirectional attention (видит весь контекст)")
print("- Masked Language Modeling")
print("- Encoder architecture")

print("\nGPT использует:")  
print("- Causal attention (видит только левый контекст)")
print("- Next token prediction")
print("- Decoder architecture")
```

**Углубления:**
1. **Bidirectional vs Autoregressive:** BERT видит будущее, GPT - нет
2. **Задачи:** BERT для понимания (classification), GPT для генерации
3. **Attention masks:** BERT - полная матрица, GPT - треугольная (causal)

---

### Вопрос 32: Что такое self-attention?

**Теория:** Attention между элементами одной последовательности

**Данные:**
```python
# Последовательность токенов
seq_len, batch_size, d_model = 10, 32, 512
x = torch.randn(batch_size, seq_len, d_model)

# Веса для Q, K, V
W_q = nn.Linear(d_model, d_model, bias=False)
W_k = nn.Linear(d_model, d_model, bias=False)  
W_v = nn.Linear(d_model, d_model, bias=False)
```

**Код:**
```python
def self_attention(x, W_q, W_k, W_v):
    # Query, Key, Value из одного входа
    Q = W_q(x)  # (batch, seq_len, d_model)
    K = W_k(x)  # (batch, seq_len, d_model)
    V = W_v(x)  # (batch, seq_len, d_model)
    
    # Attention scores
    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_model ** 0.5)  # Scaled
    
    # Softmax для получения весов
    attention_weights = torch.softmax(scores, dim=-1)  # (batch, seq_len, seq_len)
    
    # Weighted sum значений
    output = torch.matmul(attention_weights, V)  # (batch, seq_len, d_model)
    
    return output, attention_weights

# Использование
output, attention_matrix = self_attention(x, W_q, W_k, W_v)

# Attention matrix показывает, какие позиции влияют друг на друга
print(f"Attention matrix shape: {attention_matrix.shape}")  # (batch, seq_len, seq_len)

# Cross-attention (для сравнения)
def cross_attention(query_seq, key_value_seq, W_q, W_k, W_v):
    Q = W_q(query_seq)      # От одной последовательности
    K = W_k(key_value_seq)  # От другой последовательности  
    V = W_v(key_value_seq)  # От другой последовательности
    
    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_model ** 0.5)
    attention_weights = torch.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)
    
    return output, attention_weights
```

**Углубления:**
1. **Self vs Cross:** Self - внутри последовательности, Cross - между разными
2. **Вычислительная сложность:** O(n²) где n = длина последовательности
3. **Интерпретация:** Матрица attention показывает зависимости между словами

---

### Вопрос 33: Объясните Vision Transformer (ViT)

**Теория:** Transformer для изображений через разбиение на патчи

**Данные:**
```python
# Изображение
batch_size, channels, height, width = 32, 3, 224, 224
image = torch.randn(batch_size, channels, height, width)

# Параметры ViT
patch_size = 16  # Каждый патч 16x16
d_model = 768    # Размерность embeddings
num_patches = (height // patch_size) * (width // patch_size)  # 196 патчей
```

**Код:**
```python
class PatchEmbedding(nn.Module):
    def __init__(self, img_size, patch_size, in_channels, embed_dim):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        
        # Convolution для разбиения на патчи
        self.proj = nn.Conv2d(in_channels, embed_dim, 
                             kernel_size=patch_size, stride=patch_size)
        
    def forward(self, x):
        # x: (batch, channels, height, width)
        x = self.proj(x)  # (batch, embed_dim, H/patch_size, W/patch_size)
        x = x.flatten(2)  # (batch, embed_dim, num_patches)
        x = x.transpose(1, 2)  # (batch, num_patches, embed_dim)
        return x

class VisionTransformer(nn.Module):
    def __init__(self, img_size=224, patch_size=16, num_classes=1000, 
                 embed_dim=768, num_heads=12, num_layers=12):
        super().__init__()
        
        # Patch embedding
        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, embed_dim)
        
        # Class token (для классификации)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        
        # Positional embeddings
        num_patches = self.patch_embed.num_patches
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        
        # Transformer layers
        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # Classification head
        self.head = nn.Linear(embed_dim, num_classes)
        
    def forward(self, x):
        batch_size = x.shape[0]
        
        # Patch embedding
        x = self.patch_embed(x)  # (batch, num_patches, embed_dim)
        
        # Добавляем class token
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # (batch, num_patches+1, embed_dim)
        
        # Positional encoding
        x = x + self.pos_embed
        
        # Transformer
        x = self.transformer(x)
        
        # Классификация по class token
        cls_output = x[:, 0]  # Первый токен
        return self.head(cls_output)

# Использование
vit = VisionTransformer()
output = vit(image)  # (batch_size, num_classes)
```

**Углубления:**
1. **Patch embeddings:** 16x16 патчи как "слова" для Transformer
2. **Class token:** Специальный токен для агрегации информации
3. **Vs CNN:** ViT лучше на больших датасетах, CNN эффективнее на малых

---

### Вопрос 34: Что такое U-Net?

**Теория:** Encoder-decoder с skip connections для сегментации

**Данные:**
```python
# Медицинское изображение для сегментации
batch_size, channels, height, width = 8, 1, 256, 256
image = torch.randn(batch_size, channels, height, width)

# Маска сегментации (ground truth)
mask = torch.randint(0, 2, (batch_size, 1, height, width))  # Бинарная маска
```

**Код:**
```python
class UNet(nn.Module):
    def __init__(self, in_channels=1, out_channels=1, features=[64, 128, 256, 512]):
        super().__init__()
        
        # Encoder (сжимающий путь)
        self.encoder = nn.ModuleList()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        in_ch = in_channels
        for feature in features:
            self.encoder.append(nn.Sequential(
                nn.Conv2d(in_ch, feature, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(feature, feature, 3, padding=1),
                nn.ReLU(inplace=True)
            ))
            in_ch = feature
            
        # Bottleneck
        self.bottleneck = nn.Sequential(
            nn.Conv2d(features[-1], features[-1]*2, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(features[-1]*2, features[-1]*2, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        
        # Decoder (расширяющий путь)
        self.decoder = nn.ModuleList()
        self.upconvs = nn.ModuleList()
        
        for feature in reversed(features):
            self.upconvs.append(nn.ConvTranspose2d(feature*2, feature, 2, stride=2))
            self.decoder.append(nn.Sequential(
                nn.Conv2d(feature*2, feature, 3, padding=1),
                nn.ReLU(inplace=True),
                nn.Conv2d(feature, feature, 3, padding=1),
                nn.ReLU(inplace=True)
            ))
            
        # Финальная классификация
        self.final_conv = nn.Conv2d(features[0], out_channels, 1)
        
    def forward(self, x):
        # Encoder path + сохранение skip connections
        skip_connections = []
        
        for encode in self.encoder:
            x = encode(x)
            skip_connections.append(x)
            x = self.pool(x)
            
        # Bottleneck
        x = self.bottleneck(x)
        
        # Decoder path
        skip_connections = skip_connections[::-1]  # Обратный порядок
        
        for idx, (upconv, decode) in enumerate(zip(self.upconvs, self.decoder)):
            x = upconv(x)  # Увеличиваем размер
            
            # Concatenate со skip connection
            skip_connection = skip_connections[idx]
            if x.shape != skip_connection.shape:
                x = nn.functional.interpolate(x, size=skip_connection.shape[2:])
                
            concat_skip = torch.cat([skip_connection, x], dim=1)
            x = decode(concat_skip)
            
        return torch.sigmoid(self.final_conv(x))

# Использование
unet = UNet(in_channels=1, out_channels=1)
prediction = unet(image)  # (batch, 1, 256, 256)

# Loss для сегментации
criterion = nn.BCELoss()  # Binary Cross Entropy
loss = criterion(prediction, mask.float())
```

**Углубления:**
1. **Encoder-Decoder:** Сжатие для контекста + расширение для точности
2. **Skip connections:** Передают детали с encoder на decoder
3. **Применения:** Медицинская сегментация, обработка спутниковых снимков

---

### Вопрос 35: Объясните MobileNet

**Теория:** Эффективная архитектура через depthwise separable convolutions

**Данные:**
```python
# Сравнение обычной и depthwise separable свертки
input_channels, output_channels = 32, 64
height, width = 56, 56
kernel_size = 3

x = torch.randn(1, input_channels, height, width)

# Обычная свертка
standard_conv = nn.Conv2d(input_channels, output_channels, kernel_size, padding=1)
standard_params = input_channels * output_channels * kernel_size**2
```

**Код:**
```python
class DepthwiseSeparableConv(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=1):
        super().__init__()
        
        # Depthwise: каждый канал обрабатывается отдельно
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, 
                                  stride, padding, groups=in_channels)
        
        # Pointwise: 1x1 свертка для смешивания каналов
        self.pointwise = nn.Conv2d(in_channels, out_channels, 1)
        
    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        return x

class MobileNetBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv = DepthwiseSeparableConv(in_channels, out_channels, 3, stride, 1)
        self.bn1 = nn.BatchNorm2d(in_channels)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
    def forward(self, x):
        x = torch.relu(self.bn1(x))
        x = self.conv(x)
        x = torch.relu(self.bn2(x))
        return x

# Сравнение параметров
depthwise_conv = DepthwiseSeparableConv(input_channels, output_channels, kernel_size)

# Параметры стандартной свертки
standard_params = input_channels * output_channels * kernel_size**2
print(f"Standard conv parameters: {standard_params}")  # 18,432

# Параметры depthwise separable
depthwise_params = input_channels * kernel_size**2 + input_channels * output_channels  
print(f"Depthwise separable parameters: {depthwise_params}")  # 2,336

print(f"Reduction factor: {standard_params / depthwise_params:.1f}x")  # ~8x меньше

# Простая MobileNet
class SimpleMobileNet(nn.Module):
    def __init__(self, num_classes=1000):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, 3, stride=2, padding=1),
            MobileNetBlock(32, 64),
            MobileNetBlock(64, 128, stride=2),
            MobileNetBlock(128, 256, stride=2),
            nn.AdaptiveAvgPool2d(1)
        )
        self.classifier = nn.Linear(256, num_classes)
        
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)
```

**Углубления:**
1. **Depthwise separable:** Разделяет пространственную и канальную фильтрацию
2. **Эффективность:** В 8-9 раз меньше параметров при сопоставимом качестве
3. **Применение:** Мобильные устройства, embedded системы

---

### Вопрос 36: Что такое dilated convolutions?

**Теория:** Увеличивают receptive field без увеличения параметров

**Данные:**
```python
# Входной feature map
x = torch.randn(1, 64, 32, 32)

# Обычная свертка
standard_conv = nn.Conv2d(64, 64, kernel_size=3, padding=1)

# Dilated свертки с разными dilation rates  
dilated_conv_2 = nn.Conv2d(64, 64, kernel_size=3, padding=2, dilation=2)
dilated_conv_4 = nn.Conv2d(64, 64, kernel_size=3, padding=4, dilation=4)
```

**Код:**
```python
# Визуализация receptive field
def receptive_field_size(kernel_size, dilation):
    """Effective kernel size with dilation"""
    return kernel_size + (kernel_size - 1) * (dilation - 1)

print("Receptive field sizes:")
print(f"Standard 3x3: {receptive_field_size(3, 1)}")  # 3
print(f"Dilated 3x3 (d=2): {receptive_field_size(3, 2)}")  # 5  
print(f"Dilated 3x3 (d=4): {receptive_field_size(3, 4)}")  # 9

# Atrous Spatial Pyramid Pooling (ASPP) - популярный паттерн
class ASPP(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        
        # Разные dilation rates для multi-scale features
        self.conv1 = nn.Conv2d(in_channels, out_channels, 1)  # 1x1
        self.conv6 = nn.Conv2d(in_channels, out_channels, 3, padding=6, dilation=6)
        self.conv12 = nn.Conv2d(in_channels, out_channels, 3, padding=12, dilation=12)
        self.conv18 = nn.Conv2d(in_channels, out_channels, 3, padding=18, dilation=18)
        
        # Global average pooling
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.global_conv = nn.Conv2d(in_channels, out_channels, 1)
        
        # Объединение
        self.conv_concat = nn.Conv2d(out_channels * 5, out_channels, 1)
        
    def forward(self, x):
        size = x.shape[2:]
        
        # Разные масштабы
        feat1 = self.conv1(x)
        feat6 = self.conv6(x) 
        feat12 = self.conv12(x)
        feat18 = self.conv18(x)
        
        # Global context
        feat_global = self.global_conv(self.global_pool(x))
        feat_global = nn.functional.interpolate(feat_global, size=size, mode='bilinear')
        
        # Concatenate все features
        concat = torch.cat([feat1, feat6, feat12, feat18, feat_global], dim=1)
        return self.conv_concat(concat)

# Использование в DeepLab
aspp = ASPP(64, 256)
output = aspp(x)  # Multi-scale features
```

**Углубления:**
1. **Dilation rate:** Расстояние между элементами kernel
2. **Применения:** Семантическая сегментация (DeepLab), детекция объектов
3. **Vs pooling:** Сохраняет разрешение при увеличении receptive field

---

### Вопрос 37: Объясните bidirectional RNN

**Теория:** Обрабатывает последовательность в обоих направлениях

**Данные:**
```python
# Последовательность для анализа тональности
sequence = ["This", "movie", "is", "really", "good"]
seq_len, batch_size, hidden_size = len(sequence), 1, 128

# Embeddings слов
vocab_size = 1000
embedding_dim = 100
embeddings = nn.Embedding(vocab_size, embedding_dim)

# Преобразуем в индексы (для примера)
input_ids = torch.randint(0, vocab_size, (seq_len, batch_size))
x = embeddings(input_ids)  # (seq_len, batch, embedding_dim)
```

**Код:**
```python
class BidirectionalRNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=1):
        super().__init__()
        
        # Forward RNN
        self.forward_rnn = nn.LSTM(input_size, hidden_size, num_layers)
        
        # Backward RNN  
        self.backward_rnn = nn.LSTM(input_size, hidden_size, num_layers)
        
        # Объединение выходов
        self.output_layer = nn.Linear(hidden_size * 2, hidden_size)
        
    def forward(self, x):
        # Forward pass (слева направо)
        forward_out, _ = self.forward_rnn(x)
        
        # Backward pass (справа налево)
        x_reversed = torch.flip(x, [0])  # Обращаем последовательность
        backward_out, _ = self.backward_rnn(x_reversed)
        backward_out = torch.flip(backward_out, [0])  # Обращаем обратно
        
        # Concatenate forward и backward
        bidirectional_out = torch.cat([forward_out, backward_out], dim=2)
        
        # Опционально: уменьшаем размерность
        output = self.output_layer(bidirectional_out)
        
        return output, bidirectional_out

# PyTorch встроенная bidirectional LSTM
builtin_bilstm = nn.LSTM(embedding_dim, hidden_size, bidirectional=True)
output, (h_n, c_n) = builtin_bilstm(x)

print(f"Bidirectional output shape: {output.shape}")  # (seq_len, batch, hidden_size*2)
print(f"Final hidden shape: {h_n.shape}")  # (2, batch, hidden_size) - forward + backward

# Применение для классификации последовательности
class BiLSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.bilstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True)
        self.classifier = nn.Linear(hidden_dim * 2, num_classes)
        
    def forward(self, x):
        # x: (seq_len, batch)
        embedded = self.embedding(x)  # (seq_len, batch, embed_dim)
        lstm_out, (h_n, c_n) = self.bilstm(embedded)
        
        # Используем последний выход (или можно max/mean pooling)
        last_output = lstm_out[-1]  # (batch, hidden_dim * 2)
        
        return self.classifier(last_output)

# Для задач sequence labeling (NER, POS tagging)
class BiLSTMTagger(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_tags):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.bilstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True)
        self.tag_classifier = nn.Linear(hidden_dim * 2, num_tags)
        
    def forward(self, x):
        embedded = self.embedding(x)
        lstm_out, _ = self.bilstm(embedded)
        
        # Классифицируем каждую позицию
        tag_scores = self.tag_classifier(lstm_out)  # (seq_len, batch, num_tags)
        
        return tag_scores
```

**Углубления:**
1. **Две направления:** Каждая позиция видит прошлое и будущее
2. **Real-time ограничения:** Нельзя использовать для онлайн задач
3. **Применения:** NER, POS tagging, машинный перевод (encoder)

---

## 4. Продвинутые темы

### Вопрос 38: Что такое GAN?

**Теория:** Две сети соревнуются: Generator создает данные, Discriminator их отличает

**Данные:**
```python
# Параметры для генерации изображений
latent_dim = 100  # Размерность шума
img_channels = 1  # Черно-белые изображения  
img_size = 28     # 28x28 (как MNIST)

# Шум для генератора
noise = torch.randn(32, latent_dim)

# Реальные изображения
real_images = torch.randn(32, img_channels, img_size, img_size)
```

**Код:**
```python
class Generator(nn.Module):
    def __init__(self, latent_dim, img_channels, img_size):
        super().__init__()
        self.img_shape = (img_channels, img_size, img_size)
        
        self.model = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.LeakyReLU(0.2),
            nn.Linear(128, 256),
            nn.BatchNorm1d(256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.BatchNorm1d(512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, np.prod(self.img_shape)),
            nn.Tanh()  # Выход в [-1, 1]
        )
        
    def forward(self, z):
        img = self.model(z)
        return img.view(img.size(0), *self.img_shape)

class Discriminator(nn.Module):
    def __init__(self, img_channels, img_size):
        super().__init__()
        
        self.model = nn.Sequential(
            nn.Linear(img_channels * img_size * img_size, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()  # Вероятность "реальности"
        )
        
    def forward(self, img):
        img_flat = img.view(img.size(0), -1)
        return self.model(img_flat)

# Инициализация
generator = Generator(latent_dim, img_channels, img_size)
discriminator = Discriminator(img_channels, img_size)

# Оптимизаторы  
opt_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
opt_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

criterion = nn.BCELoss()

# Обучающий цикл
for epoch in range(100):
    # Обучение Discriminator
    opt_D.zero_grad()
    
    # Реальные изображения
    real_labels = torch.ones(32, 1)
    real_output = discriminator(real_images)
    d_loss_real = criterion(real_output, real_labels)
    
    # Фейковые изображения
    noise = torch.randn(32, latent_dim)
    fake_images = generator(noise).detach()  # Отключаем градиенты для G
    fake_labels = torch.zeros(32, 1)
    fake_output = discriminator(fake_images)
    d_loss_fake = criterion(fake_output, fake_labels)
    
    d_loss = d_loss_real + d_loss_fake
    d_loss.backward()
    opt_D.step()
    
    # Обучение Generator
    opt_G.zero_grad()
    
    noise = torch.randn(32, latent_dim)
    fake_images = generator(noise)
    fake_output = discriminator(fake_images)
    g_loss = criterion(fake_output, real_labels)  # G хочет обмануть D
    
    g_loss.backward()
    opt_G.step()
```

**Углубления:**
1. **Adversarial training:** Minimax игра между G и D
2. **Mode collapse:** G генерирует только несколько режимов данных
3. **Улучшения:** WGAN, Progressive GAN, StyleGAN для стабильности

---

### Вопрос 39: Объясните Variational Autoencoder (VAE)

**Теория:** Кодирует данные в вероятностное латентное пространство

**Данные:**
```python
# Изображения для реконструкции
batch_size, channels, height, width = 32, 1, 28, 28
x = torch.randn(batch_size, channels, height, width)

# Латентное пространство
latent_dim = 20
```

**Код:**
```python
class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim):
        super().__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        
        # Encoder: x -> μ, σ
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU()
        )
        
        # Параметры распределения
        self.fc_mu = nn.Linear(256, latent_dim)      # Среднее
        self.fc_logvar = nn.Linear(256, latent_dim)  # log(σ²)
        
        # Decoder: z -> x
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, input_dim),
            nn.Sigmoid()  # Выход в [0, 1]
        )
        
    def encode(self, x):
        h = self.encoder(x.view(-1, self.input_dim))
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        """Reparameterization trick: z = μ + σ * ε"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)  # Случайный шум
        return mu + eps * std
    
    def decode(self, z):
        return self.decoder(z)
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon_x = self.decode(z)
        return recon_x, mu, logvar

def vae_loss(recon_x, x, mu, logvar, beta=1.0):
    # Reconstruction loss
    recon_loss = nn.functional.binary_cross_entropy(
        recon_x, x.view(-1, x.size(-1)), reduction='sum')
    
    # KL divergence: KL(q(z|x) || p(z))
    # где p(z) = N(0, I), q(z|x) = N(μ, σ²)
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    
    return recon_loss + beta * kl_loss

# Обучение VAE
vae = VAE(input_dim=28*28, latent_dim=20)
optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)

for epoch in range(100):
    optimizer.zero_grad()
    
    recon_batch, mu, logvar = vae(x)
    loss = vae_loss(recon_batch, x, mu, logvar)
    
    loss.backward()
    optimizer.step()

# Генерация новых образцов
with torch.no_grad():
    # Сэмплируем из prior p(z) = N(0, I)
    z = torch.randn(16, latent_dim)
    generated = vae.decode(z)
    
# Интерполяция в латентном пространстве
def interpolate(vae, x1, x2, n_steps=10):
    with torch.no_grad():
        mu1, _ = vae.encode(x1)
        mu2, _ = vae.encode(x2)
        
        interpolations = []
        for alpha in torch.linspace(0, 1, n_steps):
            z_interp = alpha * mu1 + (1 - alpha) * mu2
            x_interp = vae.decode(z_interp)
            interpolations.append(x_interp)
            
        return torch.stack(interpolations)
```

**Углубления:**
1. **Отличие от Autoencoder:** VAE кодирует в распределение, не в точку
2. **Reparameterization trick:** Позволяет backprop через случайную переменную
3. **Латентное пространство:** Структурированное, позволяет интерполяцию

---

### Вопрос 40: Что такое Graph Neural Networks?

**Теория:** Нейросети для обработки графовых структур данных

**Данные:**
```python
import torch_geometric
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, global_mean_pool

# Простой граф: узлы и рёбра
num_nodes = 5
node_features = torch.randn(num_nodes, 3)  # 3 признака на узел

# Список рёбер (откуда, куда)
edge_index = torch.tensor([
    [0, 1, 1, 2, 2, 3, 3, 4],  # Источники
    [1, 0, 2, 1, 3, 2, 4, 3]   # Назначения
], dtype=torch.long)

# Метки узлов (для node classification)
node_labels = torch.randint(0, 2, (num_nodes,))

# Создание графа
graph = Data(x=node_features, edge_index=edge_index, y=node_labels)
```

**Код:**
```python
class GraphConvolutionalNetwork(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):
        super().__init__()
        
        self.convs = nn.ModuleList()
        self.convs.append(GCNConv(input_dim, hidden_dim))
        
        for _ in range(num_layers - 2):
            self.convs.append(GCNConv(hidden_dim, hidden_dim))
            
        self.convs.append(GCNConv(hidden_dim, output_dim))
        self.dropout = nn.Dropout(0.5)
        
    def forward(self, x, edge_index, batch=None):
        # Применяем Graph Convolutions
        for i, conv in enumerate(self.convs[:-1]):
            x = conv(x, edge_index)
            x = torch.relu(x)
            x = self.dropout(x)
            
        # Последний слой без активации
        x = self.convs[-1](x, edge_index)
        
        # Для graph classification - агрегируем узлы
        if batch is not None:
            x = global_mean_pool(x, batch)
            
        return x

# Простая реализация Graph Convolution
class SimpleGCN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.linear = nn.Linear(input_dim, output_dim)
        
    def forward(self, x, adjacency_matrix):
        # Нормализация матрицы смежности
        degree = adjacency_matrix.sum(dim=1, keepdim=True)
        norm_adj = adjacency_matrix / (degree + 1e-6)
        
        # Агрегация соседей + self-loop
        aggregated = torch.matmul(norm_adj, x) + x
        
        # Линейное преобразование
        return self.linear(aggregated)

# Message Passing для понимания принципа
class MessagePassing(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.message_net = nn.Linear(input_dim * 2, output_dim)
        self.update_net = nn.Linear(input_dim + output_dim, output_dim)
        
    def forward(self, x, edge_index):
        row, col = edge_index  # Источники и назначения рёбер
        
        # Message: конкатенируем признаки соседних узлов
        messages = self.message_net(torch.cat([x[row], x[col]], dim=1))
        
        # Агрегация: суммируем сообщения для каждого узла
        node_messages = torch.zeros_like(x[:, :messages.size(1)])
        node_messages.index_add_(0, col, messages)
        
        # Update: обновляем представления узлов
        updated = self.update_net(torch.cat([x, node_messages], dim=1))
        
        return updated

# Задачи с графами
print("GNN применяются для:")
print("1. Node classification - классификация узлов")
print("2. Graph classification - классификация всего графа") 
print("3. Link prediction - предсказание рёбер")
print("4. Graph generation - генерация графов")

# Примеры данных:
print("\nПримеры графовых данных:")
print("- Социальные сети (люди = узлы, связи = рёбра)")
print("- Молекулы (атомы = узлы, связи = рёбра)")
print("- Дороги (перекрёстки = узлы, дороги = рёбра)")
print("- Знания (сущности = узлы, отношения = рёбра)")
```

**Углубления:**
1. **Message passing:** Узлы обмениваются информацией через рёбра
2. **Агрегация:** Объединение сообщений от соседей (sum, mean, max)
3. **Применения:** Рекомендации, drug discovery, анализ социальных сетей

---

### Вопрос 41: Объясните контрастивное обучение

**Теория:** Обучение через сравнение похожих и непохожих примеров

**Данные:**
```python
# Batch изображений
batch_size, channels, height, width = 128, 3, 32, 32
images = torch.randn(batch_size, channels, height, width)

# Аугментации для создания positive pairs
augment1 = transforms.Compose([
    transforms.RandomResizedCrop(32),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)
])

augment2 = transforms.Compose([
    transforms.RandomResizedCrop(32),
    transforms.RandomHorizontalFlip(), 
    transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)
])
```

**Код:**
```python
class ContrastiveEncoder(nn.Module):
    def __init__(self, base_encoder, projection_dim=128):
        super().__init__()
        self.encoder = base_encoder
        
        # Projection head для контрастивного обучения
        self.projection = nn.Sequential(
            nn.Linear(base_encoder.fc.in_features, 512),
            nn.ReLU(),
            nn.Linear(512, projection_dim)
        )
        
        # Убираем classifier из base encoder
        self.encoder.fc = nn.Identity()
        
    def forward(self, x):
        features = self.encoder(x)
        projections = self.projection(features)
        
        # L2 нормализация для cosine similarity
        projections = nn.functional.normalize(projections, dim=1)
        
        return features, projections

def contrastive_loss(projections, temperature=0.1):
    """InfoNCE loss для контрастивного обучения"""
    batch_size = projections.size(0) // 2
    
    # Разделяем на два view
    z1 = projections[:batch_size]  # Первая аугментация
    z2 = projections[batch_size:]  # Вторая аугментация
    
    # Объединяем все проекции
    z = torch.cat([z1, z2], dim=0)  # (2*batch_size, projection_dim)
    
    # Вычисляем similarity matrix
    sim_matrix = torch.matmul(z, z.T) / temperature
    
    # Создаём маску для positive pairs
    mask = torch.zeros(2*batch_size, 2*batch_size)
    mask[range(batch_size), range(batch_size, 2*batch_size)] = 1
    mask[range(batch_size, 2*batch_size), range(batch_size)] = 1
    
    # InfoNCE loss
    exp_sim = torch.exp(sim_matrix)
    
    # Сумма по всем negative examples (исключая диагональ)
    exp_sim_masked = exp_sim * (1 - torch.eye(2*batch_size))
    
    # Positive pairs
    pos_sim = exp_sim * mask
    
    # Loss = -log(pos / (pos + negatives))
    loss = -torch.log(pos_sim.sum(dim=1) / exp_sim_masked.sum(dim=1))
    
    return loss.mean()

# Обучение
encoder = ContrastiveEncoder(models.resnet18())
optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)

for epoch in range(100):
    # Создаём два view каждого изображения
    view1 = torch.stack([augment1(img) for img in images])
    view2 = torch.stack([augment2(img) for img in images])
    
    # Объединяем для batch processing
    both_views = torch.cat([view1, view2], dim=0)
    
    # Forward pass
    features, projections = encoder(both_views)
    
    # Contrastive loss
    loss = contrastive_loss(projections)
    
    # Backward pass
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Self-supervised предобучение -> fine-tuning
class DownstreamClassifier(nn.Module):
    def __init__(self, pretrained_encoder, num_classes):
        super().__init__()
        self.encoder = pretrained_encoder.encoder  # Замороженный encoder
        self.classifier = nn.Linear(512, num_classes)  # Новый classifier
        
        # Заморозить encoder
        for param in self.encoder.parameters():
            param.requires_grad = False
            
    def forward(self, x):
        with torch.no_grad():
            features = self.encoder(x)
        return self.classifier(features)

# Популярные методы контрастивного обучения:
print("Контрастивные методы:")
print("- SimCLR: аугментации одного изображения как positive pair")
print("- MoCo: momentum-based контрастивное обучение") 
print("- SwAV: clustering + контрастивное обучение")
print("- BYOL: обучение без negative samples")
```

**Углубления:**
1. **Positive/Negative pairs:** Похожие примеры притягиваются, разные отталкиваются
2. **InfoNCE loss:** Максимизирует mutual information между views
3. **Применения:** Self-supervised learning, few-shot learning, retrieval

---

### Вопрос 42: Что такое few-shot learning?

**Теория:** Обучение на малом количестве примеров каждого класса

**Данные:**
```python
# N-way K-shot задача: N классов, K примеров на класс
N_way = 5      # 5 классов
K_shot = 1     # 1 пример на класс для обучения
query_size = 15  # Примеров для тестирования

# Support set: примеры для обучения
support_images = torch.randn(N_way * K_shot, 3, 84, 84)
support_labels = torch.arange(N_way).repeat_interleave(K_shot)

# Query set: примеры для тестирования  
query_images = torch.randn(query_size, 3, 84, 84)
query_labels = torch.randint(0, N_way, (query_size,))
```

**Код:**
```python
class PrototypicalNetwork(nn.Module):
    """Prototypical Networks для few-shot classification"""
    
    def __init__(self, encoder_dim=1600, hidden_dim=128):
        super().__init__()
        
        # Feature encoder (можно использовать любую CNN)
        self.encoder = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64), 
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            nn.Conv2d(64, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            nn.Flatten(),
            nn.Linear(encoder_dim, hidden_dim)
        )
        
    def forward(self, support_images, support_labels, query_images):
        # Кодируем все изображения
        support_features = self.encoder(support_images)  # (N*K, hidden_dim)
        query_features = self.encoder(query_images)      # (query_size, hidden_dim)
        
        # Вычисляем прототипы (центроиды) для каждого класса
        unique_labels = torch.unique(support_labels)
        prototypes = []
        
        for label in unique_labels:
            # Берём все примеры данного класса
            class_features = support_features[support_labels == label]
            # Вычисляем среднее (прототип)
            prototype = class_features.mean(dim=0)
            prototypes.append(prototype)
            
        prototypes = torch.stack(prototypes)  # (N_way, hidden_dim)
        
        # Вычисляем расстояния от query до прототипов
        distances = torch.cdist(query_features, prototypes)  # (query_size, N_way)
        
        # Конвертируем в логиты (чем меньше расстояние, тем выше вероятность)
        logits = -distances
        
        return logits

class MAML(nn.Module):
    """Model-Agnostic Meta-Learning"""
    
    def __init__(self, model, inner_lr=0.01, meta_lr=0.001):
        super().__init__()
        self.model = model
        self.inner_lr = inner_lr
        self.meta_lr = meta_lr
        self.meta_optimizer = torch.optim.Adam(self.model.parameters(), lr=meta_lr)
        
    def inner_loop(self, support_x, support_y, num_steps=5):
        """Быстрая адаптация к новой задаче"""
        
        # Копируем параметры модели
        fast_weights = {name: param.clone() 
                       for name, param in self.model.named_parameters()}
        
        for step in range(num_steps):
            # Forward pass с текущими весами
            logits = self.model(support_x)
            loss = nn.CrossEntropyLoss()(logits, support_y)
            
            # Вычисляем градиенты
            grads = torch.autograd.grad(loss, self.model.parameters(), 
                                      create_graph=True)
            
            # Обновляем fast weights
            fast_weights = {name: param - self.inner_lr * grad 
                           for (name, param), grad in zip(fast_weights.items(), grads)}
            
            # Обновляем параметры модели
            for name, param in self.model.named_parameters():
                param.data = fast_weights[name]
                
        return fast_weights
        
    def meta_forward(self, support_x, support_y, query_x, query_y):
        """Meta-learning step"""
        
        # Сохраняем исходные параметры
        original_params = {name: param.clone() 
                          for name, param in self.model.named_parameters()}
        
        # Inner loop: адаптация к задаче
        fast_weights = self.inner_loop(support_x, support_y)
        
        # Тестируем на query set
        query_logits = self.model(query_x)
        meta_loss = nn.CrossEntropyLoss()(query_logits, query_y)
        
        # Восстанавливаем исходные параметры
        for name, param in self.model.named_parameters():
            param.data = original_params[name]
            
        return meta_loss

# Обучение Prototypical Networks
proto_net = PrototypicalNetwork()
optimizer = torch.optim.Adam(proto_net.parameters(), lr=1e-3)

for episode in range(1000):
    # Каждый эпизод = новая few-shot задача
    logits = proto_net(support_images, support_labels, query_images)
    loss = nn.CrossEntropyLoss()(logits, query_labels)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Метрики для few-shot learning
def few_shot_accuracy(model, support_x, support_y, query_x, query_y):
    with torch.no_grad():
        logits = model(support_x, support_y, query_x)
        predictions = torch.argmax(logits, dim=1)
        accuracy = (predictions == query_y).float().mean()
    return accuracy.item()

print("Few-shot подходы:")
print("1. Metric Learning: учимся хорошей метрике расстояний")
print("2. Meta-Learning: учимся быстро адаптироваться")  
print("3. Data Augmentation: увеличиваем количество примеров")
print("4. Transfer Learning: используем предобученные признаки")
```

**Углубления:**
1. **N-way K-shot:** N классов, K примеров на обучение
2. **Meta-learning:** "Учимся учиться" на множестве задач
3. **Prototypical Networks:** Классификация через ближайший прототип

---

### Вопрос 43: Объясните multi-task learning

**Теория:** Одновременное обучение на нескольких связанных задачах

**Данные:**
```python
# Пример: анализ изображений лиц
batch_size = 32
image_size = 224

# Входные изображения лиц
face_images = torch.randn(batch_size, 3, image_size, image_size)

# Несколько задач на одних данных:
age_labels = torch.randint(18, 80, (batch_size,))        # Возраст (регрессия)
gender_labels = torch.randint(0, 2, (batch_size,))       # Пол (классификация)
emotion_labels = torch.randint(0, 7, (batch_size,))      # Эмоция (классификация)
glasses_labels = torch.randint(0, 2, (batch_size,))      # Очки (классификация)
```

**Код:**
```python
class MultiTaskModel(nn.Module):
    def __init__(self, backbone='resnet18'):
        super().__init__()
        
        # Общий backbone для всех задач
        if backbone == 'resnet18':
            self.shared_encoder = models.resnet18(pretrained=True)
            feature_dim = self.shared_encoder.fc.in_features
            self.shared_encoder.fc = nn.Identity()  # Убираем последний слой
        
        # Task-specific heads
        self.age_head = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 1)  # Регрессия возраста
        )
        
        self.gender_head = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 2)  # Бинарная классификация
        )
        
        self.emotion_head = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 7)  # 7 эмоций
        )
        
        self.glasses_head = nn.Sequential(
            nn.Linear(feature_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 2)  # Есть/нет очки
        )
        
    def forward(self, x):
        # Общие признаки
        shared_features = self.shared_encoder(x)
        
        # Предсказания для каждой задачи
        age_pred = self.age_head(shared_features)
        gender_pred = self.gender_head(shared_features)
        emotion_pred = self.emotion_head(shared_features)
        glasses_pred = self.glasses_head(shared_features)
        
        return {
            'age': age_pred,
            'gender': gender_pred, 
            'emotion': emotion_pred,
            'glasses': glasses_pred
        }

class MultiTaskLoss(nn.Module):
    def __init__(self, task_weights=None):
        super().__init__()
        
        # Веса для балансировки задач
        self.task_weights = task_weights or {
            'age': 1.0,
            'gender': 1.0,
            'emotion': 1.0,
            'glasses': 1.0
        }
        
        # Loss функции для разных типов задач
        self.mse_loss = nn.MSELoss()  # Для регрессии
        self.ce_loss = nn.CrossEntropyLoss()  # Для классификации
        
    def forward(self, predictions, targets):
        losses = {}
        
        # Age: регрессия
        losses['age'] = self.mse_loss(predictions['age'].squeeze(), 
                                     targets['age'].float())
        
        # Gender: бинарная классификация
        losses['gender'] = self.ce_loss(predictions['gender'], 
                                       targets['gender'])
        
        # Emotion: многоклассовая классификация
        losses['emotion'] = self.ce_loss(predictions['emotion'], 
                                        targets['emotion'])
        
        # Glasses: бинарная классификация
        losses['glasses'] = self.ce_loss(predictions['glasses'], 
                                        targets['glasses'])
        
        # Взвешенная сумма losses
        total_loss = sum(self.task_weights[task] * loss 
                        for task, loss in losses.items())
        
        return total_loss, losses

# Обучение multi-task модели
model = MultiTaskModel()
criterion = MultiTaskLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

for epoch in range(100):
    optimizer.zero_grad()
    
    # Forward pass
    predictions = model(face_images)
    
    # Targets для всех задач
    targets = {
        'age': age_labels,
        'gender': gender_labels,
        'emotion': emotion_labels, 
        'glasses': glasses_labels
    }
    
    # Multi-task loss
    total_loss, individual_losses = criterion(predictions, targets)
    
    # Backward pass
    total_loss.backward()
    optimizer.step()
    
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Total Loss: {total_loss:.4f}")
        for task, loss in individual_losses.items():
            print(f"  {task}: {loss:.4f}")

# Gradient balancing для multi-task learning
class GradientBalancing:
    def __init__(self, model, tasks, alpha=0.12):
        self.model = model
        self.tasks = tasks
        self.alpha = alpha
        
    def balance_gradients(self, losses):
        """Балансирует градиенты между задачами"""
        
        # Вычисляем градиенты для каждой задачи отдельно
        task_gradients = {}
        
        for task, loss in losses.items():
            # Обнуляем градиенты
            self.model.zero_grad()
            
            # Backward для конкретной задачи
            loss.backward(retain_graph=True)
            
            # Сохраняем градиенты
            task_gradients[task] = []
            for param in self.model.parameters():
                if param.grad is not None:
                    task_gradients[task].append(param.grad.clone())
                    
        # Вычисляем средние нормы градиентов
        avg_grad_norms = {}
        for task in self.tasks:
            norms = [torch.norm(grad) for grad in task_gradients[task]]
            avg_grad_norms[task] = torch.mean(torch.stack(norms))
            
        # Вычисляем веса для балансировки
        mean_norm = torch.mean(torch.stack(list(avg_grad_norms.values())))
        
        balanced_weights = {}
        for task in self.tasks:
            relative_rate = avg_grad_norms[task] / mean_norm
            balanced_weights[task] = (relative_rate ** self.alpha)
            
        return balanced_weights

print("\nПреимущества Multi-Task Learning:")
print("1. Shared representations - общие признаки для похожих задач")
print("2. Regularization effect - задачи регуляризуют друг друга")
print("3. Data efficiency - эффективнее использование данных")
print("4. Faster convergence - часто быстрее сходится")

print("\nВызовы:")
print("1. Task interference - задачи могут мешать друг другу")
print("2. Loss balancing - нужно балансировать веса losses")
print("3. Gradient conflicts - противоречивые градиенты")
```

**Углубления:**
1. **Shared vs Task-specific:** Общий encoder + специализированные heads
2. **Loss balancing:** Автоматическая балансировка весов задач
3. **Gradient interference:** Конфликты градиентов между задачами

---

### Вопрос 44: Что такое domain adaptation?

**Теория:** Адаптация модели с одного домена (источника) на другой (целевой)

**Данные:**
```python
# Source domain: синтетические изображения (с метками)
source_images = torch.randn(1000, 3, 32, 32)  # Искусственные данные
source_labels = torch.randint(0, 10, (1000,))  # Метки есть

# Target domain: реальные изображения (без меток или мало)
target_images = torch.randn(800, 3, 32, 32)   # Реальные данные
target_labels_few = torch.randint(0, 10, (50,))  # Мало меток

# Domain shift: различия в распределениях
print("Domain shift может быть из-за:")
print("- Освещения (день/ночь)")
print("- Стиля (фото/рисунки)")  
print("- Качества (HD/low-res)")
print("- Условий съемки (в помещении/на улице)")
```

**Код:**
```python
class DomainAdversarialNetwork(nn.Module):
    """DANN: Domain-Adversarial Neural Networks"""
    
    def __init__(self, input_dim=3*32*32, hidden_dim=512, num_classes=10):
        super().__init__()
        
        # Feature extractor (общий для обоих доменов)
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.5)
        )
        
        # Label classifier (предсказывает классы)
        self.label_classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, num_classes)
        )
        
        # Domain classifier (отличает источник от цели)
        self.domain_classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, 2)  # source=0, target=1
        )
        
    def forward(self, x, alpha=1.0):
        # Извлекаем признаки
        features = self.feature_extractor(x.view(x.size(0), -1))
        
        # Предсказание классов
        class_output = self.label_classifier(features)
        
        # Gradient Reversal Layer для domain classifier
        reversed_features = GradientReversalFunction.apply(features, alpha)
        domain_output = self.domain_classifier(reversed_features)
        
        return class_output, domain_output, features

class GradientReversalFunction(torch.autograd.Function):
    """Обращает градиенты во время backprop"""
    
    @staticmethod
    def forward(ctx, x, alpha):
        ctx.alpha = alpha
        return x.view_as(x)
    
    @staticmethod
    def backward(ctx, grad_output):
        return -ctx.alpha * grad_output, None

def domain_adaptation_loss(class_output, class_labels, domain_output, domain_labels, lambda_domain=1.0):
    """Комбинированная loss для domain adaptation"""
    
    # Classification loss (только для source domain)
    class_loss = nn.CrossEntropyLoss()(class_output, class_labels)
    
    # Domain adversarial loss (для всех данных)
    domain_loss = nn.CrossEntropyLoss()(domain_output, domain_labels)
    
    # Общая loss
    total_loss = class_loss + lambda_domain * domain_loss
    
    return total_loss, class_loss, domain_loss

# Обучение DANN
model = DomainAdversarialNetwork()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(100):
    # Gradient Reversal strength (увеличивается со временем)
    p = epoch / 100
    alpha = 2.0 / (1.0 + np.exp(-10 * p)) - 1
    
    # Подготовка батча
    batch_size = 64
    
    # Source data
    source_batch = source_images[:batch_size]
    source_class_labels = source_labels[:batch_size]
    source_domain_labels = torch.zeros(batch_size, dtype=torch.long)  # 0 = source
    
    # Target data  
    target_batch = target_images[:batch_size]
    target_domain_labels = torch.ones(batch_size, dtype=torch.long)   # 1 = target
    
    # Объединяем source и target
    combined_images = torch.cat([source_batch, target_batch])
    combined_domain_labels = torch.cat([source_domain_labels, target_domain_labels])
    
    # Forward pass
    class_output, domain_output, features = model(combined_images, alpha)
    
    # Loss только для source classification
    source_class_output = class_output[:batch_size]
    total_loss, class_loss, domain_loss = domain_adaptation_loss(
        source_class_output, source_class_labels, 
        domain_output, combined_domain_labels
    )
    
    # Backward pass
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

# Coral Loss для feature alignment
def coral_loss(source_features, target_features):
    """Correlation Alignment Loss"""
    
    # Вычисляем ковариационные матрицы
    source_cov = torch.cov(source_features.T)
    target_cov = torch.cov(target_features.T)
    
    # Frobenius norm разности
    loss = torch.norm(source_cov - target_cov, p='fro') ** 2
    loss = loss / (4 * source_features.size(1) ** 2)
    
    return loss

# Maximum Mean Discrepancy (MMD)
def mmd_loss(source_features, target_features, kernel='rbf', gamma=1.0):
    """MMD для выравнивания распределений"""
    
    def gaussian_kernel(x, y, gamma):
        x_norm = (x ** 2).sum(1).view(-1, 1)
        y_norm = (y ** 2).sum(1).view(1, -1)
        dist = x_norm + y_norm - 2.0 * torch.mm(x, y.transpose(0, 1))
        return torch.exp(-gamma * dist)
    
    if kernel == 'rbf':
        kernel_func = lambda x, y: gaussian_kernel(x, y, gamma)
    else:
        kernel_func = lambda x, y: torch.mm(x, y.transpose(0, 1))
    
    # MMD^2 = K(X,X) + K(Y,Y) - 2K(X,Y)
    xx = kernel_func(source_features, source_features)
    yy = kernel_func(target_features, target_features)
    xy = kernel_func(source_features, target_features)
    
    mmd = xx.mean() + yy.mean() - 2 * xy.mean()
    return mmd

# Unsupervised Domain Adaptation
class UnsupervisedDA(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.feature_extractor = base_model.feature_extractor
        self.classifier = base_model.label_classifier
        
    def forward(self, x):
        features = self.feature_extractor(x.view(x.size(0), -1))
        logits = self.classifier(features)
        return logits, features

# Pseudo-labeling для target domain
def pseudo_labeling(model, target_data, confidence_threshold=0.9):
    """Генерирует псевдо-метки для target данных"""
    
    model.eval()
    pseudo_labels = []
    confident_indices = []
    
    with torch.no_grad():
        logits, _ = model(target_data)
        probs = torch.softmax(logits, dim=1)
        
        # Берём только уверенные предсказания
        max_probs, predictions = torch.max(probs, dim=1)
        confident_mask = max_probs > confidence_threshold
        
        confident_indices = torch.where(confident_mask)[0]
        pseudo_labels = predictions[confident_mask]
    
    return confident_indices, pseudo_labels

# Self-training loop
uda_model = UnsupervisedDA(model)
optimizer = torch.optim.Adam(uda_model.parameters(), lr=1e-4)

for epoch in range(50):
    # Обучение на source данных
    source_logits, source_features = uda_model(source_images[:100])
    source_loss = nn.CrossEntropyLoss()(source_logits, source_labels[:100])
    
    # Генерация псевдо-меток для target
    if epoch > 10:  # Начинаем после некоторого обучения
        confident_indices, pseudo_labels = pseudo_labeling(uda_model, target_images)
        
        if len(confident_indices) > 0:
            # Обучение на уверенных псевдо-метках
            confident_target_data = target_images[confident_indices]
            target_logits, target_features = uda_model(confident_target_data)
            pseudo_loss = nn.CrossEntropyLoss()(target_logits, pseudo_labels)
            
            # Добавляем CORAL loss для feature alignment
            coral = coral_loss(source_features, target_features)
            
            total_loss = source_loss + 0.5 * pseudo_loss + 0.1 * coral
        else:
            total_loss = source_loss
    else:
        total_loss = source_loss
    
    optimizer.zero_grad()
    total_loss.backward()
    optimizer.step()

**Углубления:**
1. **Domain shift:** Различия в распределениях между source и target
2. **Feature alignment:** Выравнивание представлений через CORAL, MMD
3. **Adversarial adaptation:** Обучение domain-invariant features

---

### Вопрос 45: Объясните neural architecture search (NAS)

**Теория:** Автоматический поиск оптимальной архитектуры нейросети

**Данные:**
```python
# Search space: возможные операции для NAS
operations = [
    'conv_3x3',
    'conv_5x5', 
    'conv_1x1',
    'max_pool_3x3',
    'avg_pool_3x3',
    'skip_connect',
    'sep_conv_3x3',  # Separable convolution
    'dil_conv_3x3',  # Dilated convolution
    'none'           # No operation
]

# Параметры для поиска
num_layers = 8
num_ops_per_layer = len(operations)
input_channels = 32
```

**Код:**
```python
class SearchableCell(nn.Module):
    """Ячейка с поисковыми архитектурными параметрами"""
    
    def __init__(self, input_channels, output_channels, operations):
        super().__init__()
        self.operations = nn.ModuleList()
        
        # Создаём все возможные операции
        for op_name in operations:
            if op_name == 'conv_3x3':
                op = nn.Conv2d(input_channels, output_channels, 3, padding=1)
            elif op_name == 'conv_5x5':
                op = nn.Conv2d(input_channels, output_channels, 5, padding=2)
            elif op_name == 'conv_1x1':
                op = nn.Conv2d(input_channels, output_channels, 1)
            elif op_name == 'max_pool_3x3':
                op = nn.Sequential(
                    nn.MaxPool2d(3, stride=1, padding=1),
                    nn.Conv2d(input_channels, output_channels, 1)
                )
            elif op_name == 'skip_connect':
                op = nn.Identity() if input_channels == output_channels else \
                     nn.Conv2d(input_channels, output_channels, 1)
            elif op_name == 'none':
                op = Zero()
            else:
                op = nn.Conv2d(input_channels, output_channels, 3, padding=1)
                
            self.operations.append(op)
        
        # Архитектурные параметры (веса для каждой операции)
        self.arch_parameters = nn.Parameter(torch.randn(len(operations)))
        
    def forward(self, x):
        # Softmax для получения вероятностей операций
        weights = torch.softmax(self.arch_parameters, dim=0)
        
        # Взвешенная сумма всех операций
        output = sum(w * op(x) for w, op in zip(weights, self.operations))
        
        return output

class Zero(nn.Module):
    """Операция "ничего не делать" """
    def forward(self, x):
        return torch.zeros_like(x)

class DifferentiableNAS(nn.Module):
    """DARTS: Differentiable Architecture Search"""
    
    def __init__(self, num_classes=10, num_cells=8, initial_channels=32):
        super().__init__()
        
        self.stem = nn.Sequential(
            nn.Conv2d(3, initial_channels, 3, padding=1),
            nn.BatchNorm2d(initial_channels),
            nn.ReLU()
        )
        
        # Поисковые ячейки
        self.cells = nn.ModuleList()
        channels = initial_channels
        
        for i in range(num_cells):
            cell = SearchableCell(channels, channels, operations)
            self.cells.append(cell)
            
        # Классификатор
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.classifier = nn.Linear(channels, num_classes)
        
    def forward(self, x):
        x = self.stem(x)
        
        for cell in self.cells:
            x = cell(x)
            
        x = self.global_pool(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        
        return x
    
    def get_architecture(self):
        """Возвращает найденную архитектуру"""
        arch = []
        for cell in self.cells:
            # Берём операцию с максимальным весом
            best_op_idx = torch.argmax(cell.arch_parameters)
            best_op = operations[best_op_idx]
            arch.append(best_op)
        return arch

# Evolutionary NAS
class EvolutionaryNAS:
    """Эволюционный поиск архитектур"""
    
    def __init__(self, population_size=50, num_generations=100):
        self.population_size = population_size
        self.num_generations = num_generations
        
    def random_architecture(self, num_layers=8):
        """Генерирует случайную архитектуру"""
        arch = []
        for _ in range(num_layers):
            op = np.random.choice(operations)
            arch.append(op)
        return arch
    
    def mutate_architecture(self, arch, mutation_rate=0.1):
        """Мутирует архитектуру"""
        mutated = arch.copy()
        for i in range(len(mutated)):
            if np.random.random() < mutation_rate:
                mutated[i] = np.random.choice(operations)
        return mutated
    
    def crossover(self, parent1, parent2):
        """Скрещивает две архитектуры"""
        crossover_point = np.random.randint(1, len(parent1))
        child1 = parent1[:crossover_point] + parent2[crossover_point:]
        child2 = parent2[:crossover_point] + parent1[crossover_point:]
        return child1, child2
    
    def evaluate_architecture(self, arch, train_loader, val_loader, epochs=10):
        """Оценивает архитектуру"""
        # Строим модель по архитектуре
        model = self.build_model_from_arch(arch)
        
        # Быстрое обучение
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
        model.train()
        
        for epoch in range(epochs):
            for batch_idx, (data, target) in enumerate(train_loader):
                if batch_idx > 10:  # Ограничиваем для скорости
                    break
                    
                optimizer.zero_grad()
                output = model(data)
                loss = nn.CrossEntropyLoss()(output, target)
                loss.backward()
                optimizer.step()
        
        # Валидация
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                output = model(data)
                _, predicted = torch.max(output, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
                
                if total > 1000:  # Ограничиваем для скорости
                    break
        
        accuracy = correct / total
        return accuracy
    
    def search(self, train_loader, val_loader):
        """Основной цикл поиска"""
        
        # Инициализация популяции
        population = [self.random_architecture() for _ in range(self.population_size)]
        
        for generation in range(self.num_generations):
            # Оценка популяции
            fitness_scores = []
            for arch in population:
                score = self.evaluate_architecture(arch, train_loader, val_loader)
                fitness_scores.append(score)
            
            # Сортировка по fitness
            sorted_indices = np.argsort(fitness_scores)[::-1]
            population = [population[i] for i in sorted_indices]
            fitness_scores = [fitness_scores[i] for i in sorted_indices]
            
            print(f"Generation {generation}, Best fitness: {fitness_scores[0]:.4f}")
            
            # Селекция и размножение
            elite_size = self.population_size // 4
            elite = population[:elite_size]
            
            new_population = elite.copy()
            
            # Генерация потомков
            while len(new_population) < self.population_size:
                # Турнирная селекция
                parent1 = self.tournament_selection(population, fitness_scores)
                parent2 = self.tournament_selection(population, fitness_scores)
                
                # Скрещивание
                child1, child2 = self.crossover(parent1, parent2)
                
                # Мутация
                child1 = self.mutate_architecture(child1)
                child2 = self.mutate_architecture(child2)
                
                new_population.extend([child1, child2])
            
            population = new_population[:self.population_size]
        
        # Возвращаем лучшую архитектуру
        final_scores = [self.evaluate_architecture(arch, train_loader, val_loader, epochs=20) 
                       for arch in population[:5]]
        best_arch_idx = np.argmax(final_scores)
        
        return population[best_arch_idx], final_scores[best_arch_idx]

# Reinforcement Learning NAS
class RLController(nn.Module):
    """LSTM контроллер для генерации архитектур"""
    
    def __init__(self, num_operations, hidden_size=100):
        super().__init__()
        self.num_operations = num_operations
        self.hidden_size = hidden_size
        
        # LSTM для генерации последовательности операций
        self.lstm = nn.LSTM(num_operations, hidden_size, batch_first=True)
        self.classifier = nn.Linear(hidden_size, num_operations)
        
    def forward(self, sequence_length):
        """Генерирует архитектуру как последовательность операций"""
        batch_size = 1
        
        # Начальный вход
        inputs = torch.zeros(batch_size, 1, self.num_operations)
        hidden = self.init_hidden(batch_size)
        
        outputs = []
        for _ in range(sequence_length):
            output, hidden = self.lstm(inputs, hidden)
            logits = self.classifier(output)
            
            # Сэмплируем следующую операцию
            probs = torch.softmax(logits, dim=-1)
            action = torch.multinomial(probs.squeeze(), 1)
            
            # Следующий вход - one-hot encoding выбранной операции
            inputs = torch.zeros(batch_size, 1, self.num_operations)
            inputs[0, 0, action] = 1
            
            outputs.append(action.item())
            
        return outputs

# Progressive NAS
def progressive_nas_search():
    """Прогрессивный поиск: начинаем с простых архитектур"""
    
    # Этап 1: поиск среди простых архитектур
    simple_search_space = ['conv_3x3', 'conv_1x1', 'skip_connect']
    
    # Этап 2: добавляем более сложные операции
    extended_search_space = simple_search_space + ['conv_5x5', 'sep_conv_3x3']
    
    # Этап 3: полный search space
    full_search_space = operations
    
    print("Progressive NAS этапы:")
    print("1. Простые операции:", simple_search_space)
    print("2. Расширенные операции:", extended_search_space)  
    print("3. Полный search space:", full_search_space)

**Углубления:**
1. **Search space:** Определение возможных операций и связей
2. **Search strategy:** Как исследовать пространство (градиенты, эволюция, RL)
3. **Performance estimation:** Как быстро оценить архитектуру без полного обучения

---

## 5. Практическое применение

### Вопрос 46: Как развернуть модель в production?

**Теория:** Оптимизация и развертывание модели для реального использования

**Данные:**
```python
# Обученная модель
class ProductionModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.backbone = models.resnet18(pretrained=True)
        self.backbone.fc = nn.Linear(512, 10)
        
    def forward(self, x):
        return self.backbone(x)

# Загружаем обученную модель
model = ProductionModel()
model.load_state_dict(torch.load('trained_model.pth'))
model.eval()

# Тестовые данные
test_input = torch.randn(1, 3, 224, 224)
```

**Код:**
```python
# 1. ONNX экспорт для кроссплатформенности
import torch.onnx

def export_to_onnx(model, sample_input, onnx_path):
    """Экспорт в ONNX формат"""
    model.eval()
    
    torch.onnx.export(
        model,
        sample_input,
        onnx_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={'input': {0: 'batch_size'},
                     'output': {0: 'batch_size'}}
    )
    
    print(f"Model exported to {onnx_path}")

# Экспорт
export_to_onnx(model, test_input, 'model.onnx')

# 2. TorchScript для PyTorch deployment
def export_to_torchscript(model, sample_input):
    """TorchScript для PyTorch Serve"""
    model.eval()
    
    # Tracing (записывает граф вычислений)
    traced_model = torch.jit.trace(model, sample_input)
    traced_model.save('model_traced.pt')
    
    # Scripting (анализирует код)
    scripted_model = torch.jit.script(model)
    scripted_model.save('model_scripted.pt')
    
    return traced_model, scripted_model

traced, scripted = export_to_torchscript(model, test_input)

# 3. Quantization для ускорения inference
def quantize_model(model, sample_input):
    """Квантование модели"""
    model.eval()
    
    # Dynamic quantization
    quantized_dynamic = torch.quantization.quantize_dynamic(
        model, {nn.Linear}, dtype=torch.qint8
    )
    
    # Static quantization (нужна калибровка)
    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
    prepared_model = torch.quantization.prepare(model)
    
    # Калибровка на representative данных
    with torch.no_grad():
        for _ in range(100):  # Калибровочные данные
            calibration_input = torch.randn(1, 3, 224, 224)
            prepared_model(calibration_input)
    
    quantized_static = torch.quantization.convert(prepared_model)
    
    return quantized_dynamic, quantized_static

# 4. Model Serving с FastAPI
from fastapi import FastAPI, File, UploadFile
from PIL import Image
import io
import torchvision.transforms as transforms

app = FastAPI()

# Загружаем модель при старте сервера
loaded_model = ProductionModel()
loaded_model.load_state_dict(torch.load('trained_model.pth', map_location='cpu'))
loaded_model.eval()

# Preprocessing pipeline
transform = transforms.Compose([
    transforms.Resize(224),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                        std=[0.229, 0.224, 0.225])
])

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    """API endpoint для предсказаний"""
    
    # Загружаем изображение
    image_bytes = await file.read()
    image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
    
    # Preprocessing
    input_tensor = transform(image).unsqueeze(0)
    
    # Inference
    with torch.no_grad():
        logits = loaded_model(input_tensor)
        probabilities = torch.softmax(logits, dim=1)
        predicted_class = torch.argmax(probabilities, dim=1).item()
        confidence = probabilities[0][predicted_class].item()
    
    return {
        "predicted_class": predicted_class,
        "confidence": confidence,
        "probabilities": probabilities[0].tolist()
    }

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy"}

# 5. Batch inference для высокой throughput
class BatchInferenceService:
    def __init__(self, model, batch_size=32, max_wait_time=0.1):
        self.model = model
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.pending_requests = []
        
    async def predict_batch(self, inputs):
        """Батчевый inference"""
        
        # Собираем batch
        if len(inputs) < self.batch_size:
            # Ждём больше запросов или timeout
            await asyncio.sleep(self.max_wait_time)
        
        # Формируем batch
        batch_tensor = torch.stack(inputs[:self.batch_size])
        
        # Inference
        with torch.no_grad():
            batch_predictions = self.model(batch_tensor)
        
        return batch_predictions

# 6. Model caching и optimization
class OptimizedInferenceService:
    def __init__(self, model_path):
        # Загружаем квантованную модель
        self.model = torch.jit.load(model_path, map_location='cpu')
        self.model.eval()
        
        # LRU cache для результатов
        from functools import lru_cache
        self.cached_predict = lru_cache(maxsize=1000)(self._predict)
        
    def _predict(self, input_hash):
        """Внутренний метод предсказания"""
        # В реальности здесь будет inference
        pass
        
    def predict(self, input_tensor):
        """Предсказание с кэшированием"""
        # Создаём хэш входа для кэширования
        input_hash = hash(input_tensor.data.tobytes())
        return self.cached_predict(input_hash)

# 7. Мониторинг production модели
class ModelMonitor:
    def __init__(self):
        self.prediction_count = 0
        self.inference_times = []
        self.confidence_scores = []
        
    def log_prediction(self, inference_time, confidence, prediction):
        """Логирование метрик"""
        self.prediction_count += 1
        self.inference_times.append(inference_time)
        self.confidence_scores.append(confidence)
        
        # Алерт при низкой уверенности
        if confidence < 0.5:
            print(f"Low confidence prediction: {confidence:.3f}")
            
        # Алерт при медленном inference
        if inference_time > 1.0:  # 1 секунда
            print(f"Slow inference: {inference_time:.3f}s")
    
    def get_metrics(self):
        """Возвращает метрики для dashboard"""
        return {
            "total_predictions": self.prediction_count,
            "avg_inference_time": np.mean(self.inference_times),
            "avg_confidence": np.mean(self.confidence_scores),
            "low_confidence_rate": sum(1 for c in self.confidence_scores if c < 0.5) / len(self.confidence_scores)
        }

# 8. A/B Testing для моделей
class ABTestingService:
    def __init__(self, model_a, model_b, traffic_split=0.5):
        self.model_a = model_a
        self.model_b = model_b
        self.traffic_split = traffic_split
        self.metrics_a = []
        self.metrics_b = []
        
    def predict(self, input_data, user_id):
        """Роутинг запросов между моделями"""
        
        # Детерминированное разделение по user_id
        use_model_a = hash(user_id) % 100 < (self.traffic_split * 100)
        
        if use_model_a:
            prediction = self.model_a(input_data)
            model_version = "A"
        else:
            prediction = self.model_b(input_data)
            model_version = "B"
            
        return prediction, model_version
    
    def log_outcome(self, user_id, model_version, outcome):
        """Логирование результатов для анализа"""
        if model_version == "A":
            self.metrics_a.append(outcome)
        else:
            self.metrics_b.append(outcome)

**Углубления:**
1. **Форматы моделей:** ONNX для interoperability, TensorRT для NVIDIA GPU
2. **Оптимизация:** Quantization, pruning, knowledge distillation
3. **Serving:** Батчирование, кэширование, load balancing

---

### Вопрос 47: Как мониторить ML модель в production?

**Теория:** Отслеживание performance и quality модели в реальных условиях

**Данные:**
```python
import time
import numpy as np
from collections import deque
from datetime import datetime
import logging

# Настройка логирования
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Симуляция production данных
production_predictions = torch.randn(1000, 10)  # Логиты модели
production_labels = torch.randint(0, 10, (800,))  # Не все метки доступны сразу
prediction_timestamps = [datetime.now() for _ in range(1000)]
```

**Код:**
```python
class ModelPerformanceMonitor:
    """Мониторинг performance метрик модели"""
    
    def __init__(self, window_size=1000):
        self.window_size = window_size
        self.predictions = deque(maxlen=window_size)
        self.ground_truths = deque(maxlen=window_size)
        self.timestamps = deque(maxlen=window_size)
        self.inference_times = deque(maxlen=window_size)
        
    def log_prediction(self, prediction, ground_truth=None, inference_time=None):
        """Логирует предсказание"""
        self.predictions.append(prediction)
        self.ground_truths.append(ground_truth)
        self.timestamps.append(datetime.now())
        
        if inference_time:
            self.inference_times.append(inference_time)
    
    def calculate_metrics(self):
        """Вычисляет актуальные метрики"""
        
        # Фильтруем данные с ground truth
        valid_pairs = [(p, gt) for p, gt in zip(self.predictions, self.ground_truths) 
                      if gt is not None]
        
        if not valid_pairs:
            return {}
            
        predictions, ground_truths = zip(*valid_pairs)
        predictions = torch.stack(predictions)
        ground_truths = torch.tensor(ground_truths)
        
        # Accuracy
        predicted_classes = torch.argmax(predictions, dim=1)
        accuracy = (predicted_classes == ground_truths).float().mean().item()
        
        # Confidence distribution
        confidences = torch.softmax(predictions, dim=1)
        max_confidences = torch.max(confidences, dim=1)[0]
        
        # Average inference time
        avg_inference_time = np.mean(self.inference_times) if self.inference_times else 0
        
        return {
            'accuracy': accuracy,
            'avg_confidence': max_confidences.mean().item(),
            'low_confidence_rate': (max_confidences < 0.8).float().mean().item(),
            'avg_inference_time_ms': avg_inference_time * 1000,
            'total_predictions': len(self.predictions),
            'labeled_predictions': len(valid_pairs)
        }

class DataDriftDetector:
    """Детектор data drift"""
    
    def __init__(self, reference_features, threshold=0.05):
        self.reference_features = reference_features  # Training данные
        self.threshold = threshold
        self.current_window = deque(maxlen=1000)
        
    def add_sample(self, features):
        """Добавляет новый sample"""
        self.current_window.append(features)
        
    def detect_drift(self, method='ks_test'):
        """Детектирует data drift"""
        
        if len(self.current_window) < 100:
            return False, 0.0
            
        current_features = torch.stack(list(self.current_window))
        
        if method == 'ks_test':
            # Kolmogorov-Smirnov test для каждого признака
            from scipy.stats import ks_2samp
            
            p_values = []
            for i in range(current_features.shape[1]):
                ref_feature = self.reference_features[:, i].numpy()
                curr_feature = current_features[:, i].numpy()
                
                _, p_value = ks_2samp(ref_feature, curr_feature)
                p_values.append(p_value)
            
            min_p_value = min(p_values)
            drift_detected = min_p_value < self.threshold
            
            return drift_detected, min_p_value
            
        elif method == 'mmd':
            # Maximum Mean Discrepancy
            return self._mmd_drift_test(current_features)
    
    def _mmd_drift_test(self, current_features):
        """MMD based drift detection"""
        
        def gaussian_kernel(x, y, sigma=1.0):
            x_norm = (x ** 2).sum(1).view(-1, 1)
            y_norm = (y ** 2).sum(1).view(1, -1)
            dist = x_norm + y_norm - 2.0 * torch.mm(x, y.transpose(0, 1))
            return torch.exp(-dist / (2 * sigma ** 2))
        
        # Sample равного размера
        n_samples = min(len(self.reference_features), len(current_features))
        ref_sample = self.reference_features[:n_samples]
        curr_sample = current_features[:n_samples]
        
        # MMD²
        xx = gaussian_kernel(ref_sample, ref_sample).mean()
        yy = gaussian_kernel(curr_sample, curr_sample).mean()
        xy = gaussian_kernel(ref_sample, curr_sample).mean()
        
        mmd_score = xx + yy - 2 * xy
        drift_detected = mmd_score > self.threshold
        
        return drift_detected, mmd_score.item()

class ModelDriftDetector:
    """Детектор model drift (деградации качества)"""
    
    def __init__(self, baseline_accuracy, degradation_threshold=0.05):
        self.baseline_accuracy = baseline_accuracy
        self.degradation_threshold = degradation_threshold
        self.recent_accuracies = deque(maxlen=100)
        
    def update_accuracy(self, current_accuracy):
        """Обновляет текущую accuracy"""
        self.recent_accuracies.append(current_accuracy)
        
    def detect_degradation(self):
        """Детектирует деградацию модели"""
        
        if len(self.recent_accuracies) < 10:
            return False, 0.0
            
        recent_avg_accuracy = np.mean(self.recent_accuracies)
        degradation = self.baseline_accuracy - recent_avg_accuracy
        
        is_degraded = degradation > self.degradation_threshold
        
        return is_degraded, degradation

class PredictionMonitor:
    """Мониторинг аномалий в предсказаниях"""
    
    def __init__(self):
        self.confidence_history = deque(maxlen=10000)
        self.class_distribution = {}
        
    def log_prediction(self, prediction_logits, predicted_class):
        """Логирует предсказание"""
        
        # Confidence
        confidence = torch.softmax(prediction_logits, dim=0)
        max_confidence = torch.max(confidence).item()
        self.confidence_history.append(max_confidence)
        
        # Class distribution
        if predicted_class in self.class_distribution:
            self.class_distribution[predicted_class] += 1
        else:
            self.class_distribution[predicted_class] = 1
    
    def detect_anomalies(self):
        """Детектирует аномалии в предсказаниях"""
        alerts = []
        
        # Low confidence trend
        if len(self.confidence_history) >= 100:
            recent_confidence = list(self.confidence_history)[-100:]
            avg_confidence = np.mean(recent_confidence)
            
            if avg_confidence < 0.7:
                alerts.append(f"Low average confidence: {avg_confidence:.3f}")
        
        # Class imbalance
        total_predictions = sum(self.class_distribution.values())
        if total_predictions > 0:
            for class_id, count in self.class_distribution.items():
                proportion = count / total_predictions
                if proportion > 0.8:
                    alerts.append(f"Class {class_id} dominance: {proportion:.3f}")
        
        return alerts

class AlertingSystem:
    """Система алертов"""
    
    def __init__(self):
        self.alert_history = deque(maxlen=1000)
        
    def send_alert(self, alert_type, message, severity='warning'):
        """Отправляет алерт"""
        
        alert = {
            'timestamp': datetime.now(),
            'type': alert_type,
            'message': message,
            'severity': severity
        }
        
        self.alert_history.append(alert)
        
        # В реальности здесь отправка в Slack/email/PagerDuty
        logger.warning(f"ALERT [{severity.upper()}] {alert_type}: {message}")
        
        return alert

class ProductionMonitoringDashboard:
    """Главный monitoring dashboard"""
    
    def __init__(self, model, reference_data):
        self.model = model
        
        # Компоненты мониторинга
        self.performance_monitor = ModelPerformanceMonitor()
        self.data_drift_detector = DataDriftDetector(reference_data)
        self.model_drift_detector = ModelDriftDetector(baseline_accuracy=0.95)
        self.prediction_monitor = PredictionMonitor()
        self.alerting_system = AlertingSystem()
        
        # Метрики для dashboard
        self.dashboard_metrics = {}
        
    def process_prediction(self, input_data, ground_truth=None):
        """Обрабатывает одно предсказание"""
        
        start_time = time.time()
        
        # Inference
        with torch.no_grad():
            prediction_logits = self.model(input_data)
            predicted_class = torch.argmax(prediction_logits).item()
            
        inference_time = time.time() - start_time
        
        # Логирование
        self.performance_monitor.log_prediction(
            prediction_logits, ground_truth, inference_time
        )
        
        self.data_drift_detector.add_sample(input_data.flatten())
        self.prediction_monitor.log_prediction(prediction_logits, predicted_class)
        
        return prediction_logits, predicted_class
    
    def run_monitoring_checks(self):
        """Запускает все проверки мониторинга"""
        
        # Performance metrics
        performance_metrics = self.performance_monitor.calculate_metrics()
        
        # Data drift detection
        data_drift_detected, drift_score = self.data_drift_detector.detect_drift()
        
        # Model degradation
        if 'accuracy' in performance_metrics:
            self.model_drift_detector.update_accuracy(performance_metrics['accuracy'])
            model_degraded, degradation = self.model_drift_detector.detect_degradation()
        else:
            model_degraded, degradation = False, 0.0
        
        # Prediction anomalies
        prediction_alerts = self.prediction_monitor.detect_anomalies()
        
        # Generate alerts
        if data_drift_detected:
            self.alerting_system.send_alert(
                'data_drift', 
                f'Data drift detected: score={drift_score:.4f}',
                'warning'
            )
        
        if model_degraded:
            self.alerting_system.send_alert(
                'model_degradation',
                f'Model accuracy degraded by {degradation:.4f}',
                'critical'
            )
        
        for alert in prediction_alerts:
            self.alerting_system.send_alert('prediction_anomaly', alert, 'warning')
        
        # Update dashboard metrics
        self.dashboard_metrics.update({
            'performance': performance_metrics,
            'data_drift': {'detected': data_drift_detected, 'score': drift_score},
            'model_degradation': {'detected': model_degraded, 'degradation': degradation},
            'prediction_alerts': prediction_alerts,
            'timestamp': datetime.now()
        })
        
        return self.dashboard_metrics
    
    def get_dashboard_data(self):
        """Возвращает данные для dashboard"""
        return self.dashboard_metrics

# Пример использования
# model = load_production_model()
# reference_data = load_training_features()
# 
# monitor = ProductionMonitoringDashboard(model, reference_data)
# 
# # В production loop
# for input_batch, labels in production_stream:
#     prediction, predicted_class = monitor.process_prediction(input_batch, labels)
#     
#     # Периодические проверки
#     if random.random() < 0.01:  # 1% времени
#         dashboard_data = monitor.run_monitoring_checks()
#         print(f"Monitoring status: {dashboard_data}")

**Углубления:**
1. **Model drift:** Деградация качества из-за изменения данных
2. **Data drift:** Изменение распределения входных данных
3. **Alerting:** Автоматические уведомления при аномалиях

---

### Вопрос 48: Как обрабатывать большие датасеты?

**Теория:** Техники для работы с данными, не помещающимися в память

**Данные:**
```python
# Симуляция большого датасета
import os
from torch.utils.data import Dataset, DataLoader, IterableDataset
import h5py
import pandas as pd

# Параметры большого датасета
total_samples = 10_000_000  # 10M примеров
feature_dim = 1024
batch_size = 256
num_workers = 4
```

**Код:**
```python
class LargeDataset(Dataset):
    """Dataset для больших данных с lazy loading"""
    
    def __init__(self, data_path, transform=None):
        self.data_path = data_path
        self.transform = transform
        
        # Загружаем только метаданные
        with h5py.File(data_path, 'r') as f:
            self.length = f['data'].shape[0]
            self.feature_dim = f['data'].shape[1]
            
    def __len__(self):
        return self.length
    
    def __getitem__(self, idx):
        # Загружаем только нужный элемент
        with h5py.File(self.data_path, 'r') as f:
            data = f['data'][idx]
            label = f['labels'][idx]
            
        # Конвертируем в torch tensors
        data = torch.from_numpy(data).float()
        label = torch.from_numpy(label).long()
        
        if self.transform:
            data = self.transform(data)
            
        return data, label

class StreamingDataset(IterableDataset):
    """Streaming dataset для онлайн обработки"""
    
    def __init__(self, file_pattern, chunk_size=1000):
        self.file_pattern = file_pattern
        self.chunk_size = chunk_size
        self.files = sorted(glob.glob(file_pattern))
        
    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        
        if worker_info is None:
            # Single process
            iter_start = 0
            iter_end = len(self.files)
        else:
            # Multi-process data loading
            per_worker = len(self.files) // worker_info.num_workers
            iter_start = worker_info.id * per_worker
            iter_end = min(iter_start + per_worker, len(self.files))
        
        for file_idx in range(iter_start, iter_end):
            yield from self._process_file(self.files[file_idx])
    
    def _process_file(self, file_path):
        """Обрабатывает один файл по частям"""
        df = pd.read_csv(file_path)
        
        for i in range(0, len(df), self.chunk_size):
            chunk = df.iloc[i:i + self.chunk_size]
            
            for _, row in chunk.iterrows():
                data = torch.tensor(row[:-1].values, dtype=torch.float32)
                label = torch.tensor(row[-1], dtype=torch.long)
                yield data, label

# Эффективный DataLoader
def create_efficient_dataloader(dataset, batch_size, num_workers=4):
    """Создаёт оптимизированный DataLoader"""
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=True,        # Ускоряет GPU transfer
        persistent_workers=True, # Переиспользует workers
        prefetch_factor=2,      # Prefetch batches
        shuffle=True
    )

# Memory mapping для очень больших файлов
class MemoryMappedDataset(Dataset):
    """Dataset с memory mapping для экономии RAM"""
    
    def __init__(self, data_file, labels_file):
        # Memory-mapped arrays
        self.data = np.memmap(data_file, dtype=np.float32, mode='r')
        self.labels = np.memmap(labels_file, dtype=np.int64, mode='r')
        
        # Reshape данных
        self.data = self.data.reshape(-1, feature_dim)
        self.length = len(self.data)
        
    def __len__(self):
        return self.length
    
    def __getitem__(self, idx):
        # Данные загружаются только при обращении
        data = torch.from_numpy(self.data[idx].copy())
        label = torch.from_numpy(np.array(self.labels[idx]))
        return data, label

# Distributed data loading
def setup_distributed_dataloader(dataset, world_size, rank):
    """Distributed DataLoader для multi-GPU обучения"""
    
    sampler = torch.utils.data.distributed.DistributedSampler(
        dataset,
        num_replicas=world_size,
        rank=rank,
        shuffle=True
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size // world_size,  # Делим batch между GPU
        sampler=sampler,
        num_workers=num_workers,
        pin_memory=True
    )
    
    return dataloader, sampler

# Gradient accumulation для больших effective batch sizes
class LargeScaleTrainer:
    """Trainer для больших batch sizes через accumulation"""
    
    def __init__(self, model, optimizer, accumulation_steps=4):
        self.model = model
        self.optimizer = optimizer
        self.accumulation_steps = accumulation_steps
        
    def train_step(self, dataloader):
        """Один шаг обучения с gradient accumulation"""
        
        self.model.train()
        total_loss = 0
        
        for batch_idx, (data, target) in enumerate(dataloader):
            # Forward pass
            output = self.model(data)
            loss = nn.CrossEntropyLoss()(output, target)
            
            # Нормализуем loss на количество шагов накопления
            loss = loss / self.accumulation_steps
            
            # Backward pass
            loss.backward()
            
            total_loss += loss.item()
            
            # Обновляем веса каждые accumulation_steps
            if (batch_idx + 1) % self.accumulation_steps == 0:
                self.optimizer.step()
                self.optimizer.zero_grad()
        
        return total_loss

# Mixed precision для экономии памяти
class MixedPrecisionTrainer:
    """Trainer с mixed precision (fp16/fp32)"""
    
    def __init__(self, model, optimizer):
        self.model = model
        self.optimizer = optimizer
        self.scaler = torch.cuda.amp.GradScaler()
        
    def train_step(self, data, target):
        """Mixed precision training step"""
        
        # Autocast для forward pass
        with torch.cuda.amp.autocast():
            output = self.model(data)
            loss = nn.CrossEntropyLoss()(output, target)
        
        # Scaled backward pass
        self.scaler.scale(loss).backward()
        self.scaler.step(self.optimizer)
        self.scaler.update()
        self.optimizer.zero_grad()
        
        return loss.item()

# Checkpoint система для длительного обучения
class CheckpointManager:
    """Управление checkpoints для восстановления обучения"""
    
    def __init__(self, model, optimizer, save_dir):
        self.model = model
        self.optimizer = optimizer
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
    def save_checkpoint(self, epoch, loss, is_best=False):
        """Сохраняет checkpoint"""
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': loss,
            'timestamp': datetime.now()
        }
        
        # Обычный checkpoint
        checkpoint_path = os.path.join(self.save_dir, f'checkpoint_epoch_{epoch}.pth')
        torch.save(checkpoint, checkpoint_path)
        
        # Лучший checkpoint
        if is_best:
            best_path = os.path.join(self.save_dir, 'best_model.pth')
            torch.save(checkpoint, best_path)
        
        # Последний checkpoint (для восстановления)
        latest_path = os.path.join(self.save_dir, 'latest_checkpoint.pth')
        torch.save(checkpoint, latest_path)
        
        return checkpoint_path
    
    def load_checkpoint(self, checkpoint_path=None):
        """Загружает checkpoint"""
        
        if checkpoint_path is None:
            checkpoint_path = os.path.join(self.save_dir, 'latest_checkpoint.pth')
        
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path)
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            return checkpoint['epoch'], checkpoint['loss']
        
        return 0, float('inf')

# Оптимизация I/O операций
class OptimizedDataPipeline:
    """Оптимизированный pipeline для данных"""
    
    def __init__(self, data_dir, cache_dir=None):
        self.data_dir = data_dir
        self.cache_dir = cache_dir
        
        if cache_dir:
            os.makedirs(cache_dir, exist_ok=True)
    
    def preprocess_and_cache(self, preprocessing_fn):
        """Предобработка с кэшированием"""
        
        cache_file = os.path.join(self.cache_dir, 'preprocessed_data.h5')
        
        if os.path.exists(cache_file):
            print("Loading cached preprocessed data...")
            return h5py.File(cache_file, 'r')
        
        print("Preprocessing data...")
        
        # Обработка данных по частям
        with h5py.File(cache_file, 'w') as cache_f:
            
            first_chunk = True
            total_samples = 0
            
            for chunk_file in os.listdir(self.data_dir):
                chunk_data = pd.read_csv(os.path.join(self.data_dir, chunk_file))
                processed_chunk = preprocessing_fn(chunk_data)
                
                if first_chunk:
                    # Создаём datasets
                    cache_f.create_dataset(
                        'data', 
                        data=processed_chunk,
                        maxshape=(None, processed_chunk.shape[1]),
                        compression='gzip'
                    )
                    first_chunk = False
                else:
                    # Расширяем dataset
                    cache_f['data'].resize((total_samples + len(processed_chunk),) + processed_chunk.shape[1:])
                    cache_f['data'][total_samples:] = processed_chunk
                
                total_samples += len(processed_chunk)
        
        return h5py.File(cache_file, 'r')

# Профилирование для оптимизации
def profile_dataloader(dataloader, num_batches=10):
    """Профилирует DataLoader для поиска узких мест"""
    
    import time
    
    times = []
    
    for batch_idx, (data, target) in enumerate(dataloader):
        if batch_idx >= num_batches:
            break
            
        start_time = time.time()
        
        # Симулируем обработку
        _ = data.mean()
        
        batch_time = time.time() - start_time
        times.append(batch_time)
        
        print(f"Batch {batch_idx}: {batch_time:.4f}s")
    
    print(f"Average batch time: {np.mean(times):.4f}s")
    print(f"Throughput: {batch_size / np.mean(times):.1f} samples/s")

print("Стратегии для больших датасетов:")
print("1. 💾 Lazy loading (загружаем по требованию)")
print("2. 🗂️  Memory mapping (экономим RAM)")
print("3. 🔄 Streaming datasets (онлайн обработка)")
print("4. ⚡ Efficient DataLoader (prefetch, pin_memory)")
print("5. 🚀 Distributed loading (multi-GPU)")
print("6. 💻 Mixed precision (экономим memory)")
print("7. 📊 Gradient accumulation (большие effective batches)")
print("8. 💽 Preprocessing caching (ускоряем повторные загрузки)")