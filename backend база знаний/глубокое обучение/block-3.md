# Ğ‘Ğ»Ğ¾Ğº 3: Ğ¡Ğ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ

**â± Ğ”Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ:** 6-8 Ğ½ĞµĞ´ĞµĞ»ÑŒ  
**ğŸ¯ Ğ¦ĞµĞ»ÑŒ:** ĞÑĞ²Ğ¾Ğ¸Ñ‚ÑŒ CNN Ğ¸ Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ

---

## ğŸ” ĞĞ±Ğ·Ğ¾Ñ€ Ğ±Ğ»Ğ¾ĞºĞ°

Ğ¡Ğ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ (CNN) Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ»Ğ¸ Ñ€ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ² Ğ¾Ğ±Ğ»Ğ°ÑÑ‚Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ. Ğ’ ÑÑ‚Ğ¾Ğ¼ Ğ±Ğ»Ğ¾ĞºĞµ Ğ¼Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ğ¼:
- ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸
- ĞšĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¸ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ CNN
- Transfer Learning Ğ¸ Fine-tuning
- ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ
- Ğ Ğ°Ğ±Ğ¾Ñ‚Ñƒ Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ğ¼Ğ¸ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹

---

## ğŸ“– Ğ“Ğ»Ğ°Ğ²Ğ° 3.1: ĞŸÑ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸
**ğŸ“… ĞĞµĞ´ĞµĞ»Ñ 1**

### Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ

#### Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ ÑĞ²ĞµÑ€Ñ‚ĞºĞ°?

Ğ¡Ğ²ĞµÑ€Ñ‚ĞºĞ° - ÑÑ‚Ğ¾ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµÑ‚ Ğ´Ğ²Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ñ‚Ñ€ĞµÑ‚ÑŒĞµĞ¹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸. Ğ’ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹, ÑĞ²ĞµÑ€Ñ‚ĞºĞ° Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑĞµÑ‚ÑÑ Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ÑĞ¼ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ².

```
ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ° ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸:
(f * g)(t) = âˆ« f(Ï„)g(t-Ï„)dÏ„
```

Ğ”Ğ»Ñ Ğ´Ğ¸ÑĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹):
```
(f * g)[m,n] = âˆ‘âˆ‘ f[i,j] Ã— g[m-i,n-j]
```

#### Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸

```
Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ (5x5):        Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ (3x3):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  0  1  0  1 â”‚            â”‚ 1  0 -1 â”‚
â”‚ 0  1  0  1  0 â”‚    *       â”‚ 1  0 -1 â”‚
â”‚ 1  0  1  0  1 â”‚            â”‚ 1  0 -1 â”‚
â”‚ 0  1  0  1  0 â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ 1  0  1  0  1 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸ (3x3):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ -1  0  1 â”‚
â”‚  1  0 -1 â”‚
â”‚ -1  0  1 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸

**1. Padding (Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ)**
```
Ğ‘ĞµĞ· padding:                   Ğ¡ padding = 1:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  0  1  0  1 â”‚        â”‚ 0  0  0  0  0  0  0 â”‚
â”‚ 0  1  0  1  0 â”‚        â”‚ 0  1  0  1  0  1  0 â”‚
â”‚ 1  0  1  0  1 â”‚   â†’    â”‚ 0  0  1  0  1  0  0 â”‚
â”‚ 0  1  0  1  0 â”‚        â”‚ 0  1  0  1  0  1  0 â”‚
â”‚ 1  0  1  0  1 â”‚        â”‚ 0  0  1  0  1  0  0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ 0  1  0  1  0  1  0 â”‚
                               â”‚ 0  0  0  0  0  0  0 â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
5x5 â†’ 3x3 (ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ğµ)         7x7 â†’ 5x5 (ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ°)
```

**2. Stride (Ğ¨Ğ°Ğ³)**
```
Stride = 1:                    Stride = 2:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [1  0  1] 0  1 â”‚        â”‚ [1  0  1] 0  1 â”‚
â”‚ [0  1  0] 1  0 â”‚        â”‚  0  1  0  1  0 â”‚
â”‚ [1  0  1] 0  1 â”‚        â”‚ [1  0  1] 0  1 â”‚
â”‚  0  1  0  1  0 â”‚        â”‚  0  1  0  1  0 â”‚
â”‚  1  0  1  0  1 â”‚        â”‚  1  0  1  0  1 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“                              â†“
    3x3                            2x2
```

#### ĞšĞ°Ñ€Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (Feature Maps)

```
Ğ’Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ (RGB):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Red Channel   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ R R R R R â”‚   â”‚
â”‚ â”‚ R R R R R â”‚   â”‚
â”‚ â”‚ R R R R R â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                       â”‚
â”‚ Green Channel   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ G G G G G â”‚   â”‚
â”‚ â”‚ G G G G G â”‚   â”‚
â”‚ â”‚ G G G G G â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                       â”‚
â”‚ Blue Channel    â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ B B B B B â”‚   â”‚
â”‚ â”‚ B B B B B â”‚   â”‚
â”‚ â”‚ B B B B B â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                â†“ Ğ¡Ğ²ĞµÑ€Ñ‚ĞºĞ° Ñ 32 Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ğ¼Ğ¸

32 ĞºĞ°Ñ€Ñ‚Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Map1  Map2  Map3  ...  Map30  Map31  Map32 â”‚
â”‚ â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”      â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â” â”‚
â”‚ â”‚ F â”‚ â”‚ F â”‚ â”‚ F â”‚      â”‚ F â”‚  â”‚ F â”‚  â”‚ F â”‚ â”‚
â”‚ â”‚ F â”‚ â”‚ F â”‚ â”‚ F â”‚ ...  â”‚ F â”‚  â”‚ F â”‚  â”‚ F â”‚ â”‚
â”‚ â”‚ F â”‚ â”‚ F â”‚ â”‚ F â”‚      â”‚ F â”‚  â”‚ F â”‚  â”‚ F â”‚ â”‚
â”‚ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜      â””â”€â”€â”€â”˜  â””â”€â”€â”€â”˜  â””â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ĞĞ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¿ÑƒĞ»Ğ¸Ğ½Ğ³Ğ° (Pooling)

**Max Pooling:**
```
Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (4x4):    Max Pooling 2x2:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  3  2  4 â”‚        â”‚ 3  4 â”‚
â”‚ 5  6  7  8 â”‚   â†’    â”‚ 6  8 â”‚
â”‚ 1  2  3  4 â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ 5  6  7  8 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Average Pooling:**
```
Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ° Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (4x4):    Average Pooling 2x2:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1  3  2  4 â”‚        â”‚3.75 5.25â”‚
â”‚ 5  6  7  8 â”‚   â†’    â”‚3.75 5.25â”‚
â”‚ 1  2  3  4 â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ 5  6  7  8 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ

#### Ğ—Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ 1: Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸ Ñ Ğ½ÑƒĞ»Ñ

```python
import numpy as np
import matplotlib.pyplot as plt

def convolution2d(image, kernel, padding=0, stride=1):
    """
    Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ 2D ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸
    """
    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ padding
    if padding > 0:
        image = np.pad(image, ((padding, padding), (padding, padding)), mode='constant')
    
    # Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹
    image_h, image_w = image.shape
    kernel_h, kernel_w = kernel.shape
    
    # Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
    output_h = (image_h - kernel_h) // stride + 1
    output_w = (image_w - kernel_w) // stride + 1
    
    # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑĞ¸Ğ²Ğ°
    output = np.zeros((output_h, output_w))
    
    # Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ ÑĞ²ĞµÑ€Ñ‚ĞºÑƒ
    for i in range(0, output_h):
        for j in range(0, output_w):
            y_start = i * stride
            y_end = y_start + kernel_h
            x_start = j * stride
            x_end = x_start + kernel_w
            
            # ĞŸĞ¾ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ½Ğ¾Ğµ Ğ¿ĞµÑ€ĞµĞ¼Ğ½Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ ÑÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
            output[i, j] = np.sum(image[y_start:y_end, x_start:x_end] * kernel)
    
    return output

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
if __name__ == "__main__":
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
    image = np.array([
        [1, 0, 1, 0, 1],
        [0, 1, 0, 1, 0],
        [1, 0, 1, 0, 1],
        [0, 1, 0, 1, 0],
        [1, 0, 1, 0, 1]
    ])
    
    # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ²ĞµÑ€Ñ‚Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†
    edge_filter = np.array([
        [1, 0, -1],
        [1, 0, -1],
        [1, 0, -1]
    ])
    
    # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ ÑĞ²ĞµÑ€Ñ‚ĞºÑƒ
    result = convolution2d(image, edge_filter)
    
    print("Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ:")
    print(image)
    print("\nĞ¤Ğ¸Ğ»ÑŒÑ‚Ñ€:")
    print(edge_filter)
    print("\nĞ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸:")
    print(result)
```

#### Ğ—Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ 2: Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²

```python
def create_filters():
    """
    Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
    """
    filters = {}
    
    # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ²ĞµÑ€Ñ‚Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†
    filters['vertical_edge'] = np.array([
        [1, 0, -1],
        [1, 0, -1],
        [1, 0, -1]
    ])
    
    # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ´Ğ»Ñ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ³Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ½Ñ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†
    filters['horizontal_edge'] = np.array([
        [1, 1, 1],
        [0, 0, 0],
        [-1, -1, -1]
    ])
    
    # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ñ€Ğ°Ğ·Ğ¼Ñ‹Ñ‚Ğ¸Ñ
    filters['blur'] = np.array([
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]
    ]) / 9
    
    # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ Ğ¿Ğ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ĞºĞ¾ÑÑ‚Ğ¸
    filters['sharpen'] = np.array([
        [0, -1, 0],
        [-1, 5, -1],
        [0, -1, 0]
    ])
    
    return filters

# Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ²
filters = create_filters()
for name, filter_kernel in filters.items():
    result = convolution2d(image, filter_kernel)
    print(f"\n{name}:")
    print(result)
```

### Ğ”Ğ¾Ğ¼Ğ°ÑˆĞ½ĞµĞµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ

1. **Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ:** Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ:
   - Ğ’Ñ…Ğ¾Ğ´: 224Ã—224, Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€: 5Ã—5, padding=2, stride=1
   - Ğ’Ñ…Ğ¾Ğ´: 128Ã—128, Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€: 3Ã—3, padding=0, stride=2

2. **ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ:** Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ max_pooling2d Ñ Ğ½ÑƒĞ»Ñ

3. **Ğ˜ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¾Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ:** Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ 5 Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ² Ğ¸ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¸Ñ… Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ

---

## ğŸ“– Ğ“Ğ»Ğ°Ğ²Ğ° 3.2: ĞšĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ CNN
**ğŸ“… ĞĞµĞ´ĞµĞ»Ğ¸ 2-3**

### Ğ­Ğ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€ CNN

```
Ğ’Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ Ğ»Ğ¸Ğ½Ğ¸Ñ Ñ€Ğ°Ğ·Ğ²Ğ¸Ñ‚Ğ¸Ñ CNN:
1998      2012      2014      2015      2016
 |         |         |         |         |
LeNet-5   AlexNet   VGGNet   ResNet   DenseNet
 |         |         |         |         |
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ²   Ğ“Ğ»ÑƒĞ±Ğ¸Ğ½Ğ°  ĞÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ
          Ğ² ImageNet         ÑĞ²ÑĞ·Ğ¸
```

### LeNet-5 (1998) - ĞŸĞ¸Ğ¾Ğ½ĞµÑ€ CNN

```
ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° LeNet-5:
Input (32x32x1) â†’ Conv1 (5x5) â†’ Pool1 (2x2) â†’ Conv2 (5x5) â†’ Pool2 (2x2) â†’ FC1 (120) â†’ FC2 (84) â†’ Output (10)
     â†“              â†“           â†“           â†“           â†“         â†“         â†“         â†“
   32Ã—32Ã—1      â†’ 28Ã—28Ã—6   â†’ 14Ã—14Ã—6   â†’ 10Ã—10Ã—16  â†’ 5Ã—5Ã—16  â†’ 400Ã—1   â†’ 120Ã—1   â†’ 84Ã—1   â†’ 10Ã—1

Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                    LeNet-5 Architecture                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input Image    â”‚ Conv Layer 1   â”‚ SubSample 1    â”‚ Conv Layer 2   â”‚ SubSample 2    â”‚ Fully Connected â”‚
â”‚ 32Ã—32Ã—1        â”‚ 28Ã—28Ã—6        â”‚ 14Ã—14Ã—6        â”‚ 10Ã—10Ã—16       â”‚ 5Ã—5Ã—16         â”‚ 120 â†’ 84 â†’ 10  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Handwritten â”‚ â”‚ â”‚ 6 Feature   â”‚ â”‚ â”‚ 6 Feature   â”‚ â”‚ â”‚ 16 Feature  â”‚ â”‚ â”‚ 16 Feature  â”‚ â”‚ â”‚ Classificationâ”‚ â”‚
â”‚ â”‚ Digit       â”‚ â”‚ â”‚ Maps        â”‚ â”‚ â”‚ Maps        â”‚ â”‚ â”‚ Maps        â”‚ â”‚ â”‚ Maps        â”‚ â”‚ â”‚ Vector      â”‚ â”‚
â”‚ â”‚ (grayscale) â”‚ â”‚ â”‚ (5Ã—5 conv)  â”‚ â”‚ â”‚ (2Ã—2 pool)  â”‚ â”‚ â”‚ (5Ã—5 conv)  â”‚ â”‚ â”‚ (2Ã—2 pool)  â”‚ â”‚ â”‚             â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ LeNet-5:**
- ĞŸĞµÑ€Ğ²Ğ°Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ°Ñ CNN Ğ´Ğ»Ñ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€ÑƒĞºĞ¾Ğ¿Ğ¸ÑĞ½Ñ‹Ñ… Ñ†Ğ¸Ñ„Ñ€
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ñ‚Ğ°nh Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ
- Subsampling Ğ²Ğ¼ĞµÑÑ‚Ğ¾ max pooling
- Ğ’ÑĞµĞ³Ğ¾ ~60,000 Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²

### AlexNet (2012) - ĞŸÑ€Ğ¾Ñ€Ñ‹Ğ² Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸

```
ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° AlexNet:
Input (227x227x3) â†’ Conv1 â†’ Pool1 â†’ Conv2 â†’ Pool2 â†’ Conv3 â†’ Conv4 â†’ Conv5 â†’ Pool3 â†’ FC1 â†’ FC2 â†’ FC3
     â†“              â†“       â†“       â†“       â†“       â†“       â†“       â†“       â†“       â†“     â†“     â†“
  227Ã—227Ã—3    â†’ 55Ã—55Ã—96 â†’ 27Ã—27Ã—96 â†’ 27Ã—27Ã—256 â†’ 13Ã—13Ã—256 â†’ 13Ã—13Ã—384 â†’ 13Ã—13Ã—384 â†’ 13Ã—13Ã—256 â†’ 6Ã—6Ã—256 â†’ 4096 â†’ 4096 â†’ 1000

Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                   AlexNet Architecture                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer    â”‚ Type      â”‚ Size        â”‚ Filters â”‚ Stride â”‚ Padding â”‚ Activation â”‚ Params     â”‚ Output    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input    â”‚ Image     â”‚ 227Ã—227Ã—3   â”‚ -       â”‚ -      â”‚ -       â”‚ -          â”‚ 0          â”‚ 227Ã—227Ã—3 â”‚
â”‚ Conv1    â”‚ Conv      â”‚ 11Ã—11Ã—96    â”‚ 96      â”‚ 4      â”‚ 0       â”‚ ReLU       â”‚ 34,848     â”‚ 55Ã—55Ã—96  â”‚
â”‚ Pool1    â”‚ MaxPool   â”‚ 3Ã—3         â”‚ -       â”‚ 2      â”‚ 0       â”‚ -          â”‚ 0          â”‚ 27Ã—27Ã—96  â”‚
â”‚ Conv2    â”‚ Conv      â”‚ 5Ã—5Ã—256     â”‚ 256     â”‚ 1      â”‚ 2       â”‚ ReLU       â”‚ 614,656    â”‚ 27Ã—27Ã—256 â”‚
â”‚ Pool2    â”‚ MaxPool   â”‚ 3Ã—3         â”‚ -       â”‚ 2      â”‚ 0       â”‚ -          â”‚ 0          â”‚ 13Ã—13Ã—256 â”‚
â”‚ Conv3    â”‚ Conv      â”‚ 3Ã—3Ã—384     â”‚ 384     â”‚ 1      â”‚ 1       â”‚ ReLU       â”‚ 885,120    â”‚ 13Ã—13Ã—384 â”‚
â”‚ Conv4    â”‚ Conv      â”‚ 3Ã—3Ã—384     â”‚ 384     â”‚ 1      â”‚ 1       â”‚ ReLU       â”‚ 1,327,488  â”‚ 13Ã—13Ã—384 â”‚
â”‚ Conv5    â”‚ Conv      â”‚ 3Ã—3Ã—256     â”‚ 256     â”‚ 1      â”‚ 1       â”‚ ReLU       â”‚ 884,992    â”‚ 13Ã—13Ã—256 â”‚
â”‚ Pool3    â”‚ MaxPool   â”‚ 3Ã—3         â”‚ -       â”‚ 2      â”‚ 0       â”‚ -          â”‚ 0          â”‚ 6Ã—6Ã—256   â”‚
â”‚ FC1      â”‚ Dense     â”‚ 4096        â”‚ -       â”‚ -      â”‚ -       â”‚ ReLU       â”‚ 37,752,832 â”‚ 4096      â”‚
â”‚ FC2      â”‚ Dense     â”‚ 4096        â”‚ -       â”‚ -      â”‚ -       â”‚ ReLU       â”‚ 16,781,312 â”‚ 4096      â”‚
â”‚ FC3      â”‚ Dense     â”‚ 1000        â”‚ -       â”‚ -      â”‚ -       â”‚ Softmax    â”‚ 4,097,000  â”‚ 1000      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Total Parameters: 62,378,248
```

**ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ½Ğ¾Ğ²Ğ°Ñ†Ğ¸Ğ¸ AlexNet:**
- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ReLU Ğ²Ğ¼ĞµÑÑ‚Ğ¾ tanh
- Dropout Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
- ĞÑƒĞ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
- GPU Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ
- Local Response Normalization

### VGGNet (2014) - Ğ¡Ğ¸Ğ»Ğ° Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ñ‹

```
ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° VGG-16:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                    VGG-16 Architecture                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Block 1:  â”‚ Input (224Ã—224Ã—3) â†’ Conv(3Ã—3Ã—64) â†’ Conv(3Ã—3Ã—64) â†’ MaxPool(2Ã—2) â†’ 112Ã—112Ã—64            â”‚
â”‚ Block 2:  â”‚ Conv(3Ã—3Ã—128) â†’ Conv(3Ã—3Ã—128) â†’ MaxPool(2Ã—2) â†’ 56Ã—56Ã—128                                â”‚
â”‚ Block 3:  â”‚ Conv(3Ã—3Ã—256) â†’ Conv(3Ã—3Ã—256) â†’ Conv(3Ã—3Ã—256) â†’ MaxPool(2Ã—2) â†’ 28Ã—28Ã—256                â”‚
â”‚ Block 4:  â”‚ Conv(3Ã—3Ã—512) â†’ Conv(3Ã—3Ã—512) â†’ Conv(3Ã—3Ã—512) â†’ MaxPool(2Ã—2) â†’ 14Ã—14Ã—512                â”‚
â”‚ Block 5:  â”‚ Conv(3Ã—3Ã—512) â†’ Conv(3Ã—3Ã—512) â†’ Conv(3Ã—3Ã—512) â†’ MaxPool(2Ã—2) â†’ 7Ã—7Ã—512                  â”‚
â”‚ Classifier:â”‚ FC(4096) â†’ FC(4096) â†’ FC(1000) â†’ Softmax                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ¾Ğ²:
224Ã—224Ã—3 â†’ 224Ã—224Ã—64 â†’ 112Ã—112Ã—64 â†’ 112Ã—112Ã—128 â†’ 56Ã—56Ã—128 â†’ 56Ã—56Ã—256 â†’ 28Ã—28Ã—256 â†’ 28Ã—28Ã—512 â†’ 14Ã—14Ã—512 â†’ 14Ã—14Ã—512 â†’ 7Ã—7Ã—512 â†’ 4096 â†’ 4096 â†’ 1000
    â†“            â†“            â†“            â†“           â†“           â†“           â†“           â†“           â†“           â†“         â†“      â†“      â†“
  Input      Conv+Conv     MaxPool     Conv+Conv    MaxPool   Conv+Conv+Conv MaxPool  Conv+Conv+Conv MaxPool Conv+Conv+Conv MaxPool  FC1    FC2    FC3
```

**ĞŸÑ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ VGG:**
- Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ 3Ã—3 Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹
- Ğ£Ğ´Ğ²Ğ¾ĞµĞ½Ğ¸Ğµ Ñ‡Ğ¸ÑĞ»Ğ° Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ¾Ğ² Ğ¿Ğ¾ÑĞ»Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ pooling
- ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ğ¸ ĞµĞ´Ğ¸Ğ½Ğ¾Ğ¾Ğ±Ñ€Ğ°Ğ·Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°
- Ğ“Ğ»ÑƒĞ±Ğ¸Ğ½Ğ° 16-19 ÑĞ»Ğ¾ĞµĞ²

### ResNet (2015) - Ğ ĞµĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ñ Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑĞ²ÑĞ·ĞµĞ¹

```
ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¸ÑÑ‡ĞµĞ·Ğ°ÑÑ‰Ğ¸Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ÑĞµÑ‚Ğ¸ Ğ±ĞµĞ· skip connections                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Input â†’ Layer1 â†’ Layer2 â†’ ... â†’ Layer50 â†’ ... â†’ Layer152 â†’ Output                                   â”‚
â”‚   â†“        â†“        â†“               â†“               â†“         â†“                                      â”‚
â”‚ Gradient backpropagation: âˆ‡L/âˆ‚xâ‚â‚…â‚‚ â†’ âˆ‡L/âˆ‚xâ‚…â‚€ â†’ ... â†’ âˆ‡L/âˆ‚xâ‚‚ â†’ âˆ‡L/âˆ‚xâ‚                           â”‚
â”‚                                                                                                     â”‚
â”‚ Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ°ĞµÑ‚ÑÑ ÑĞºÑĞ¿Ğ¾Ğ½ĞµĞ½Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾: âˆ‡L/âˆ‚xâ‚ â‰ˆ âˆ‡L/âˆ‚xâ‚â‚…â‚‚ Ã— âˆ(âˆ‚xáµ¢/âˆ‚xáµ¢â‚‹â‚)                        â”‚
â”‚                                                        i=2                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Residual Block (Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ±Ğ»Ğ¾Ğº ResNet):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                  Residual Block                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Input x                                                                                   â”‚
â”‚              â”‚                                                                                      â”‚
â”‚              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚              â”‚                                                                     â”‚               â”‚
â”‚              â–¼                                                                     â”‚               â”‚
â”‚        Weight Layer 1                                                             â”‚               â”‚
â”‚        Conv 3Ã—3, ReLU                                                             â”‚               â”‚
â”‚              â”‚                                                                     â”‚               â”‚
â”‚              â–¼                                                                     â”‚               â”‚
â”‚        Weight Layer 2                                                             â”‚               â”‚
â”‚        Conv 3Ã—3                                                                   â”‚               â”‚
â”‚              â”‚                                                                     â”‚               â”‚
â”‚              â–¼                                                                     â”‚               â”‚
â”‚             [+] â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚              â”‚                                                                                      â”‚
â”‚              â–¼                                                                                      â”‚
â”‚            ReLU                                                                                     â”‚
â”‚              â”‚                                                                                      â”‚
â”‚              â–¼                                                                                      â”‚
â”‚         Output F(x) + x                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸: H(x) = F(x) + x, Ğ³Ğ´Ğµ F(x) - Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
```

**ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ResNet-50:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                  ResNet-50 Architecture                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Stage      â”‚ Output Size â”‚ Residual Block                                        â”‚ Layers            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Conv1      â”‚ 112Ã—112     â”‚ 7Ã—7 conv, 64, stride 2                              â”‚ 1                 â”‚
â”‚ Conv2_x    â”‚ 56Ã—56       â”‚ 3Ã—3 maxpool, stride 2                               â”‚ 1                 â”‚
â”‚            â”‚             â”‚ [1Ã—1 conv, 64]                                       â”‚                   â”‚
â”‚            â”‚             â”‚ [3Ã—3 conv, 64] Ã— 3                                   â”‚ 9                 â”‚
â”‚            â”‚             â”‚ [1Ã—1 conv, 256]                                      â”‚                   â”‚
â”‚ Conv3_x    â”‚ 28Ã—28       â”‚ [1Ã—1 conv, 128]                                      â”‚                   â”‚
â”‚            â”‚             â”‚ [3Ã—3 conv, 128] Ã— 4                                  â”‚ 12                â”‚
â”‚            â”‚             â”‚ [1Ã—1 conv, 512]                                      â”‚                   â”‚
â”‚ Conv4_x    â”‚ 14Ã—14       â”‚ [1Ã—1 conv, 256]                                      â”‚                   â”‚
â”‚            â”‚             â”‚ [3Ã—3 conv, 256] Ã— 6                                  â”‚ 18                â”‚
â”‚            â”‚             â”‚ [1Ã—1 conv, 1024]                                     â”‚                   â”‚
â”‚ Conv5_x    â”‚ 7Ã—7         â”‚ [1Ã—1 conv, 512]                                      â”‚                   â”‚
â”‚            â”‚             â”‚ [3Ã—3 conv, 512] Ã— 3                                  â”‚ 9                 â”‚
â”‚            â”‚             â”‚ [1Ã—1 conv, 2048]                                     â”‚                   â”‚
â”‚ Classifier â”‚ 1Ã—1         â”‚ average pool, 1000-d fc, softmax                     â”‚ 2                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Total Layers: 50                                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ

#### Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ResNet Block Ğ½Ğ° PyTorch

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class BasicBlock(nn.Module):
    """Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ±Ğ»Ğ¾Ğº ResNet"""
    expansion = 1
    
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        
        # ĞŸĞµÑ€Ğ²Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, 
                              stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        
        # Ğ’Ñ‚Ğ¾Ñ€Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¹ ÑĞ²ĞµÑ€Ñ‚ĞºĞ¸
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, 
                              stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # Skip connection
        self.downsample = downsample
        
    def forward(self, x):
        identity = x
        
        # ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¿ÑƒÑ‚ÑŒ
        out = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)
        
        out = self.conv2(out)
        out = self.bn2(out)
        
        # Skip connection
        if self.downsample is not None:
            identity = self.downsample(x)
        
        out += identity  # ĞÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğµ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ
        out = F.relu(out)
        
        return out

class ResNet(nn.Module):
    """Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ResNet"""
    
    def __init__(self, block, layers, num_classes=1000):
        super(ResNet, self).__init__()
        
        self.in_channels = 64
        
        # ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        # Residual layers
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        
        # ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)
        
    def _make_layer(self, block, out_channels, blocks, stride=1):
        downsample = None
        if stride != 1 or self.in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.in_channels, out_channels * block.expansion,
                         kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels * block.expansion),
            )
        
        layers = []
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.in_channels, out_channels))
        
        return nn.Sequential(*layers)
    
    def forward(self, x):
        # ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.maxpool(x)
        
        # Residual blocks
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        # ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        
        return x

def resnet18(num_classes=1000):
    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)

def resnet34(num_classes=1000):
    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
if __name__ == "__main__":
    model = resnet18(num_classes=10)  # Ğ´Ğ»Ñ CIFAR-10
    x = torch.randn(1, 3, 224, 224)
    output = model(x)
    print(f"Input shape: {x.shape}")
    print(f"Output shape: {output.shape}")
```

### Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ‡ĞµÑĞºĞ¸Ñ… CNN Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ĞœĞ¾Ğ´ĞµĞ»ÑŒ    â”‚ Ğ“Ğ¾Ğ´  â”‚ Ğ¡Ğ»Ğ¾Ğ¸ â”‚ ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ â”‚ Top-1 Acc â”‚ ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LeNet-5   â”‚ 1998 â”‚ 5    â”‚ 60K       â”‚ ~99% (MNIST)â”‚ ĞŸĞµÑ€Ğ²Ğ°Ñ CNN, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°                â”‚
â”‚ AlexNet   â”‚ 2012 â”‚ 8    â”‚ 62M       â”‚ 57.1%     â”‚ ReLU, Dropout, GPU, Ğ¿Ñ€Ğ¾Ñ€Ñ‹Ğ² Ğ² ImageNet            â”‚
â”‚ VGG-16    â”‚ 2014 â”‚ 16   â”‚ 138M      â”‚ 71.3%     â”‚ Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ 3Ã—3 Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ñ‚Ğ°                     â”‚
â”‚ VGG-19    â”‚ 2014 â”‚ 19   â”‚ 144M      â”‚ 71.1%     â”‚ Ğ•Ñ‰Ğµ Ğ³Ğ»ÑƒĞ±Ğ¶Ğµ, Ğ½Ğ¾ Ğ½Ğµ Ğ»ÑƒÑ‡ÑˆĞµ                          â”‚
â”‚ ResNet-50 â”‚ 2015 â”‚ 50   â”‚ 26M       â”‚ 76.0%     â”‚ Skip connections, Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ÑĞµÑ‚Ğ¸             â”‚
â”‚ ResNet-152â”‚ 2015 â”‚ 152  â”‚ 60M       â”‚ 77.8%     â”‚ Ğ¡Ğ°Ğ¼Ğ°Ñ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ°Ñ Ğ½Ğ° Ñ‚Ğ¾Ñ‚ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ğ”Ğ¾Ğ¼Ğ°ÑˆĞ½ĞµĞµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ

1. **Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ VGG-Ğ±Ğ»Ğ¾ĞºĞ°:** Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹Ñ‚Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ VGG-Ğ±Ğ»Ğ¾ĞºĞ°
2. **ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²:** ĞŸĞ¾Ğ´ÑÑ‡Ğ¸Ñ‚Ğ°Ğ¹Ñ‚Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑĞ»Ğ¾Ğµ ResNet-18
3. **Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚:** Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ñ‚Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ ResNet-18 vs VGG-16 Ğ½Ğ° CIFAR-10

---

## ğŸ“– Ğ“Ğ»Ğ°Ğ²Ğ° 3.3: Transfer Learning Ğ¸ Fine-tuning
**ğŸ“… ĞĞµĞ´ĞµĞ»Ñ 4**

### ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ Transfer Learning

Transfer Learning - ÑÑ‚Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´ Ğ¼Ğ°ÑˆĞ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ, Ğ¿Ñ€Ğ¸ ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ½Ğ° Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğµ, Ğ°Ğ´Ğ°Ğ¿Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ ÑĞ²ÑĞ·Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸.

```
Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° A                                    â”‚ Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° B                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ A â†’ ĞœĞ¾Ğ´ĞµĞ»ÑŒ A â†’ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ A          â”‚ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ B â†’ ĞœĞ¾Ğ´ĞµĞ»ÑŒ B â†’ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ B                   â”‚
â”‚ (ImageNet)  (ResNet-50)  (1000 ĞºĞ»Ğ°ÑÑĞ¾Ğ²)   â”‚ (ĞšĞ¾ÑˆĞºĞ¸/Ğ¡Ğ¾Ğ±Ğ°ĞºĞ¸) (ĞĞ¾Ğ²Ğ°Ñ ÑĞµÑ‚ÑŒ) (2 ĞºĞ»Ğ°ÑÑĞ°)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Transfer Learning:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° A                                    â”‚ Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ° B                                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ A â†’ ĞœĞ¾Ğ´ĞµĞ»ÑŒ A â†’ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ A          â”‚ ĞŸÑ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ A â†’ Fine-tuning â†’ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ B    â”‚
â”‚ (ImageNet)  (ResNet-50)  (1000 ĞºĞ»Ğ°ÑÑĞ¾Ğ²)   â”‚ (Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ²ĞµÑĞ°) + (ĞĞ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€) (2 ĞºĞ»Ğ°ÑÑĞ°) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² CNN

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             Ğ˜ĞµÑ€Ğ°Ñ€Ñ…Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² Ğ² Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… CNN                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ğ¡Ğ»Ğ¾Ğ¹        â”‚ Ğ§Ñ‚Ğ¾ Ğ¸Ğ·ÑƒÑ‡Ğ°ĞµÑ‚                    â”‚ Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ â”‚ Ğ¡Ğ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ĞŸĞµÑ€Ğ²Ñ‹Ğµ      â”‚ Ğ“Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹, ÑƒĞ³Ğ»Ñ‹, Ñ‚ĞµĞºÑÑ‚ÑƒÑ€Ñ‹        â”‚ ĞÑ‡ĞµĞ½ÑŒ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ   â”‚ ĞĞ±Ñ‰Ğ¸Ğµ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹         â”‚
â”‚ ÑĞ»Ğ¾Ğ¸        â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“         â”‚                 â”‚                                    â”‚
â”‚             â”‚                                 â”‚                 â”‚                                    â”‚
â”‚ Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğµ     â”‚ Ğ¤Ğ¾Ñ€Ğ¼Ñ‹, Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹, ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸    â”‚ Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ         â”‚ Ğ—Ğ°Ğ²Ğ¸ÑĞ¸Ñ‚ Ğ¾Ñ‚ Ğ´Ğ¾Ğ¼ĞµĞ½Ğ°                  â”‚
â”‚ ÑĞ»Ğ¾Ğ¸        â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“         â”‚                 â”‚                                    â”‚
â”‚             â”‚                                 â”‚                 â”‚                                    â”‚
â”‚ ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ   â”‚ Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ñ‹, ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸ĞºĞ°     â”‚ ĞĞ¸Ğ·ĞºĞ°Ñ          â”‚ ĞÑ‡ĞµĞ½ÑŒ ÑĞ¿ĞµÑ†Ğ¸Ñ„Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸       â”‚
â”‚ ÑĞ»Ğ¾Ğ¸        â”‚ â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“â–“         â”‚                 â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ğ¡Ğ»Ğ¾Ğ¹ 1:  [/] [\\] [|] [-] [â—‹] [â–³]  â† ĞŸÑ€Ğ¾ÑÑ‚Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ñ‹                                    â”‚
â”‚ Ğ¡Ğ»Ğ¾Ğ¹ 2:  [â—] [â—‘] [â—’] [â—“] [â¬¢] [â¬¡]  â† ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ†                                         â”‚
â”‚ Ğ¡Ğ»Ğ¾Ğ¹ 3:  [ğŸ‘] [ğŸ”²] [ğŸ”³] [âšª] [ğŸ”´] [ğŸ”µ]  â† Ğ‘Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ñ‹                              â”‚
â”‚ Ğ¡Ğ»Ğ¾Ğ¹ 4:  [ğŸ±] [ğŸ¶] [ğŸš—] [ğŸ ] [ğŸŒ³] [ğŸ‘¤]  â† ĞĞ±ÑŠĞµĞºÑ‚Ñ‹ Ğ¸ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ²                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Transfer Learning

#### 1. Feature Extraction (Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²)

```python
import torch
import torch.nn as nn
import torchvision.models as models

# Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
model = models.resnet50(pretrained=True)

# Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
for param in model.parameters():
    param.requires_grad = False

# Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¹ Ğ´Ğ»Ñ Ğ½Ğ¾Ğ²Ğ¾Ğ¹ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸
num_classes = 2  # ĞºĞ¾ÑˆĞºĞ¸ vs ÑĞ¾Ğ±Ğ°ĞºĞ¸
model.fc = nn.Linear(model.fc.in_features, num_classes)

# Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¾Ğ±ÑƒÑ‡Ğ°Ñ‚ÑŒÑÑ
print("ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ:")
for name, param in model.named_parameters():
    if param.requires_grad:
        print(f"  {name}: {param.shape}")
```

#### 2. Fine-tuning (Ğ¢Ğ¾Ğ½ĞºĞ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°)

```python
# Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
model = models.resnet50(pretrained=True)

# Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¹
num_classes = 2
model.fc = nn.Linear(model.fc.in_features, num_classes)

# Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ€Ğ°Ğ·Ğ½Ñ‹Ğµ learning rates Ğ´Ğ»Ñ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
optimizer = torch.optim.Adam([
    {'params': model.conv1.parameters(), 'lr': 1e-5},      # ĞÑ‡ĞµĞ½ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¹ lr
    {'params': model.layer1.parameters(), 'lr': 1e-5},     # ĞÑ‡ĞµĞ½ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¹ lr
    {'params': model.layer2.parameters(), 'lr': 1e-4},     # ĞĞ¸Ğ·ĞºĞ¸Ğ¹ lr
    {'params': model.layer3.parameters(), 'lr': 1e-4},     # ĞĞ¸Ğ·ĞºĞ¸Ğ¹ lr
    {'params': model.layer4.parameters(), 'lr': 1e-3},     # Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ lr
    {'params': model.fc.parameters(), 'lr': 1e-2}          # Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ lr
])
```

#### 3. Gradual Unfreezing (ĞŸĞ¾ÑÑ‚ĞµĞ¿ĞµĞ½Ğ½Ğ¾Ğµ Ñ€Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ)

```python
def unfreeze_layers(model, num_layers_to_unfreeze):
    """Ğ Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ num_layers_to_unfreeze ÑĞ»Ğ¾ĞµĞ²"""
    all_layers = list(model.children())
    
    # Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ²ÑĞµ ÑĞ»Ğ¾Ğ¸
    for layer in all_layers:
        for param in layer.parameters():
            param.requires_grad = False
    
    # Ğ Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸
    for layer in all_layers[-num_layers_to_unfreeze:]:
        for param in layer.parameters():
            param.requires_grad = True

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
model = models.resnet50(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, 2)

# Ğ­Ñ‚Ğ°Ğ¿ 1: ĞĞ±ÑƒÑ‡Ğ°ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€
unfreeze_layers(model, 1)
# ... Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ 10 ÑĞ¿Ğ¾Ñ… ...

# Ğ­Ñ‚Ğ°Ğ¿ 2: Ğ Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ 2 ÑĞ»Ğ¾Ñ
unfreeze_layers(model, 2)
# ... Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞµÑ‰Ğµ 10 ÑĞ¿Ğ¾Ñ… Ñ Ğ¼ĞµĞ½ÑŒÑˆĞ¸Ğ¼ lr ...
```

### Ğ’Ñ‹Ğ±Ğ¾Ñ€ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ° Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¸ Transfer Learning                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    â”‚ ĞœĞ°Ğ»Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…          â”‚ ĞœĞ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğ¸      â”‚ Feature Extraction   â”‚ Fine-tuning Ñ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¼ lr                                â”‚
â”‚ Ğ½Ğ° Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ñ‹Ğµ        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                    â”‚ â”‚ â€¢ Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ²ĞµÑĞ°  â”‚ â”‚ â”‚ â€¢ Ğ Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ ÑĞ»Ğ¾Ğ¸                             â”‚ â”‚
â”‚                    â”‚ â”‚ â€¢ ĞĞ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾   â”‚ â”‚ â”‚ â€¢ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ½Ğ¸Ğ·ĞºĞ¸Ğ¹ learning rate          â”‚ â”‚
â”‚                    â”‚ â”‚   ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€    â”‚ â”‚ â”‚ â€¢ Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ gradual unfreezing                      â”‚ â”‚
â”‚                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ ÑĞ¸Ğ»ÑŒĞ½Ğ¾      â”‚ Feature Extraction   â”‚ Fine-tuning Ñ Ğ²Ñ‹ÑĞ¾ĞºĞ¸Ğ¼ lr                               â”‚
â”‚ Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ°ÑÑ‚ÑÑ         â”‚ + Fine-tuning        â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚                    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â€¢ Ğ Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ Ğ²ÑĞµ ÑĞ»Ğ¾Ğ¸                             â”‚ â”‚
â”‚                    â”‚ â”‚ â€¢ ĞĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ¸â”‚ â”‚ â”‚ â€¢ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹ learning rate           â”‚ â”‚
â”‚                    â”‚ â”‚ â€¢ ĞŸĞ¾Ñ‚Ğ¾Ğ¼ Ñ€Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ñ‚ÑŒ â”‚ â”‚ â”‚ â€¢ Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ½ÑƒĞ»Ñ                         â”‚ â”‚
â”‚                    â”‚ â”‚   Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğµ ÑĞ»Ğ¾Ğ¸   â”‚ â”‚ â”‚                                                     â”‚ â”‚
â”‚                    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€: ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¼ĞµĞ´Ğ¸Ñ†Ğ¸Ğ½ÑĞºĞ¸Ñ… Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

class MedicalImageClassifier:
    def __init__(self, num_classes=2, model_name='resnet50'):
        self.num_classes = num_classes
        self.model_name = model_name
        self.model = self._create_model()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
    def _create_model(self):
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ transfer learning"""
        if self.model_name == 'resnet50':
            model = models.resnet50(pretrained=True)
            num_features = model.fc.in_features
            model.fc = nn.Linear(num_features, self.num_classes)
        elif self.model_name == 'vgg16':
            model = models.vgg16(pretrained=True)
            num_features = model.classifier[6].in_features
            model.classifier[6] = nn.Linear(num_features, self.num_classes)
        
        return model
    
    def freeze_feature_extractor(self):
        """Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·ĞºĞ° Ğ²ĞµÑĞ¾Ğ² feature extractor"""
        for param in self.model.parameters():
            param.requires_grad = False
        
        # Ğ Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€
        if self.model_name == 'resnet50':
            for param in self.model.fc.parameters():
                param.requires_grad = True
        elif self.model_name == 'vgg16':
            for param in self.model.classifier.parameters():
                param.requires_grad = True
    
    def unfreeze_all(self):
        """Ğ Ğ°Ğ·Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²ÑĞµÑ… Ğ²ĞµÑĞ¾Ğ²"""
        for param in self.model.parameters():
            param.requires_grad = True
    
    def train_epoch(self, dataloader, optimizer, criterion):
        """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ¿Ğ¾Ñ…Ğ¸"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(self.device), labels.to(self.device)
            
            optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
        
        return running_loss / len(dataloader), 100 * correct / total
    
    def validate(self, dataloader, criterion):
        """Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, labels in dataloader:
                inputs, labels = inputs.to(self.device), labels.to(self.device)
                
                outputs = self.model(inputs)
                loss = criterion(outputs, labels)
                
                running_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        return running_loss / len(dataloader), 100 * correct / total
    
    def two_stage_training(self, train_loader, val_loader, num_epochs_stage1=10, num_epochs_stage2=10):
        """Ğ”Ğ²ÑƒÑ…ÑÑ‚Ğ°Ğ¿Ğ½Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ"""
        criterion = nn.CrossEntropyLoss()
        
        # Ğ­Ñ‚Ğ°Ğ¿ 1: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°
        print("Ğ­Ñ‚Ğ°Ğ¿ 1: ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°...")
        self.freeze_feature_extractor()
        optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()), 
                              lr=0.001)
        
        for epoch in range(num_epochs_stage1):
            train_loss, train_acc = self.train_epoch(train_loader, optimizer, criterion)
            val_loss, val_acc = self.validate(val_loader, criterion)
            
            print(f'Epoch {epoch+1}/{num_epochs_stage1}:')
            print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        
        # Ğ­Ñ‚Ğ°Ğ¿ 2: Fine-tuning Ğ²ÑĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        print("\nĞ­Ñ‚Ğ°Ğ¿ 2: Fine-tuning Ğ²ÑĞµĞ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...")
        self.unfreeze_all()
        optimizer = optim.Adam(self.model.parameters(), lr=0.0001)  # ĞœĞµĞ½ÑŒÑˆĞ¸Ğ¹ lr
        
        for epoch in range(num_epochs_stage2):
            train_loss, train_acc = self.train_epoch(train_loader, optimizer, criterion)
            val_loss, val_acc = self.validate(val_loader, criterion)
            
            print(f'Epoch {epoch+1}/{num_epochs_stage2}:')
            print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
            print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
if __name__ == "__main__":
    # Ğ¢Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ°
    classifier = MedicalImageClassifier(num_classes=2, model_name='resnet50')
    
    # Ğ—Ğ´ĞµÑÑŒ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ° Ğ±Ñ‹Ñ‚ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    # train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    # val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # classifier.two_stage_training(train_loader, val_loader)
```

### Ğ”Ğ¾Ğ¼Ğ°ÑˆĞ½ĞµĞµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ

1. **Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚ Ñ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸ÑĞ¼Ğ¸:** Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ñ‚Ğµ Feature Extraction vs Fine-tuning Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ CIFAR-10
2. **ĞĞ½Ğ°Ğ»Ğ¸Ğ· ÑĞ»Ğ¾ĞµĞ²:** Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ² Ğ¿Ñ€ĞµĞ´Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
3. **Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°:** Ğ ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞ¹Ñ‚Ğµ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ transfer learning Ğ´Ğ»Ñ ÑĞ¾Ğ±ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°

---

## ğŸ“š Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹

### Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ°
- "Deep Learning" by Ian Goodfellow, Yoshua Bengio, Aaron Courville (Ğ“Ğ»Ğ°Ğ²Ğ° 9)
- "Hands-On Machine Learning" by AurÃ©lien GÃ©ron (Ğ“Ğ»Ğ°Ğ²Ğ° 14)
- CS231n Stanford: Convolutional Neural Networks for Visual Recognition

### ĞŸĞ¾Ğ»ĞµĞ·Ğ½Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹
- [Papers With Code - Computer Vision](https://paperswithcode.com/area/computer-vision)
- [PyTorch Vision Models](https://pytorch.org/vision/stable/models.html)
- [Kaggle Competitions - Computer Vision](https://www.kaggle.com/competitions)

### ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ
1. Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ CNN Ñ Ğ½ÑƒĞ»Ñ Ğ½Ğ° NumPy
2. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€Ğ° Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ Transfer Learning
3. Ğ£Ñ‡Ğ°ÑÑ‚Ğ¸Ğµ Ğ² Kaggle ÑĞ¾Ñ€ĞµĞ²Ğ½Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ Ğ¿Ğ¾ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼Ñƒ Ğ·Ñ€ĞµĞ½Ğ¸Ñ

---

**ğŸ¯ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ±Ğ»Ğ¾ĞºĞ°:** ĞŸĞ¾ÑĞ»Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¸Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ±Ğ»Ğ¾ĞºĞ° Ğ²Ñ‹ Ğ±ÑƒĞ´ĞµÑ‚Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ CNN, ÑĞ¼Ğ¾Ğ¶ĞµÑ‚Ğµ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ñ‹Ğ²Ğ°Ñ‚ÑŒ ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹, Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ÑÑ‚ÑŒ transfer learning Ğ¸ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ³Ğ¾ Ğ·Ñ€ĞµĞ½Ğ¸Ñ Ñ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸ĞµĞ¼ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ½Ğ° Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ°Ñ….