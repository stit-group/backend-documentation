# План изучения глубокого обучения: от основ до продвинутого уровня

## Фаза 1: Фундаментальные основы (2-3 месяца)

### Неделя 1-2: Математические основы
- Линейная алгебра: матрицы, векторы, операции
- Математический анализ: производные, частные производные, цепное правило
- Теория вероятностей: распределения, условная вероятность
- Статистика: среднее, дисперсия, корреляция

### Неделя 3-4: Основы машинного обучения
- Понятие модели, обучения, предсказания
- Типы задач: классификация, регрессия, кластеризация
- Разделение данных: train/validation/test
- Переобучение и недообучение
- Метрики качества: accuracy, precision, recall, F1-score

### Неделя 5-6: Введение в нейросети
- Что такое нейрон: веса, bias, функция активации
- Простейшая нейросеть: perceptron
- Функции активации: sigmoid, tanh, ReLU
- Forward pass: как данные проходят через сеть
- Понятие о loss функциях

### Неделя 7-8: Обучение нейросетей
- Backpropagation: как работает обратное распространение ошибки
- Градиентный спуск: поиск минимума функции потерь
- Learning rate: влияние на скорость и качество обучения
- Batch, mini-batch, stochastic градиентный спуск

### Неделя 9-12: Практическая работа с простыми сетями
- Реализация простой нейросети на Python
- Работа с популярными фреймворками (TensorFlow/PyTorch)
- Обучение на простых датасетах (MNIST, Iris)
- Анализ кривых обучения

## Фаза 2: Классические архитектуры (2-3 месяца)

### Неделя 13-14: Сверточные нейросети (CNN)
- Операция свертки: kernel, stride, padding
- Pooling слои: max pooling, average pooling
- Понятие feature map и receptive field
- Архитектуры: LeNet, AlexNet

### Неделя 15-16: Продвинутые CNN
- Batch normalization: зачем нужно и как работает
- Dropout: борьба с переобучением
- Data augmentation: увеличение разнообразия данных
- Transfer learning: использование предобученных моделей

### Неделя 17-18: Современные CNN архитектуры
- VGG: глубина и малые фильтры
- ResNet: residual connections и решение проблемы исчезающих градиентов
- Inception: параллельные свертки разных размеров
- MobileNet: эффективные архитектуры для мобильных устройств

### Неделя 19-20: Рекуррентные сети (RNN)
- Обработка последовательностей
- Vanilla RNN: проблемы с длинными зависимостями
- LSTM: долгосрочная память через gates
- GRU: упрощенная альтернатива LSTM

### Неделя 21-24: Практическое применение
- Компьютерное зрение: классификация изображений, детекция объектов
- Обработка естественного языка: классификация текстов, анализ тональности
- Временные ряды: прогнозирование, анализ трендов
- Проектная работа: решение реальной задачи

## Фаза 3: Современные подходы (2-3 месяца)

### Неделя 25-26: Attention механизм
- Проблемы RNN с длинными последовательностями
- Концепция attention: query, key, value
- Self-attention: как элементы последовательности взаимодействуют
- Multi-head attention: параллельные attention головы

### Неделя 27-28: Transformer архитектура
- Encoder-decoder структура
- Positional encoding: добавление информации о позиции
- Преимущества перед RNN: параллелизация, лучшее качество
- Применения: машинный перевод, генерация текста

### Неделя 29-30: Языковые модели
- BERT: bidirectional encoder для понимания контекста
- GPT: autoregressive декодер для генерации
- Задачи: masked language modeling, next sentence prediction
- Fine-tuning для конкретных задач

### Неделя 31-32: Computer Vision Transformers
- Vision Transformer (ViT): применение transformer к изображениям
- Patch embeddings: разбиение изображения на патчи
- Сравнение с CNN: когда что лучше использовать
- Гибридные подходы: комбинирование CNN и Transformer

### Неделя 33-36: Специализированные архитектуры
- U-Net: encoder-decoder для сегментации
- Autoencoder: сжатие и восстановление данных
- Variational Autoencoder (VAE): генеративные модели
- Graph Neural Networks: работа с графовыми данными

## Фаза 4: Продвинутые техники (2-3 месяца)

### Неделя 37-38: Оптимизация обучения
- Продвинутые оптимизаторы: Adam, AdamW, RMSprop
- Learning rate scheduling: cosine annealing, warm-up
- Gradient clipping: контроль взрывающихся градиентов
- Mixed precision training: ускорение обучения

### Неделя 39-40: Регуляризация и стабилизация
- L1/L2 регуляризация: контроль сложности модели
- Early stopping: предотвращение переобучения
- Label smoothing: смягчение жестких меток
- Mixup и CutMix: аугментация на уровне признаков

### Неделя 41-42: Генеративные модели
- Generative Adversarial Networks (GAN): состязательное обучение
- Variational methods: VAE и его вариации
- Diffusion models: современный подход к генерации
- Autoregressive models: поэтапная генерация

### Неделя 43-44: Специальные техники обучения
- Contrastive learning: обучение через сравнение
- Few-shot learning: обучение на малых данных
- Meta-learning: обучение обучаться
- Multi-task learning: одновременное решение нескольких задач

### Неделя 45-48: Исследовательские направления
- Neural Architecture Search (NAS): автоматический поиск архитектур
- Knowledge distillation: передача знаний от больших моделей к малым
- Domain adaptation: адаптация к новым доменам
- Interpretability: понимание решений моделей

## Фаза 5: Production и практические навыки (1-2 месяца)

### Неделя 49-50: Оптимизация для продакшена
- Model compression: pruning, quantization
- Форматы моделей: ONNX, TensorRT, TensorFlow Lite
- Оптимизация инференса: batch processing, caching
- Hardware acceleration: GPU, TPU оптимизации

### Неделя 51-52: MLOps и мониторинг
- Deployment стратегии: staging, canary releases
- Model versioning: отслеживание версий моделей
- Monitoring в production: drift detection, performance tracking
- A/B тестирование моделей

### Неделя 53-56: Масштабирование и инфраструктура
- Distributed training: обучение на нескольких GPU/машинах
- Data pipelines: эффективная загрузка и предобработка данных
- Cloud platforms: AWS, GCP, Azure для ML
- Containerization: Docker, Kubernetes для ML сервисов

## Рекомендации по изучению

### Практические проекты на каждом этапе
- **Фаза 1:** Классификация рукописных цифр
- **Фаза 2:** Распознавание объектов на изображениях
- **Фаза 3:** Chatbot или система вопросов-ответов
- **Фаза 4:** Генерация изображений или текста
- **Фаза 5:** Полный ML pipeline от данных до продакшена

### Ресурсы для изучения
- **Теория:** Курсы Andrew Ng, Deep Learning Specialization
- **Практика:** Kaggle соревнования, Papers with Code
- **Документация:** Официальные гайды TensorFlow/PyTorch
- **Сообщество:** ML Twitter, Reddit r/MachineLearning

### Ключевые навыки к концу обучения
- Понимание математических основ нейросетей
- Умение выбирать архитектуру под задачу
- Навыки отладки и оптимизации моделей
- Знание современных исследований и трендов
- Практический опыт внедрения в продакшен

### Оценка прогресса
- **Месяц 1-3:** Базовое понимание, решение простых задач
- **Месяц 4-6:** Работа с реальными датасетами, понимание архитектур
- **Месяц 7-9:** Использование state-of-the-art моделей
- **Месяц 10-12:** Самостоятельное исследование и продакшен навыки

Этот план предполагает изучение по 10-15 часов в неделю и может быть адаптирован под индивидуальный темп и цели обучения.