# Интервью Backend разработчика: Глубокое обучение

**Структура:** 150 основных вопросов + 5 углубляющих к каждому (900 вопросов всего)

---

## 1. Основы нейросетей и математика (20 вопросов)

### Вопрос 1: Что такое персептрон и как он работает?
**Углубляющие вопросы:**
1. Как математически записать выход персептрона?
2. Почему персептрон не может решить задачу XOR?
3. Что такое линейная разделимость и как она связана с персептроном?
4. Как обучается персептрон? Опишите правило обучения Розенблатта.
5. В чем разница между персептроном и логистической регрессией?

### Вопрос 2: Объясните концепцию многослойного персептрона (MLP)
**Углубляющие вопросы:**
1. Почему добавление скрытых слоев решает проблему XOR?
2. Что такое универсальная теорема аппроксимации?
3. Как количество нейронов в скрытом слое влияет на способность сети к обучению?
4. Что происходит с градиентами в глубоких сетях?
5. Как выбрать оптимальное количество скрытых слоев?

### Вопрос 3: Что такое forward pass в нейронной сети?
**Углубляющие вопросы:**
1. Как математически описать forward pass для одного слоя?
2. Что такое аффинное преобразование в контексте нейросетей?
3. Как организуется вычисление для batch данных?
4. Что такое computational graph и как он строится?
5. Как оптимизировать forward pass для больших данных?

### Вопрос 4: Объясните роль весов и смещений (bias) в нейронах
**Углубляющие вопросы:**
1. Что происходит, если убрать bias из нейрона?
2. Как инициализировать веса в нейронной сети?
3. Что такое Xavier/Glorot инициализация и почему она важна?
4. Как bias влияет на способность нейрона к сдвигу функции активации?
5. Можно ли обучить сеть без bias? В каких случаях это оправдано?

### Вопрос 5: Что такое нелинейность в нейросетях и зачем она нужна?
**Углубляющие вопросы:**
1. Что произойдет, если убрать все функции активации из глубокой сети?
2. Как нелинейности позволяют аппроксимировать сложные функции?
3. Что такое выразительная способность нейронной сети?
4. Как количество параметров связано с сложностью аппроксимируемых функций?
5. Приведите пример задачи, которую нельзя решить без нелинейностей.

### Вопрос 6: Объясните концепцию градиента в контексте машинного обучения
**Углубляющие вопросы:**
1. Как вычислить градиент сложной композитной функции?
2. Что такое частные производные и как они связаны с градиентом?
3. Как градиент указывает направление наискорейшего возрастания?
4. Что такое градиентный спуск и почему мы идем против градиента?
5. Как численно проверить правильность вычисления градиента?

### Вопрос 7: Что такое функция потерь (loss function)?
**Углубляющие вопросы:**
1. В чем разница между loss и cost function?
2. Как выбрать подходящую функцию потерь для конкретной задачи?
3. Что такое регуляризационные слагаемые в функции потерь?
4. Как функция потерь связана с вероятностной интерпретацией модели?
5. Можно ли использовать несколько функций потерь одновременно?

### Вопрос 8: Объясните backpropagation на интуитивном уровне
**Углубляющие вопросы:**
1. Почему backpropagation эффективнее вычисления градиентов "в лоб"?
2. Как цепное правило дифференцирования применяется в backpropagation?
3. Что такое вычислительный граф и как по нему распространяются градиенты?
4. В каком порядке вычисляются градиенты в процессе backpropagation?
5. Как backpropagation работает с разветвленными архитектурами?

### Вопрос 9: Что такое эпоха, батч и итерация в обучении нейросетей?
**Углубляющие вопросы:**
1. Как размер батча влияет на качество обучения?
2. В чем разница между batch, mini-batch и stochastic gradient descent?
3. Как выбрать оптимальный размер батча?
4. Что такое gradient accumulation и когда его использовать?
5. Как батчинг влияет на использование памяти GPU?

### Вопрос 10: Объясните проблему переобучения (overfitting)
**Углубляющие вопросы:**
1. Как визуально определить переобучение по кривым обучения?
2. Что такое bias-variance tradeoff?
3. Какие методы помогают бороться с переобучением?
4. Как размер датасета влияет на склонность к переобучению?
5. Что такое early stopping и как его правильно применять?

### Вопрос 11: Что такое валидационная выборка и зачем она нужна?
**Углубляющие вопросы:**
1. В чем разница между validation и test наборами?
2. Как правильно разделить данные на train/validation/test?
3. Что такое cross-validation и когда его использовать?
4. Как избежать data leakage между выборками?
5. Что делать, если validation loss начинает расти?

### Вопрос 12: Объясните концепцию learning rate
**Углубляющие вопросы:**
1. Что происходит при слишком большом learning rate?
2. Что происходит при слишком маленьком learning rate?
3. Что такое learning rate scheduling?
4. Как adaptive optimizers (Adam, RMSprop) работают с learning rate?
5. Что такое warmup и зачем он нужен?

### Вопрос 13: Что такое batch normalization?
**Углубляющие вопросы:**
1. Какую проблему решает batch normalization?
2. Как batch normalization влияет на градиенты?
3. В чем разница между batch norm во время обучения и инференса?
4. Что такое internal covariate shift?
5. Какие альтернативы batch normalization существуют?

### Вопрос 14: Объясните проблему исчезающих градиентов
**Углубляющие вопросы:**
1. Почему градиенты исчезают в глубоких сетях?
2. Как функции активации влияют на исчезновение градиентов?
3. Какие архитектурные решения помогают с этой проблемой?
4. Что такое gradient clipping и когда его применять?
5. Как инициализация весов связана с исчезающими градиентами?

### Вопрос 15: Что такое dropout и как он работает?
**Углубляющие вопросы:**
1. Почему dropout помогает против переобучения?
2. Как dropout влияет на forward pass во время инференса?
3. Что такое inverted dropout?
4. Можно ли применять dropout к любым слоям?
5. Как выбрать оптимальную вероятность dropout?

### Вопрос 16: Объясните различия между регрессией и классификацией
**Углубляющие вопросы:**
1. Какие функции активации используются для каждого типа задач?
2. Как отличаются функции потерь для регрессии и классификации?
3. Что такое softmax и почему он используется в классификации?
4. Как интерпретировать выходы нейросети для каждого типа задач?
5. Можно ли решать задачу классификации как регрессию?

### Вопрос 17: Что такое feature engineering в контексте нейросетей?
**Углубляющие вопросы:**
1. Нужен ли feature engineering для нейросетей?
2. Что такое representation learning?
3. Как нейросети автоматически извлекают признаки?
4. В каких случаях ручная обработка признаков все еще важна?
5. Как нормализация входных данных влияет на обучение?

### Вопрос 18: Объясните концепцию емкости модели (model capacity)
**Углубляющие вопросы:**
1. Как количество параметров связано с емкостью модели?
2. Что такое VC-dimension в контексте нейросетей?
3. Как глубина сети влияет на ее выразительную способность?
4. Что такое memorization vs generalization?
5. Как емкость модели связана с размером датасета?

### Вопрос 19: Что такое transfer learning?
**Углубляющие вопросы:**
1. Какие слои нейросети лучше всего переносятся между задачами?
2. Что такое fine-tuning и как его правильно применять?
3. В каких случаях transfer learning наиболее эффективен?
4. Как выбрать предобученную модель для transfer learning?
5. Что такое domain adaptation?

### Вопрос 20: Объясните концепцию ensemble методов в нейросетях
**Углубляющие вопросы:**
1. Как объединить предсказания нескольких моделей?
2. Что такое bagging и boosting в контексте нейросетей?
3. Как dropout связан с ensemble методами?
4. Что такое model averaging?
5. Какие вычислительные затраты связаны с ensemble подходами?

---

## 2. Функции активации (15 вопросов)

### Вопрос 21: Что такое функция активации и зачем она нужна?
**Углубляющие вопросы:**
1. Что произойдет с нейросетью без функций активации?
2. Какие свойства должна иметь хорошая функция активации?
3. Как функция активации влияет на градиенты?
4. Что такое saturating и non-saturating функции активации?
5. Как выбрать функцию активации для конкретного слоя?

### Вопрос 22: Объясните функцию активации ReLU
**Углубляющие вопросы:**
1. Почему ReLU стала так популярна в глубоком обучении?
2. Что такое "dying ReLU" проблема?
3. Как ReLU решает проблему исчезающих градиентов?
4. В каких случаях ReLU может быть неподходящей?
5. Как ReLU влияет на разреженность активаций?

### Вопрос 23: Что такое Leaky ReLU и чем она отличается от ReLU?
**Углубляющие вопросы:**
1. Как Leaky ReLU решает проблему "dying ReLU"?
2. Как выбрать коэффициент "утечки" в Leaky ReLU?
3. Что такое Parametric ReLU (PReLU)?
4. В каких задачах Leaky ReLU показывает лучшие результаты?
5. Как Leaky ReLU влияет на статистики активаций?

### Вопрос 24: Объясните функцию активации Sigmoid
**Углубляющие вопросы:**
1. Почему Sigmoid редко используется в скрытых слоях глубоких сетей?
2. В каких случаях Sigmoid все еще актуален?
3. Что такое vanishing gradient problem применительно к Sigmoid?
4. Как Sigmoid связан с логистической регрессией?
5. Что такое logit функция?

### Вопрос 25: Что такое функция Tanh и как она соотносится с Sigmoid?
**Углубляющие вопросы:**
1. Почему Tanh часто предпочтительнее Sigmoid?
2. Как центрирование вокруг нуля влияет на обучение?
3. В каких архитектурах Tanh все еще используется?
4. Как производная Tanh влияет на градиенты?
5. Что такое saturated режим для Tanh?

### Вопрос 26: Объясните Softmax функцию
**Углубляющие вопросы:**
1. Почему Softmax используется в задачах многоклассовой классификации?
2. Как Softmax превращает логиты в вероятности?
3. Что такое temperature в Softmax?
4. Как numerical stability связана с реализацией Softmax?
5. Что такое Gumbel-Softmax и зачем он нужен?

### Вопрос 27: Что такое GELU функция активации?
**Углубляющие вопросы:**
1. Какая мотивация стоит за GELU?
2. Как GELU связана с нормальным распределением?
3. В чем преимущества GELU перед ReLU?
4. Почему GELU популярна в трансформерах?
5. Как аппроксимировать GELU для быстрых вычислений?

### Вопрос 28: Объясните Swish функцию активации
**Углубляющие вопросы:**
1. Как была найдена Swish функция?
2. В чем преимущества Swish перед ReLU?
3. Что такое β-параметр в Swish?
4. Как Swish влияет на gradient flow?
5. В каких задачах Swish показывает лучшие результаты?

### Вопрос 29: Что такое Mish функция активации?
**Углубляющие вопросы:**
1. Какие свойства делают Mish привлекательной?
2. Как Mish сравнивается с другими функциями активации?
3. Что такое self-regularizing свойство Mish?
4. Какова вычислительная сложность Mish?
5. В каких архитектурах Mish наиболее эффективна?

### Вопрос 30: Объясните ELU (Exponential Linear Unit)
**Углубляющие вопросы:**
1. Как ELU решает проблемы ReLU?
2. Что такое α-параметр в ELU?
3. Как ELU влияет на mean activation?
4. Почему ELU может ускорять обучение?
5. Какие вычислительные затраты связаны с ELU?

### Вопрос 31: Что такое PReLU (Parametric ReLU)?
**Углубляющие вопросы:**
1. Как обучается параметр в PReLU?
2. В чем преимущества обучаемого параметра утечки?
3. Как PReLU влияет на количество параметров модели?
4. Может ли PReLU привести к переобучению?
5. Как инициализировать параметр PReLU?

### Вопрос 32: Объясните концепцию Maxout сетей
**Углубляющие вопросы:**
1. Как Maxout может аппроксимировать любую функцию активации?
2. Что такое piece-wise linear approximation?
3. Как Maxout связан с dropout?
4. Какие вычислительные затраты у Maxout?
5. В каких случаях Maxout предпочтительнее обычных активаций?

### Вопрос 33: Что такое Hard Swish и Hard Sigmoid?
**Углубляющие вопросы:**
1. Зачем нужны "hard" версии функций активации?
2. Как Hard Swish аппроксимирует обычный Swish?
3. Какие преимущества для мобильных устройств дают hard активации?
4. Как точность аппроксимации влияет на качество модели?
5. В каких архитектурах используются hard активации?

### Вопрос 34: Объясните FReLU (Funnel ReLU)
**Углубляющие вопросы:**
1. Какую проблему решает FReLU?
2. Как пространственная зависимость встроена в FReLU?
3. В каких задачах FReLU наиболее эффективна?
4. Как FReLU влияет на вычислительную сложность?
5. Можно ли применять FReLU в любых слоях?

### Вопрос 35: Как выбрать функцию активации для конкретной задачи?
**Углубляющие вопросы:**
1. Какие факторы влияют на выбор функции активации?
2. Как архитектура сети влияет на выбор активации?
3. Есть ли универсальные рекомендации по выбору активаций?
4. Как тестировать разные функции активации?
5. Можно ли использовать разные активации в разных слоях?

---

## 3. Функции потерь и метрики (18 вопросов)

### Вопрос 36: Что такое Mean Squared Error (MSE)?
**Углубляющие вопросы:**
1. В каких задачах используется MSE?
2. Почему MSE чувствительна к выбросам?
3. Как MSE связана с максимальным правдоподобием для нормального распределения?
4. Что такое RMSE и в чем ее преимущества?
5. Когда MSE может быть неподходящей функцией потерь?

### Вопрос 37: Объясните Cross-Entropy Loss
**Углубляющие вопросы:**
1. Как Cross-Entropy связана с информационной теорией?
2. В чем разница между Binary и Categorical Cross-Entropy?
3. Почему Cross-Entropy предпочтительнее MSE для классификации?
4. Что такое log-likelihood и как он связан с Cross-Entropy?
5. Как численно стабильно вычислить Cross-Entropy?

### Вопрос 38: Что такое Focal Loss?
**Углубляющие вопросы:**
1. Какую проблему решает Focal Loss?
2. Как параметры α и γ влияют на поведение Focal Loss?
3. В каких задачах Focal Loss наиболее эффективна?
4. Как Focal Loss связана с hard negative mining?
5. Можно ли применять Focal Loss к многоклассовой классификации?

### Вопрос 39: Объясните Huber Loss
**Углубляющие вопросы:**
1. Какие преимущества Huber Loss перед MSE и MAE?
2. Как выбрать параметр δ в Huber Loss?
3. В каких задачах Huber Loss предпочтительнее?
4. Как Huber Loss влияет на градиенты?
5. Что такое robust regression в контексте Huber Loss?

### Вопрос 40: Что такое Contrastive Loss?
**Углубляющие вопросы:**
1. В каких задачах используется Contrastive Loss?
2. Как Contrastive Loss обучает similarity/dissimilarity?
3. Что такое margin в Contrastive Loss?
4. Как формировать пары для Contrastive Loss?
5. В чем разница между Contrastive и Triplet Loss?

### Вопрос 41: Объясните Triplet Loss
**Углубляющие вопросы:**
1. Что такое anchor, positive и negative в Triplet Loss?
2. Как выбирать тройки для эффективного обучения?
3. Что такое hard negative mining в контексте Triplet Loss?
4. Как margin влияет на обучение с Triplet Loss?
5. В каких задачах Triplet Loss наиболее эффективна?

### Вопрос 42: Что такое Dice Loss?
**Углубляющие вопросы:**
1. В каких задачах используется Dice Loss?
2. Как Dice Loss связана с Dice коэффициентом?
3. Почему Dice Loss эффективна при class imbalance?
4. Как сгладить Dice Loss для стабильного обучения?
5. Можно ли комбинировать Dice Loss с другими функциями потерь?

### Вопрос 43: Объясните IoU Loss
**Углубляющие вопросы:**
1. Что такое Intersection over Union?
2. Как превратить IoU метрику в дифференцируемую функцию потерь?
3. В каких задачах IoU Loss предпочтительнее Cross-Entropy?
4. Что такое GIoU, DIoU, CIoU?
5. Как IoU Loss влияет на качество локализации объектов?

### Вопрос 44: Что такое Wasserstein Loss?
**Углубляющие вопросы:**
1. Как Wasserstein distance используется в машинном обучении?
2. В чем преимущества Wasserstein Loss перед KL-divergence?
3. Как аппроксимировать Wasserstein distance для практических задач?
4. В каких задачах Wasserstein Loss наиболее эффективна?
5. Что такое Sinkhorn divergence?

### Вопрос 45: Объясните Label Smoothing
**Углубляющие вопросы:**
1. Как Label Smoothing влияет на обучение?
2. Почему Label Smoothing может улучшать генерализацию?
3. Как выбрать параметр smoothing?
4. В каких задачах Label Smoothing наиболее эффективно?
5. Может ли Label Smoothing навредить качеству модели?

### Вопрос 46: Что такое Class Weighting в функциях потерь?
**Углубляющие вопросы:**
1. Как вычислить веса классов для несбалансированных данных?
2. В чем разница между class weighting и resampling?
3. Как class weighting влияет на градиенты?
4. Можно ли использовать динамические веса классов?
5. Какие метрики лучше использовать при class weighting?

### Вопрос 47: Объясните метрику Accuracy
**Углубляющие вопросы:**
1. В каких случаях Accuracy может быть обманчивой?
2. Что такое Balanced Accuracy?
3. Как Accuracy связана с другими метриками классификации?
4. Можно ли использовать Accuracy как функцию потерь?
5. Что такое Top-k Accuracy?

### Вопрос 48: Что такое Precision и Recall?
**Углубляющие вопросы:**
1. Как интерпретировать Precision и Recall в разных задачах?
2. Что такое Precision-Recall curve?
3. Как Precision и Recall связаны с confusion matrix?
4. В каких случаях важнее Precision, а в каких Recall?
5. Что такое macro и micro усреднение для многоклассовой классификации?

### Вопрос 49: Объясните F1-score
**Углубляющие вопросы:**
1. Почему F1-score является гармоническим средним?
2. Что такое F-beta score и как выбрать β?
3. Как F1-score ведет себя при class imbalance?
4. Можно ли использовать F1-score как функцию потерь?
5. Что такое weighted F1-score?

### Вопрос 50: Что такое ROC-AUC?
**Углубляющие вопросы:**
1. Как интерпретировать ROC кривую?
2. Что такое True Positive Rate и False Positive Rate?
3. В каких случаях ROC-AUC может быть неинформативной?
4. Что такое PR-AUC и когда она предпочтительнее ROC-AUC?
5. Как ROC-AUC обобщается на многоклассовую классификацию?

### Вопрос 51: Объясните Mean Absolute Error (MAE)
**Углубляющие вопросы:**
1. В чем преимущества MAE перед MSE?
2. Почему MAE менее чувствительна к выбросам?
3. Как MAE влияет на градиенты при обучении?
4. В каких задачах MAE предпочтительнее?
5. Что такое Mean Absolute Percentage Error (MAPE)?

### Вопрос 52: Что такое Perplexity в языковых моделях?
**Углубляющие вопросы:**
1. Как Perplexity связана с Cross-Entropy?
2. Как интерпретировать значения Perplexity?
3. Почему Perplexity важна для языковых моделей?
4. Как сравнивать модели по Perplexity?
5. Что такое bits per character метрика?

### Вопрос 53: Объясните BLEU метрику
**Углубляющие вопросы:**
1. Как BLEU оценивает качество машинного перевода?
2. Что такое n-gram precision в BLEU?
3. Как brevity penalty влияет на BLEU?
4. Какие ограничения есть у BLEU метрики?
5. Какие альтернативы BLEU существуют (ROUGE, METEOR, BERTScore)?

---

## 4. Оптимизация и градиенты (15 вопросов)

### Вопрос 54: Объясните алгоритм градиентного спуска
**Углубляющие вопросы:**
1. Почему мы движемся в направлении, противоположном градиенту?
2. Как выбрать learning rate для градиентного спуска?
3. Что такое momentum и как он помогает оптимизации?
4. В чем разница между batch, mini-batch и stochastic градиентным спуском?
5. Как градиентный спуск может застревать в локальных минимумах?

### Вопрос 55: Что такое Adam optimizer?
**Углубляющие вопросы:**
1. Как Adam адаптирует learning rate для каждого параметра?
2. Что такое первый и второй моменты в Adam?
3. Зачем нужна bias correction в Adam?
4. Какие гиперпараметры есть у Adam и как их настраивать?
5. В каких случаях Adam может работать хуже SGD?

### Вопрос 56: Объясните RMSprop optimizer
**Углубляющие вопросы:**
1. Какую проблему решает RMSprop?
2. Как RMSprop связан с AdaGrad?
3. Что такое decay rate в RMSprop?
4. Как RMSprop работает с разреженными градиентами?
5. В каких задачах RMSprop предпочтительнее Adam?

### Вопрос 57: Что такое AdaGrad optimizer?
**Углубляющие вопросы:**
1. Как AdaGrad адаптирует learning rate?
2. Почему AdaGrad может со временем останавливать обучение?
3. Что такое диагональная аппроксимация в AdaGrad?
4. Как AdaGrad работает с разреженными данными?
5. Какие модификации AdaGrad существуют?

### Вопрос 58: Объясните концепцию momentum в оптимизации
**Углубляющие вопросы:**
1. Как momentum помогает преодолевать локальные минимумы?
2. Что такое Nesterov momentum?
3. Как выбрать коэффициент momentum?
4. Что происходит с momentum в начале обучения?
5. Как momentum влияет на сходимость оптимизации?

### Вопрос 59: Что такое learning rate scheduling?
**Углубляющие вопросы:**
1. Какие стратегии scheduling существуют?
2. Что такое step decay, exponential decay, cosine annealing?
3. Как warmup влияет на начальные этапы обучения?
4. Что такое cyclical learning rates?
5. Как автоматически подбирать learning rate?

### Вопрос 60: Объясните AdamW optimizer
**Углубляющие вопросы:**
1. В чем разница между Adam и AdamW?
2. Как weight decay реализован в AdamW?
3. Почему AdamW лучше для трансформеров?
4. Что такое decoupled weight decay?
5. Как AdamW влияет на регуляризацию?

### Вопрос 61: Что такое gradient clipping?
**Углубляющие вопросы:**
1. Какие проблемы решает gradient clipping?
2. В чем разница между clipping by value и clipping by norm?
3. Как выбрать threshold для clipping?
4. В каких архитектурах gradient clipping особенно важен?
5. Может ли gradient clipping навредить обучению?

### Вопрос 62: Объясните концепцию adaptive learning rates
**Углубляющие вопросы:**
1. Что такое per-parameter learning rates?
2. Как вычисляется adaptive learning rate в Adam?
3. Что такое AdaBound и AdaBelief optimizers?
4. В чем преимущества и недостатки adaptive методов?
5. Как adaptive optimizers работают с batch normalization?

### Вопрос 63: Что такое второй порядок оптимизации?
**Углубляющие вопросы:**
1. Что такое метод Ньютона в оптимизации?
2. Почему вторые производные редко используются в глубоком обучении?
3. Что такое quasi-Newton методы (L-BFGS)?
4. Как аппроксимировать Hessian матрицу?
5. В каких случаях второй порядок может быть полезен?

### Вопрос 64: Объясните natural gradient descent
**Углубляющие вопросы:**
1. Что такое natural gradient и как он отличается от обычного?
2. Что такое Fisher Information Matrix?
3. Как natural gradient связан с KL-divergence?
4. Почему natural gradient дорого вычислять?
5. Какие аппроксимации natural gradient существуют?

### Вопрос 65: Что такое gradient accumulation?
**Углубляющие вопросы:**
1. Зачем нужен gradient accumulation?
2. Как gradient accumulation влияет на эффективный batch size?
3. Как правильно реализовать gradient accumulation?
4. Влияет ли accumulation на batch normalization?
5. Какие memory benefits дает gradient accumulation?

### Вопрос 66: Объясните концепцию gradient noise
**Углубляющие вопросы:**
1. Откуда берется шум в градиентах при SGD?
2. Как batch size влияет на уровень gradient noise?
3. Может ли gradient noise помогать обобщению?
4. Что такое gradient noise injection?
5. Как измерить уровень gradient noise?

### Вопрос 67: Что такое learning rate finder?
**Углубляющие вопросы:**
1. Как работает алгоритм поиска learning rate?
2. Что показывает график loss vs learning rate?
3. Как интерпретировать результаты learning rate finder?
4. В каких случаях learning rate finder может обманывать?
5. Как адаптировать найденный learning rate для длительного обучения?

### Вопрос 68: Объясните LAMB optimizer
**Углубляющие вопросы:**
1. Для каких задач был разработан LAMB?
2. Как LAMB масштабируется на большие batch sizes?
3. Что такое layer-wise adaptive learning rates?
4. Как LAMB сравнивается с Adam на больших батчах?
5. В каких случаях LAMB предпочтительнее других optimizers?

---

## 5. Обратное распространение ошибки (12 вопросов)

### Вопрос 69: Что такое алгоритм обратного распространения ошибки?
**Углубляющие вопросы:**
1. Как математически описать backpropagation?
2. Почему backpropagation эффективнее numerical differentiation?
3. Что такое chain rule и как он применяется в backpropagation?
4. Как backpropagation связан с automatic differentiation?
5. Какова вычислительная сложность backpropagation?

### Вопрос 70: Объясните forward и backward pass подробно
**Углубляющие вопросы:**
1. Что происходит на каждом шаге forward pass?
2. Как строится computational graph?
3. В каком порядке выполняется backward pass?
4. Как сохраняются промежуточные значения для backprop?
5. Что такое gradient checkpointing?

### Вопрос 71: Как вычисляются градиенты для разных операций?
**Углубляющие вопросы:**
1. Как вычислить градиент для матричного умножения?
2. Как backprop работает через нелинейные функции?
3. Что происходит с градиентами при элементарных операциях?
4. Как вычисляются градиенты для convolution операций?
5. Как backprop работает через branching в графе?

### Вопрос 72: Что такое automatic differentiation?
**Углубляющие вопросы:**
1. В чем разница между forward и reverse mode AD?
2. Как AD реализован в современных фреймворках?
3. Что такое dual numbers в forward mode AD?
4. Как reverse mode AD связан с backpropagation?
5. Какие преимущества AD перед символическим дифференцированием?

### Вопрос 73: Объясните проблему исчезающих/взрывающихся градиентов
**Углубляющие вопросы:**
1. Почему градиенты исчезают в глубоких сетях?
2. Как функции активации влияют на magnitude градиентов?
3. Что такое gradient flow и как его анализировать?
4. Какие архитектурные решения помогают с gradient flow?
5. Как инициализация весов влияет на градиенты?

### Вопрос 74: Что такое gradient checkpointing?
**Углубляющие вопросы:**
1. Какую проблему решает gradient checkpointing?
2. Как выбрать точки для checkpointing?
3. Какой trade-off между памятью и вычислениями в checkpointing?
4. Как gradient checkpointing влияет на время обучения?
5. В каких случаях gradient checkpointing критически важен?

### Вопрос 75: Объясните higher-order gradients
**Углубляющие вопросы:**
1. Что такое градиенты второго порядка?
2. Как вычислить Hessian матрицу через backprop?
3. Зачем могут понадобиться higher-order gradients?
4. Что такое meta-learning и как там используются градиенты градиентов?
5. Какова вычислительная сложность higher-order gradients?

### Вопрос 76: Что такое в контексте backprop?
**Углубляющие вопросы:**
1. Как backprop работает с рекуррентными связями?
2. Что такое unrolling в RNN backpropagation?
3. Как обрезать backprop through time?
4. Почему длинные последовательности проблематичны для BPTT?
5. Какие альтернативы BPTT существуют?

### Вопрос 77: Объясните gradient surgery
**Углубляющие вопросы:**
1. Что такое gradient surgery и когда он нужен?
2. Как проекция градиентов помогает в multi-task learning?
3. Что такое gradient interference между задачами?
4. Как PCGrad (Project Conflicting Gradients) работает?
5. В каких еще задачах применим gradient surgery?

### Вопрос 78: Что такое differentiable programming?
**Углубляющие вопросы:**
1. Как backprop обобщается на произвольные программы?
2. Что такое дифференцируемые структуры данных?
3. Как сделать дискретные операции дифференцируемыми?
4. Что такое straight-through estimator?
5. Какие ограничения у differentiable programming?

### Вопрос 79: Объясните memory-efficient backpropagation
**Углубляющие вопросы:**
1. Как оптимизировать использование памяти в backprop?
2. Что такое in-place operations и когда они безопасны?
3. Как gradient accumulation экономит память?
4. Что такое reversible layers?
5. Как mixed precision влияет на memory efficiency?

### Вопрос 80: Что такое distributed backpropagation?
**Углубляющие вопросы:**
1. Как синхронизировать градиенты между устройствами?
2. Что такое parameter server архитектура?
3. Как all-reduce операции работают в distributed training?
4. Что такое gradient compression для распределенного обучения?
5. Как asynchronous SGD отличается от synchronous?

---

## 6. Bias, регуляризация и нормализация (15 вопросов)

### Вопрос 81: Что такое bias в машинном обучении?
**Углубляющие вопросы:**
1. В чем разница между bias в нейроне и algorithmic bias?
2. Как bias влияет на выразительную способность модели?
3. Что происходит, если убрать все bias из сети?
4. Как инициализировать bias в разных слоях?
5. Может ли bias приводить к переобучению?

### Вопрос 82: Объясните bias-variance tradeoff
**Углубляющие вопросы:**
1. Как декомпозировать ошибку на bias, variance и noise?
2. Как complexity модели влияет на bias и variance?
3. Что такое irreducible error?
4. Как bias-variance tradeoff проявляется в нейросетях?
5. Какие техники помогают балансировать bias и variance?

### Вопрос 83: Что такое L1 и L2 регуляризация?
**Углубляющие вопросы:**
1. Как L1 регуляризация способствует sparsity?
2. Почему L2 регуляризация называется weight decay?
3. Как выбрать коэффициент регуляризации?
4. В чем геометрическая интерпретация L1 и L2 регуляризации?
5. Можно ли комбинировать L1 и L2 регуляризацию?

### Вопрос 84: Объясните dropout подробнее
**Углубляющие вопросы:**
1. Почему dropout работает как ensemble метод?
2. Как dropout влияет на co-adaptation нейронов?
3. Что такое DropConnect и чем он отличается от dropout?
4. Как применять dropout к сверточным слоям?
5. Почему dropout может мешать batch normalization?

### Вопрос 85: Что такое batch normalization?
**Углубляющие вопросы:**
1. Как batch norm влияет на internal covariate shift?
2. Почему batch norm позволяет использовать большие learning rates?
3. Как batch norm работает во время inference?
4. Что такое running statistics в batch norm?
5. В каких случаях batch norm может навредить?

### Вопрос 86: Объясните layer normalization
**Углубляющие вопросы:**
1. В чем преимущества layer norm перед batch norm?
2. Почему layer norm лучше для RNN и трансформеров?
3. Как layer norm работает с переменной длиной последовательностей?
4. Что такое RMSNorm и в чем его преимущества?
5. Когда использовать layer norm вместо batch norm?

### Вопрос 87: Что такое group normalization?
**Углубляющие вопросы:**
1. Какую проблему решает group normalization?
2. Как выбрать количество групп в group norm?
3. В каких задачах group norm предпочтительнее?
4. Как group norm связана с instance normalization?
5. Что происходит при крайних значениях групп (1 группа vs N групп)?

### Вопрос 88: Объясните instance normalization
**Углубляющие вопросы:**
1. В каких задачах используется instance normalization?
2. Почему instance norm эффективна для style transfer?
3. Как instance norm влияет на spatial statistics?
4. В чем разница между instance norm и layer norm?
5. Можно ли комбинировать instance norm с другими нормализациями?

### Вопрос 89: Что такое spectral normalization?
**Углубляющие вопросы:**
1. Какую проблему решает spectral normalization?
2. Что такое spectral norm матрицы весов?
3. Как spectral norm влияет на Lipschitz константу?
4. В каких архитектурах spectral norm наиболее полезна?
5. Как efficiently вычислять spectral norm?

### Вопрос 90: Объясните weight decay
**Углубляющие вопросы:**
1. В чем разница между L2 регуляризацией и weight decay?
2. Как weight decay реализован в разных optimizers?
3. Почему AdamW отделяет weight decay от градиентного шага?
4. Как выбрать коэффициент weight decay?
5. К каким параметрам применять weight decay?

### Вопрос 91: Что такое early stopping?
**Углубляющие вопросы:**
1. Как выбрать критерий для early stopping?
2. Что такое patience в early stopping?
3. Как early stopping влияет на reproducibility?
4. Можно ли комбинировать early stopping с другими техниками?
5. Как early stopping работает с learning rate scheduling?

### Вопрос 92: Объясните data augmentation как регуляризацию
**Углубляющие вопросы:**
1. Как data augmentation помогает генерализации?
2. Какие виды augmentation существуют для разных типов данных?
3. Что такое consistency regularization?
4. Как выбрать интенсивность augmentation?
5. Может ли excessive augmentation навредить?

### Вопрос 93: Что такое noise injection?
**Углубляющие вопросы:**
1. Как добавление шума помогает регуляризации?
2. К каким частям модели можно добавлять шум?
3. Что такое Gaussian noise injection?
4. Как noise injection связана с robustness?
5. Как выбрать уровень шума?

### Вопрос 94: Объясните mixup и cutmix
**Углубляющие вопросы:**
1. Как mixup изменяет training distribution?
2. Почему mixup помогает калибровке модели?
3. В чем разница между mixup и cutmix?
4. Как реализовать mixup для разных типов задач?
5. Какие гиперпараметры важны для mixup?

### Вопрос 95: Что такое knowledge distillation?
**Углубляющие вопросы:**
1. Как knowledge distillation работает как регуляризация?
2. Что такое soft targets в distillation?
3. Как выбрать teacher модель для distillation?
4. Что такое self-distillation?
5. Можно ли использовать distillation с multiple teachers?

---

## 7. Сверточные сети (CNN) (18 вопросов)

### Вопрос 96: Что такое сверточный слой и как он работает?
**Углубляющие вопросы:**
1. Как математически описать операцию свертки?
2. Что такое kernel, stride, padding в свертке?
3. Почему свертки эффективны для изображений?
4. Как вычислить размер выходного feature map?
5. Что такое dilated convolutions?

### Вопрос 97: Объясните концепцию параметрического разделения в CNN
**Углубляющие вопросы:**
1. Что такое parameter sharing в сверточных слоях?
2. Как parameter sharing связано с translation invariance?
3. Почему CNN требуют меньше параметров чем fully connected?
4. Что такое local connectivity в CNN?
5. Как parameter sharing влияет на градиенты?

### Вопрос 98: Что такое pooling слои?
**Углубляющие вопросы:**
1. В чем разница между max pooling и average pooling?
2. Как pooling влияет на spatial dimensions?
3. Что такое global average pooling?
4. Почему pooling помогает с overfitting?
5. Какие альтернативы pooling существуют?

### Вопрос 99: Объясните архитектуру LeNet
**Углубляющие вопросы:**
1. Какие принципы заложены в LeNet?
2. Как LeNet обрабатывает spatial hierarchy?
3. Что такое sparse connectivity в LeNet?
4. Как LeNet повлияла на современные архитектуры?
5. Какие ограничения были у оригинальной LeNet?

### Вопрос 100: Что такое AlexNet и его вклад в глубокое обучение?
**Углубляющие вопросы:**
1. Какие инновации принес AlexNet?
2. Как ReLU активации помогли AlexNet?
3. Что такое dropout в контексте AlexNet?
4. Как data augmentation использовалась в AlexNet?
5. Почему AlexNet стала breakthrough в computer vision?

### Вопрос 101: Объясните архитектуру VGG
**Углубляющие вопросы:**
1. Какой принцип лежит в основе VGG архитектуры?
2. Почему VGG использует только 3x3 фильтры?
3. Как несколько 3x3 фильтров заменяют один большой?
4. Что такое очень глубокие сети в контексте VGG?
5. Какие проблемы возникли с глубиной в VGG?

### Вопрос 102: Что такое ResNet и residual connections?
**Углубляющие вопросы:**
1. Какую проблему решают residual connections?
2. Как skip connections помогают gradient flow?
3. Что такое identity mapping в ResNet?
4. Как residual блоки математически описываются?
5. Почему ResNet позволили обучать очень глубокие сети?

### Вопрос 103: Объясните DenseNet архитектуру
**Углубляющие вопросы:**
1. В чем отличие DenseNet от ResNet?
2. Что такое dense connectivity?
3. Как DenseNet решает проблему feature reuse?
4. Что такое transition layers в DenseNet?
5. Какие преимущества и недостатки у DenseNet?

### Вопрос 104: Что такое Inception архитектура?
**Углубляющие вопросы:**
1. Какую идею воплощает Inception модуль?
2. Как Inception использует фильтры разных размеров?
3. Что такое 1x1 convolutions в Inception?
4. Как Inception решает computational efficiency?
5. Что такое auxiliary classifiers в Inception?

### Вопрос 105: Объясните MobileNet архитектуру
**Углубляющие вопросы:**
1. Что такое depthwise separable convolutions?
2. Как MobileNet достигает efficiency?
3. Что такое width multiplier и resolution multiplier?
4. Как MobileNet сравнивается с обычными CNN по точности?
5. В каких применениях MobileNet наиболее полезна?

### Вопрос 106: Что такое EfficientNet?
**Углубляющие вопросы:**
1. Что такое compound scaling в EfficientNet?
2. Как балансировать depth, width и resolution?
3. Что такое neural architecture search в контексте EfficientNet?
4. Как EfficientNet достигает sota results с меньшими ресурсами?
5. Какие принципы scaling можно применить к другим архитектурам?

### Вопрос 107: Объясните 1x1 convolutions
**Углубляющие вопросы:**
1. Зачем нужны 1x1 свертки?
2. Как 1x1 convolutions изменяют количество каналов?
3. Что такое bottleneck layers?
4. Как 1x1 свертки добавляют нелинейность?
5. В каких архитектурах 1x1 convolutions критически важны?

### Вопрос 108: Что такое grouped convolutions?
**Углубляющие вопросы:**
1. Как grouped convolutions экономят computation?
2. Что происходит при группировке каналов?
3. Как grouped convolutions влияют на cross-channel information?
4. В каких архитектурах используются grouped convolutions?
5. Что такое shuffled grouped convolutions?

### Вопрос 109: Объясните dilated (atrous) convolutions
**Углубляющие вопросы:**
1. Как dilated convolutions увеличивают receptive field?
2. Что такое dilation rate?
3. В каких задачах dilated convolutions особенно полезны?
4. Как dilated convolutions используются в semantic segmentation?
5. Что такое atrous spatial pyramid pooling?

### Вопрос 110: Что такое transposed convolutions?
**Углубляющие вопросы:**
1. Как transposed convolutions связаны с upsampling?
2. Почему transposed convolutions иногда называют deconvolutions?
3. Как математически описать transposed convolution?
4. В каких задачах используются transposed convolutions?
5. Что такое checkerboard artifacts в transposed convolutions?

### Вопрос 111: Объясните receptive field в CNN
**Углубляющие вопросы:**
1. Как вычислить effective receptive field?
2. Как глубина сети влияет на receptive field?
3. Как pooling влияет на receptive field?
4. Что такое theoretical vs effective receptive field?
5. Как receptive field связан с spatial reasoning?

### Вопрос 112: Что такое Feature Pyramid Networks (FPN)?
**Углубляющие вопросы:**
1. Какую проблему решает FPN?
2. Как FPN комбинирует features разного уровня?
3. Что такое top-down pathway в FPN?
4. В каких задачах FPN наиболее эффективна?
5. Как FPN интегрируется с object detection networks?

### Вопрос 113: Объясните архитектуру U-Net
**Углубляющие вопросы:**
1. Для каких задач была разработана U-Net?
2. Что такое encoder-decoder архитектура?
3. Как skip connections работают в U-Net?
4. Почему U-Net эффективна для segmentation?
5. Какие модификации U-Net существуют?

---

## 8. Рекуррентные сети (RNN/LSTM/GRU) (12 вопросов)

### Вопрос 114: Что такое рекуррентные нейронные сети (RNN)?
**Углубляющие вопросы:**
1. Как RNN обрабатывают последовательности?
2. Что такое hidden state в RNN?
3. Как математически описать RNN?
4. Что такое parameter sharing во времени в RNN?
5. В каких задачах RNN наиболее применимы?

### Вопрос 115: Объясните проблему vanishing gradients в RNN
**Углубляющие вопросы:**
1. Почему градиенты исчезают в длинных последовательностях?
2. Как backpropagation through time связано с vanishing gradients?
3. Что такое gradient clipping в контексте RNN?
4. Как функции активации влияют на gradient flow в RNN?
5. Какие архитектурные решения помогают с vanishing gradients?

### Вопрос 116: Что такое LSTM (Long Short-Term Memory)?
**Углубляющие вопросы:**
1. Как LSTM решает проблему long-term dependencies?
2. Что такое cell state и hidden state в LSTM?
3. Объясните работу forget gate, input gate, output gate
4. Как gates контролируют information flow в LSTM?
5. Почему LSTM может запоминать long-term информацию?

### Вопрос 117: Объясните архитектуру GRU (Gated Recurrent Unit)
**Углубляющие вопросы:**
1. В чем отличие GRU от LSTM?
2. Что такое reset gate и update gate в GRU?
3. Как GRU упрощает архитектуру LSTM?
4. В каких случаях GRU предпочтительнее LSTM?
5. Как производительность GRU сравнивается с LSTM?

### Вопрос 118: Что такое bidirectional RNN?
**Углубляющие вопросы:**
1. Как bidirectional RNN обрабатывает информацию?
2. В каких задачах bidirectional RNN полезны?
3. Как объединяются forward и backward hidden states?
4. Какие ограничения у bidirectional RNN?
5. Можно ли использовать bidirectional RNN для real-time задач?

### Вопрос 119: Объясните attention mechanism в RNN
**Углубляющие вопросы:**
1. Какую проблему решает attention в RNN?
2. Как вычисляются attention weights?
3. Что такое context vector в attention?
4. Как attention помогает с long sequences?
5. В чем разница между additive и multiplicative attention?

### Вопрос 120: Что такое sequence-to-sequence модели?
**Углубляющие вопросы:**
1. Как работает encoder-decoder архитектура?
2. Что такое context vector в seq2seq?
3. Как обучаются seq2seq модели?
4. Что такое teacher forcing в seq2seq обучении?
5. В каких задачах seq2seq модели применяются?

### Вопрос 121: Объясните beam search в RNN
**Углубляющие вопросы:**
1. Как beam search улучшает generation quality?
2. Что такое beam width и как его выбирать?
3. В чем разница между greedy search и beam search?
4. Как beam search влияет на diversity генерации?
5. Какие альтернативы beam search существуют?

### Вопрос 122: Что такое packed sequences в RNN?
**Углубляющие вопросы:**
1. Зачем нужны packed sequences?
2. Как обрабатывать sequences разной длины эффективно?
3. Что такое padding и masking в RNN?
4. Как packed sequences влияют на computational efficiency?
5. Как правильно реализовать packed sequences?

### Вопрос 123: Объясните residual connections в RNN
**Углубляющие вопросы:**
1. Как residual connections помогают в RNN?
2. Где добавлять skip connections в RNN архитектуре?
3. Как residual RNN влияют на gradient flow?
4. В каких случаях residual connections в RNN наиболее полезны?
5. Можно ли комбинировать residual connections с attention?

### Вопрос 124: Что такое hierarchical RNN?
**Углубляющие вопросы:**
1. Как hierarchical RNN обрабатывают структурированные данные?
2. Что такое multi-scale temporal modeling?
3. Как можно построить hierarchy в RNN?
4. В каких задачах hierarchical RNN эффективны?
5. Как hierarchical RNN связаны с compositional representations?

### Вопрос 125: Объясните конволюционные RNN
**Углубляющие вопросы:**
1. Как комбинировать convolutions с recurrent processing?
2. Что такое ConvLSTM и ConvGRU?
3. В каких задачах конволюционные RNN полезны?
4. Как spatial и temporal информация объединяются в ConvRNN?
5. Какие computational benefits дают конволюционные RNN?

---

## 9. Трансформеры и Attention (20 вопросов)

### Вопрос 126: Что такое attention mechanism?
**Углубляющие вопросы:**
1. Какую проблему решает attention в глубоком обучении?
2. Как вычисляются attention weights?
3. Что такое query, key, value в attention?
4. В чем интуиция за attention механизмом?
5. Как attention связан с человеческим вниманием?

### Вопрос 127: Объясните scaled dot-product attention
**Углубляющие вопросы:**
1. Почему используется scaling в dot-product attention?
2. Как математически описать scaled dot-product attention?
3. Что происходит без scaling при больших размерностях?
4. Как softmax влияет на attention distribution?
5. Какие альтернативы dot-product attention существуют?

### Вопрос 128: Что такое multi-head attention?
**Углубляющие вопросы:**
1. Зачем нужно несколько attention heads?
2. Как heads обрабатывают разные типы зависимостей?
3. Как объединяются outputs разных heads?
4. Как выбрать количество heads?
5. Что происходит в каждом attention head?

### Вопрос 129: Объясните self-attention механизм
**Углубляющие вопросы:**
1. В чем отличие self-attention от cross-attention?
2. Как self-attention моделирует зависимости в последовательности?
3. Что такое contextual representations в self-attention?
4. Как self-attention сравнивается с RNN по modeling capacity?
5. Какова вычислительная сложность self-attention?

### Вопрос 130: Что такое архитектура Transformer?
**Углубляющие вопросы:**
1. Как устроена архитектура Transformer?
2. Что такое encoder-decoder структура в Transformer?
3. Как residual connections используются в Transformer?
4. Что такое layer normalization в Transformer блоках?
5. Почему Transformer не нуждается в recurrence?

### Вопрос 131: Объясните позиционное кодирование (positional encoding)
**Углубляющие вопросы:**
1. Зачем нужно positional encoding в Transformer?
2. Как работает sinusoidal positional encoding?
3. Что такое learned positional embeddings?
4. Как positional encoding влияет на длинные последовательности?
5. Какие альтернативы sinusoidal encoding существуют?

### Вопрос 132: Что такое masked attention?
**Углубляющие вопросы:**
1. Зачем нужно masking в attention?
2. Как работает causal masking в decoder?
3. Что такое padding mask?
4. Как реализовать attention masking эффективно?
5. В каких задачах используются разные типы масок?

### Вопрос 133: Объясните BERT архитектуру
**Углубляющие вопросы:**
1. Что такое bidirectional encoding в BERT?
2. Как работает masked language modeling?
3. Что такое next sentence prediction задача?
4. Как BERT fine-tuning работает для downstream задач?
5. В чем преимущества BERT перед предыдущими моделями?

### Вопрос 134: Что такое GPT архитектура?
**Углубляющие вопросы:**
1. В чем отличие GPT от BERT?
2. Как работает autoregressive language modeling?
3. Что такое causal self-attention в GPT?
4. Как GPT обучается генерировать текст?
5. Как scaling влияет на capabilities GPT моделей?

### Вопрос 135: Объясните T5 (Text-to-Text Transfer Transformer)
**Углубляющие вопросы:**
1. Что такое text-to-text подход?
2. Как T5 унифицирует разные NLP задачи?
3. Как работает span corruption в T5?
4. В чем преимущества text-to-text формата?
5. Как T5 сравнивается с BERT и GPT?

### Вопрос 136: Что такое Vision Transformer (ViT)?
**Углубляющие вопросы:**
1. Как Transformer применяется к изображениям?
2. Что такое patch embeddings в ViT?
3. Как ViT обрабатывает spatial информацию?
4. В каких условиях ViT превосходит CNN?
5. Какие модификации ViT существуют?

### Вопрос 137: Объясните cross-attention
**Углубляющие вопросы:**
1. Как cross-attention работает в encoder-decoder?
2. В чем разница между self-attention и cross-attention?
3. Как cross-attention соединяет разные модальности?
4. В каких задачах cross-attention критически важен?
5. Как efficient реализовать cross-attention?

### Вопрос 138: Что такое sparse attention?
**Углубляющие вопросы:**
1. Какую проблему решает sparse attention?
2. Какие паттерны sparsity используются в attention?
3. Что такое local attention windows?
4. Как sparse attention влияет на modeling capability?
5. Какие библиотеки поддерживают sparse attention?

### Вопрос 139: Объясните linear attention
**Углубляющие вопросы:**
1. Как linear attention снижает computational complexity?
2. Что такое kernel trick в linear attention?
3. Как linear attention аппроксимирует standard attention?
4. В каких случаях linear attention предпочтительнее?
5. Какие trade-offs у linear attention?

### Вопрос 140: Что такое rotary position embedding (RoPE)?
**Углубляющие вопросы:**
1. Как RoPE кодирует позиционную информацию?
2. Что такое rotation в RoPE?
3. Как RoPE обобщается на длинные последовательности?
4. В чем преимущества RoPE перед sinusoidal encoding?
5. В каких моделях используется RoPE?

### Вопрос 141: Объясните attention weights visualization
**Углубляющие вопросы:**
1. Что показывают attention weights?
2. Как интерпретировать attention patterns?
3. Соответствуют ли attention weights важности features?
4. Какие инструменты существуют для attention analysis?
5. Как attention patterns различаются между tasks?

### Вопрос 142: Что такое attention regularization?
**Углубляющие вопросы:**
1. Зачем регуляризовать attention weights?
2. Что такое attention dropout?
3. Как sparse attention patterns помогают интерпретируемости?
4. Что такое attention supervision?
5. Как balanced attention влияет на performance?

### Вопрос 143: Объясните local vs global attention
**Углубляющие вопросы:**
1. В чем разница между local и global attention?
2. Как комбинировать local и global attention?
3. Что такое sliding window attention?
4. В каких задачах local attention предпочтительнее?
5. Как local attention влияет на computational efficiency?

### Вопрос 144: Что такое memory-efficient attention?
**Углубляющие вопросы:**
1. Какие memory bottlenecks есть в standard attention?
2. Что такое gradient checkpointing для attention?
3. Как FlashAttention оптимизирует memory usage?
4. Что такое reversible attention layers?
5. Как trade-off между memory и compute в attention?

### Вопрос 145: Объясните attention в multimodal задачах
**Углубляющие вопросы:**
1. Как attention работает между разными модальностями?
2. Что такое co-attention mechanisms?
3. Как align features разных модальностей через attention?
4. В каких multimodal задачах attention критичен?
5. Как handle разные temporal resolutions в multimodal attention?

---

## 10. Специализированные архитектуры (5 вопросов)

### Вопрос 146: Что такое Variational Autoencoders (VAE)?
**Углубляющие вопросы:**
1. Как VAE отличается от обычного autoencoder?
2. Что такое latent space в VAE?
3. Как работает reparameterization trick?
4. Что такое KL divergence в VAE loss?
5. В каких задачах VAE наиболее применимы?

### Вопрос 147: Объясните Generative Adversarial Networks (GAN)
**Углубляющие вопросы:**
1. Как работает adversarial training в GAN?
2. Что такое minimax игра между generator и discriminator?
3. Какие проблемы training stability есть у GAN?
4. Что такое mode collapse в GAN?
5. Какие улучшения GAN существуют (WGAN, StyleGAN, etc.)?

### Вопрос 148: Что такое Graph Neural Networks (GNN)?
**Углубляющие вопросы:**
1. Как GNN обрабатывают graph-структурированные данные?
2. Что такое message passing в GNN?
3. Как aggregation functions работают в GNN?
4. В каких задачах GNN наиболее эффективны?
5. Какие типы GNN существуют (GCN, GraphSAGE, GAT)?

### Вопрос 149: Объясните Neural ODEs
**Углубляющие вопросы:**
1. Как обыкновенные дифференциальные уравнения связаны с нейросетями?
2. Что такое continuous depth в Neural ODEs?
3. Как обучаются Neural ODEs?
4. В каких задачах Neural ODEs полезны?
5. Какие computational challenges есть у Neural ODEs?

### Вопрос 150: Что такое Memory Networks?
**Углубляющие вопросы:**
1. Как external memory используется в нейросетях?
2. Что такое differentiable memory access?
3. Как Memory Networks решают задачи reasoning?
4. В каких задачах external memory критически важна?
5. Какие архитектуры memory-augmented networks существуют?

---

## Дополнительные вопросы по архитектурам и задачам

### Бонусные вопросы о задачах, которые решают нейросети:

**Computer Vision:**
- Object Detection (YOLO, R-CNN семейство)
- Semantic Segmentation (FCN, U-Net, DeepLab)
- Instance Segmentation (Mask R-CNN)
- Style Transfer
- Super Resolution
- Optical Flow

**Natural Language Processing:**
- Machine Translation
- Text Summarization
- Sentiment Analysis
- Named Entity Recognition
- Question Answering
- Language Generation

**Speech and Audio:**
- Speech Recognition
- Speech Synthesis
- Audio Classification
- Music Generation

**Reinforcement Learning:**
- Policy Gradient Methods
- Q-Learning с нейросетями
- Actor-Critic методы

**Рекомендации для интервьюера:**

1. **Начинайте с основных вопросов** и переходите к углубляющим в зависимости от уровня кандидата
2. **Просите объяснять концепции простым языком** - это показывает глубину понимания
3. **Задавайте вопросы о практической реализации** - как бы кандидат реализовал это в коде
4. **Обсуждайте trade-offs** - когда использовать одну технику вместо другой
5. **Проверяйте знание современных достижений** - что нового в области за последние годы

**Система оценки:**
- **Junior (1-2 года опыта):** Основные вопросы + 1-2 углубляющих
- **Middle (2-4 года):** Основные + 2-3 углубляющих + понимание архитектур
- **Senior (4+ лет):** Полные блоки + способность объяснить математику + знание state-of-the-art
- **Lead/Architect:** Все вопросы + способность делать архитектурные решения + understanding business impact