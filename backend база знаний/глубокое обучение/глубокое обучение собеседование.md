# Компактное интервью: Глубокое обучение для Backend разработчика

**Структура:** 50 основных вопросов + 3 ключевых углубления к каждому (200 вопросов всего)

**Время проведения:**
- **Junior (1-2 года):** 15-20 вопросов (30-45 минут)
- **Middle (2-4 года):** 25-35 вопросов (45-60 минут) 
- **Senior (4+ лет):** 35-50 вопросов (60-90 минут)

---

## 1. Основы нейросетей (12 вопросов)

### Вопрос 1: Что такое нейрон и как он работает?
**Углубления:**
1. Как математически записать выход нейрона?
2. Зачем нужен bias и что происходит без него?
3. Как инициализировать веса нейрона?

### Вопрос 2: Объясните forward pass в нейронной сети
**Углубления:**
1. Как организовать вычисления для batch данных?
2. Что такое computational graph?
3. Как оптимизировать forward pass для больших данных?

### Вопрос 3: Что такое функции активации и зачем они нужны?
**Углубления:**
1. Что произойдет без функций активации в глубокой сети?
2. В чем разница между ReLU, Sigmoid и Tanh?
3. Когда использовать GELU или Swish?

### Вопрос 4: Объясните backpropagation простыми словами
**Углубления:**
1. Как цепное правило применяется в backpropagation?
2. Почему backpropagation эффективнее численного дифференцирования?
3. Что такое проблема исчезающих градиентов?

### Вопрос 5: Что такое функция потерь?
**Углубления:**
1. Как выбрать функцию потерь для задачи классификации vs регрессии?
2. В чем разница между Cross-Entropy и MSE?
3. Что такое Focal Loss и когда его использовать?

### Вопрос 6: Объясните переобучение (overfitting)
**Углубления:**
1. Как определить переобучение по кривым обучения?
2. Что такое bias-variance tradeoff?
3. Какие методы борьбы с переобучением знаете?

### Вопрос 7: Зачем нужны train/validation/test выборки?
**Углубления:**
1. Как правильно разделить данные?
2. Что такое data leakage и как его избежать?
3. Когда использовать cross-validation?

### Вопрос 8: Что такое batch normalization?
**Углубления:**
1. Какую проблему решает batch norm?
2. Как batch norm работает во время inference?
3. В чем разница между batch norm и layer norm?

### Вопрос 9: Объясните dropout
**Углубления:**
1. Почему dropout помогает против переобучения?
2. Как dropout работает во время inference?
3. Можно ли применять dropout к любым слоям?

### Вопрос 10: Что такое learning rate?
**Углубления:**
1. Что происходит при слишком большом/маленьком learning rate?
2. Что такое learning rate scheduling?
3. Как работают adaptive optimizers (Adam)?

### Вопрос 11: Что такое transfer learning?
**Углубления:**
1. Какие слои лучше переносятся между задачами?
2. Что такое fine-tuning?
3. Когда transfer learning наиболее эффективен?

### Вопрос 12: Объясните метрики качества
**Углубления:**
1. Когда Accuracy может обманывать?
2. В чем разница между Precision и Recall?
3. Что такое ROC-AUC и когда он неинформативен?

---

## 2. Обучение и оптимизация (10 вопросов)

### Вопрос 13: Как работает градиентный спуск?
**Углубления:**
1. В чем разница между batch, mini-batch и stochastic SGD?
2. Что такое momentum и как он помогает?
3. Как градиентный спуск находит минимум?

### Вопрос 14: Что такое Adam optimizer?
**Углубления:**
1. Как Adam адаптирует learning rate для каждого параметра?
2. Что такое первый и второй моменты в Adam?
3. В каких случаях Adam хуже SGD?

### Вопрос 15: Объясните gradient clipping
**Углубления:**
1. Какую проблему решает gradient clipping?
2. В чем разница между clipping by value и by norm?
3. В каких архитектурах это особенно важно?

### Вопрос 16: Что такое регуляризация?
**Углубления:**
1. В чем разница между L1 и L2 регуляризацией?
2. Что такое weight decay?
3. Как data augmentation работает как регуляризация?

### Вопрос 17: Объясните early stopping
**Углубления:**
1. Как выбрать критерий для early stopping?
2. Что такое patience?
3. Как early stopping влияет на воспроизводимость?

### Вопрос 18: Что такое learning rate finder?
**Углубления:**
1. Как работает алгоритм поиска learning rate?
2. Как интерпретировать график loss vs learning rate?
3. Как адаптировать найденный learning rate?

### Вопрос 19: Объясните mixup/cutmix
**Углубления:**
1. Как mixup изменяет распределение данных?
2. В чем разница между mixup и cutmix?
3. Как выбрать параметры для mixup?

### Вопрос 20: Что такое gradient accumulation?
**Углубления:**
1. Зачем нужен gradient accumulation?
2. Как он влияет на эффективный batch size?
3. Как влияет на batch normalization?

### Вопрос 21: Объясните knowledge distillation
**Углубления:**
1. Что такое soft targets в distillation?
2. Как выбрать teacher модель?
3. Что такое self-distillation?

### Вопрос 22: Что такое ensemble методы?
**Углубления:**
1. Как объединить предсказания нескольких моделей?
2. Как dropout связан с ensemble?
3. Какие вычислительные затраты у ensemble?

---

## 3. Архитектуры (15 вопросов)

### Вопрос 23: Что такое сверточный слой?
**Углубления:**
1. Что такое kernel, stride, padding?
2. Как вычислить размер выходного feature map?
3. Почему свертки эффективны для изображений?

### Вопрос 24: Объясните pooling слои
**Углубления:**
1. В чем разница между max и average pooling?
2. Что такое global average pooling?
3. Какие альтернативы pooling существуют?

### Вопрос 25: Что такое ResNet и residual connections?
**Углубления:**
1. Какую проблему решают residual connections?
2. Как skip connections помогают gradient flow?
3. Что такое identity mapping?

### Вопрос 26: Объясните 1x1 convolutions
**Углубления:**
1. Зачем нужны 1x1 свертки?
2. Что такое bottleneck layers?
3. Как 1x1 convolutions добавляют нелинейность?

### Вопрос 27: Что такое рекуррентные сети (RNN)?
**Углубления:**
1. Как RNN обрабатывают последовательности?
2. Что такое hidden state?
3. В чем проблема vanishing gradients в RNN?

### Вопрос 28: Объясните LSTM
**Углубления:**
1. Как LSTM решает проблему long-term dependencies?
2. Что такое gates в LSTM (forget, input, output)?
3. В чем разница между cell state и hidden state?

### Вопрос 29: Что такое attention mechanism?
**Углубления:**
1. Какую проблему решает attention?
2. Что такое query, key, value?
3. Как вычисляются attention weights?

### Вопрос 30: Объясните архитектуру Transformer
**Углубления:**
1. Почему Transformer не нуждается в recurrence?
2. Что такое multi-head attention?
3. Зачем нужно positional encoding?

### Вопрос 31: В чем разница между BERT и GPT?
**Углубления:**
1. Что такое bidirectional vs autoregressive подход?
2. Как работает masked language modeling?
3. Для каких задач лучше BERT, а для каких GPT?

### Вопрос 32: Что такое self-attention?
**Углубления:**
1. В чем отличие от cross-attention?
2. Как self-attention моделирует зависимости?
3. Какова вычислительная сложность self-attention?

### Вопрос 33: Объясните Vision Transformer (ViT)
**Углубления:**
1. Как Transformer применяется к изображениям?
2. Что такое patch embeddings?
3. Когда ViT превосходит CNN?

### Вопрос 34: Что такое U-Net?
**Углубления:**
1. Для каких задач была разработана U-Net?
2. Что такое encoder-decoder архитектура?
3. Как работают skip connections в U-Net?

### Вопрос 35: Объясните MobileNet
**Углубления:**
1. Что такое depthwise separable convolutions?
2. Как MobileNet достигает эффективности?
3. В каких применениях MobileNet полезна?

### Вопрос 36: Что такое dilated convolutions?
**Углубления:**
1. Как dilated convolutions увеличивают receptive field?
2. Что такое dilation rate?
3. В каких задачах особенно полезны?

### Вопрос 37: Объясните bidirectional RNN
**Углубления:**
1. Как обрабатывается информация в обе стороны?
2. В каких задачах bidirectional RNN полезны?
3. Можно ли использовать для real-time задач?

---

## 4. Продвинутые темы (8 вопросов)

### Вопрос 38: Что такое GAN?
**Углубления:**
1. Как работает adversarial training?
2. Что такое mode collapse?
3. Какие улучшения GAN знаете?

### Вопрос 39: Объясните Variational Autoencoder (VAE)
**Углубления:**
1. Чем VAE отличается от обычного autoencoder?
2. Что такое reparameterization trick?
3. Что такое latent space?

### Вопрос 40: Что такое Graph Neural Networks?
**Углубления:**
1. Как GNN обрабатывают graph данные?
2. Что такое message passing?
3. В каких задачах GNN эффективны?

### Вопрос 41: Объясните контрастивное обучение
**Углубления:**
1. Что такое contrastive loss?
2. Как формировать positive/negative пары?
3. В каких задачах это применяется?

### Вопрос 42: Что такое few-shot learning?
**Углубления:**
1. Как обучать на малом количестве примеров?
2. Что такое meta-learning?
3. Какие подходы к few-shot learning знаете?

### Вопрос 43: Объясните multi-task learning
**Углубления:**
1. Как обучать одну модель на нескольких задачах?
2. Что такое gradient interference?
3. Как балансировать losses разных задач?

### Вопрос 44: Что такое domain adaptation?
**Углубления:**
1. Как адаптировать модель к новому домену?
2. Что такое domain shift?
3. Какие методы domain adaptation знаете?

### Вопрос 45: Объясните neural architecture search (NAS)
**Углубления:**
1. Как автоматически найти архитектуру?
2. Что такое search space в NAS?
3. Какие методы NAS существуют?

---

## 5. Практическое применение (5 вопросов)

### Вопрос 46: Как развернуть модель в production?
**Углубления:**
1. Какие форматы моделей знаете (ONNX, TensorRT)?
2. Как оптимизировать модель для инференса?
3. Что такое quantization и pruning?

### Вопрос 47: Как мониторить ML модель в production?
**Углубления:**
1. Что такое model drift?
2. Как детектировать деградацию модели?
3. Какие метрики отслеживать в production?

### Вопрос 48: Как обрабатывать большие датасеты?
**Углубления:**
1. Что такое data loading pipeline?
2. Как организовать distributed training?
3. Как использовать mixed precision?

### Вопрос 49: Объясните A/B тестирование ML моделей
**Углубления:**
1. Как корректно сравнить две модели?
2. Что такое champion/challenger подход?
3. Как учесть delayed feedback?

### Вопрос 50: Как дебажить нейронную сеть?
**Углубления:**
1. Что проверить, если модель не обучается?
2. Как анализировать loss curves?
3. Какие техники debugging знаете?

---

## Система оценки

### Junior Backend Developer (1-2 года)
**Ожидания:** Понимание основ, умение работать с готовыми моделями
**Вопросы:** 1-12, 23-25, 46-47 (15-20 вопросов)
**Критерии:**
- Объясняет основные концепции простыми словами
- Понимает разницу между обучением и инференсом
- Знает базовые архитектуры (CNN, RNN основы)

### Middle Backend Developer (2-4 года)
**Ожидания:** Может обучать модели, понимает различные архитектуры
**Вопросы:** 1-25, 46-50 (25-35 вопросов)
**Критерии:**
- Понимает процесс обучения и оптимизации
- Знает современные архитектуры
- Может решать практические задачи

### Senior Backend Developer (4+ лет)
**Ожидания:** Глубокие знания, может принимать архитектурные решения
**Вопросы:** 1-50 (35-50 вопросов)
**Критерии:**
- Понимает математику за алгоритмами
- Знает state-of-the-art методы
- Может оптимизировать для production
- Понимает trade-offs различных подходов

## Рекомендации для интервьюера

1. **Начинайте с основ** и углубляйтесь по мере ответов
2. **Просите примеры** из практики кандидата
3. **Обсуждайте trade-offs** - когда использовать один подход vs другой
4. **Проверяйте практические навыки** - как бы реализовали в коде
5. **Адаптируйте сложность** под уровень позиции

**Красные флаги:**
- Не понимает разницу между обучением и инференсом
- Путает основные концепции (loss vs metric)
- Не может объяснить зачем нужна валидационная выборка
- Не знает современных архитектур для своего уровня