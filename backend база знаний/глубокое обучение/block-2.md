# Ğ‘Ğ»Ğ¾Ğº 2: ĞÑĞ½Ğ¾Ğ²Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¸ Ğ¸Ñ… Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ

---

**â± Ğ”Ğ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ:** 5-7 Ğ½ĞµĞ´ĞµĞ»ÑŒ  
**ğŸ¯ Ğ¦ĞµĞ»ÑŒ:** ĞŸĞ¾Ğ½ÑÑ‚ÑŒ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñƒ Ğ¸ Ğ¿Ñ€Ğ¸Ğ½Ñ†Ğ¸Ğ¿Ñ‹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹  
**ğŸ“Š Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸:** â­â­â­â­â˜†

---

## ğŸ¯ Ğ§Ñ‚Ğ¾ Ğ²Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚Ğµ Ğ² ÑÑ‚Ğ¾Ğ¼ Ğ±Ğ»Ğ¾ĞºĞµ

```
Ğ’Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ          ĞŸÑ€Ğ¾Ñ†ĞµÑÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ          Ğ’Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â€¢ ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ°    â”‚â”€â”€â”€â”€â–¶â”‚ â€¢ Ğ¢ĞµĞ¾Ñ€Ğ¸Ñ        â”‚â”€â”€â”€â”€â–¶â”‚ â€¢ Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ    â”‚
â”‚ â€¢ Python        â”‚     â”‚ â€¢ ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°      â”‚     â”‚ â€¢ ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ     â”‚
â”‚ â€¢ ĞÑĞ½Ğ¾Ğ²Ñ‹ ML     â”‚     â”‚ â€¢ ĞŸÑ€Ğ¾ĞµĞºÑ‚Ñ‹       â”‚     â”‚ â€¢ ĞĞ¿Ñ‹Ñ‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Ğ“Ğ»Ğ°Ğ²Ğ° 2.1: ĞŸĞµÑ€ÑĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½ Ğ¸ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸

### ğŸ›ï¸ Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹

ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¸ÑÑŒ Ñ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¸Ğ´ĞµĞ¸ - ÑĞ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°:

```
Ğ‘Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½          Ğ˜ÑĞºÑƒÑÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Ğ”ĞµĞ½Ğ´Ñ€Ğ¸Ñ‚Ñ‹       â”‚   â†’    â”‚     Ğ’Ñ…Ğ¾Ğ´Ñ‹ (x)     â”‚
â”‚       â†“           â”‚        â”‚       â†“           â”‚
â”‚    Ğ¢ĞµĞ»Ğ¾ ĞºĞ»ĞµÑ‚ĞºĞ¸    â”‚   â†’    â”‚  Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ (Î£wx+b)  â”‚
â”‚       â†“           â”‚        â”‚       â†“           â”‚
â”‚     ĞĞºÑĞ¾Ğ½         â”‚   â†’    â”‚ ĞĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ f(z)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”¢ ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¿ĞµÑ€ÑĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ğ°

**ĞŸĞµÑ€ÑĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½** - ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚ĞµĞ¹ÑˆĞ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ, ÑĞ¾ÑÑ‚Ğ¾ÑÑ‰Ğ°Ñ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°:

```
Ğ’Ñ…Ğ¾Ğ´Ñ‹ â†’ Ğ’ĞµÑĞ° â†’ Ğ¡ÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ â†’ ĞĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ â†’ Ğ’Ñ‹Ñ…Ğ¾Ğ´

xâ‚ â”€â”€â”€â”€â”€â”
         â”‚ wâ‚
xâ‚‚ â”€â”€â”€â”€â”€â”¤     â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”
         â”‚ wâ‚‚  â”‚  Î£  â”‚â”€â”€â”€â”€â”‚  f(z)   â”‚â”€â”€â”€â”€â”‚  y  â”‚
xâ‚ƒ â”€â”€â”€â”€â”€â”¤     â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜
         â”‚ wâ‚ƒ      â†‘
xâ‚™ â”€â”€â”€â”€â”€â”˜           b (bias)
```

**ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°:**
```
z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b = Î£wáµ¢xáµ¢ + b
y = f(z)
```

### ğŸ§® Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸

```
Ğ¡Ñ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ      Ğ¡Ğ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ğ°              ReLU
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      1 â”‚        â”‚     â”‚        â•­â”€â”€â”€â”€â”€    â”‚    â”‚      â•±          â”‚
â”‚        â”‚        â”‚     â”‚      â•±           â”‚    â”‚    â•±            â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”‚     â”‚    â•±             â”‚    â”‚  â•±              â”‚
â”‚      0 â”‚        â”‚     â”‚  â•±               â”‚    â”‚â•±________________â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
f(z) = {1 ĞµÑĞ»Ğ¸ zâ‰¥0     f(z) = 1/(1+e^(-z))    f(z) = max(0,z)
        {0 ĞµÑĞ»Ğ¸ z<0
```

### ğŸ—ï¸ ĞœĞ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ğ¹ Ğ¿ĞµÑ€ÑĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½ (MLP)

```
Ğ’Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¹    Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğ¹ ÑĞ»Ğ¾Ğ¹    Ğ’Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¹
                                              
xâ‚ â”€â”€â”€â”         â”Œâ”€â”€â”€ hâ‚ â”€â”€â”€â”         â”Œâ”€â”€â”€ yâ‚
      â”‚         â”‚          â”‚         â”‚
xâ‚‚ â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€ hâ‚‚ â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€ yâ‚‚
      â”‚         â”‚          â”‚         â”‚
xâ‚ƒ â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€ hâ‚ƒ â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€ yâ‚ƒ
      â”‚         â”‚          â”‚         â”‚
xâ‚„ â”€â”€â”€â”˜         â””â”€â”€â”€ hâ‚„ â”€â”€â”€â”˜         â””â”€â”€â”€ yâ‚„

ĞŸĞ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° - ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½ ÑĞ¾ĞµĞ´Ğ¸Ğ½ĞµĞ½ Ñ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ¼ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ³Ğ¾ ÑĞ»Ğ¾Ñ
```

### ğŸ’» ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ 2.1: Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ğ°

```python
import numpy as np
import matplotlib.pyplot as plt

class Perceptron:
    def __init__(self, n_inputs, learning_rate=0.01):
        """
        Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€ÑĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ğ°
        
        Args:
            n_inputs: ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²
            learning_rate: ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
        """
        self.weights = np.random.randn(n_inputs) * 0.01
        self.bias = 0
        self.learning_rate = learning_rate
        self.training_history = []
    
    def activation(self, z):
        """Ğ¡Ñ‚ÑƒĞ¿ĞµĞ½Ñ‡Ğ°Ñ‚Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸"""
        return np.where(z >= 0, 1, 0)
    
    def forward(self, X):
        """ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ"""
        z = np.dot(X, self.weights) + self.bias
        return self.activation(z)
    
    def fit(self, X, y, epochs=100):
        """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿ĞµÑ€ÑĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½Ğ°"""
        for epoch in range(epochs):
            errors = 0
            for i in range(len(X)):
                # ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
                prediction = self.forward(X[i])
                
                # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸
                error = y[i] - prediction
                
                # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ
                self.weights += self.learning_rate * error * X[i]
                self.bias += self.learning_rate * error
                
                errors += int(error != 0)
            
            self.training_history.append(errors)
            
            if errors == 0:
                print(f"Ğ¡Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ½ÑƒÑ‚Ğ° Ğ½Ğ° ÑĞ¿Ğ¾Ñ…Ğµ {epoch + 1}")
                break
    
    def plot_training_history(self):
        """Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ"""
        plt.figure(figsize=(10, 6))
        plt.plot(self.training_history)
        plt.title('ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ')
        plt.xlabel('Ğ­Ğ¿Ğ¾Ñ…Ğ°')
        plt.ylabel('ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº')
        plt.grid(True)
        plt.show()

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸ XOR
# (Ğ’Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿ĞµÑ€ÑĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½ ĞĞ• Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ñ€ĞµÑˆĞ¸Ñ‚ÑŒ XOR!)
```

### ğŸ” ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¿Ğ¾Ğ½ÑÑ‚Ğ¸Ñ Ğ³Ğ»Ğ°Ğ²Ñ‹ 2.1

| ĞŸĞ¾Ğ½ÑÑ‚Ğ¸Ğµ | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ |
|---------|----------|
| **ĞŸĞµÑ€ÑĞµĞ¿Ñ‚Ñ€Ğ¾Ğ½** | ĞŸÑ€Ğ¾ÑÑ‚ĞµĞ¹ÑˆĞ°Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ° |
| **Ğ’ĞµÑĞ° (weights)** | ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹, Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑÑÑ‰Ğ¸Ğµ ÑĞ¸Ğ»Ñƒ ÑĞ²ÑĞ·Ğ¸ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°Ğ¼Ğ¸ |
| **Ğ¡Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ (bias)** | ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€, Ğ¿Ğ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‰Ğ¸Ğ¹ ÑĞ´Ğ²Ğ¸Ğ³Ğ°Ñ‚ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ |
| **Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ** | Ğ’Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ ĞºĞ»Ğ°ÑÑÑ‹ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¹ Ğ»Ğ¸Ğ½Ğ¸ĞµĞ¹ |
| **Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚ĞµĞ¾Ñ€ĞµĞ¼Ğ° Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ°Ñ†Ğ¸Ğ¸** | MLP Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ°Ğ¿Ğ¿Ñ€Ğ¾ĞºÑĞ¸Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ»ÑĞ±ÑƒÑ Ğ½ĞµĞ¿Ñ€ĞµÑ€Ñ‹Ğ²Ğ½ÑƒÑ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ |

---

## ğŸ­ Ğ“Ğ»Ğ°Ğ²Ğ° 2.2: Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸

### ğŸŒŸ Ğ Ğ¾Ğ»ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸

```
Ğ‘ĞµĞ· Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸           Ğ¡ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸ĞµĞ¹
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ°Ñ       â”‚    â”‚  ĞĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ°Ñ     â”‚
â”‚  ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ     â”‚    â”‚  ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ     â”‚
â”‚     â†“           â”‚    â”‚     â†“           â”‚
â”‚  Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ°Ñ       â”‚    â”‚  Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ğ°Ñ        â”‚
â”‚  Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ        â”‚    â”‚  Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ğ—Ğ°Ñ‡ĞµĞ¼ Ğ½ÑƒĞ¶Ğ½Ñ‹ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸?**
- Ğ’Ğ½Ğ¾ÑÑÑ‚ Ğ½ĞµĞ»Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ÑÑ‚ÑŒ Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
- ĞŸĞ¾Ğ·Ğ²Ğ¾Ğ»ÑÑÑ‚ Ñ€ĞµÑˆĞ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸
- ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒÑÑ‚ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ²

### ğŸ“Š ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸

#### 1. **Sigmoid (Ğ¡Ğ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ğ°)**
```
Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°: f(x) = 1/(1 + e^(-x))
Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº:     1 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              â”‚        â•­â”€â”€â”€â”€â”€
              â”‚      â•±
              â”‚    â•±
              â”‚  â•±
            0 â””â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             -âˆ  -2  0  2  +âˆ

ĞŸĞ»ÑÑÑ‹:  â€¢ Ğ“Ğ»Ğ°Ğ´ĞºĞ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
        â€¢ Ğ—Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğµ (0,1)
        â€¢ Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ĞºĞ°Ğº Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸

ĞœĞ¸Ğ½ÑƒÑÑ‹: â€¢ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°ÑÑ‰Ğ¸Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²
        â€¢ ĞĞµ Ñ†ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ÑƒĞ»Ñ
        â€¢ Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ
```

#### 2. **Tanh (Ğ“Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ‚Ğ°Ğ½Ğ³ĞµĞ½Ñ)**
```
Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°: f(x) = (e^x - e^(-x))/(e^x + e^(-x))
Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº:     1 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              â”‚      â•­â”€â”€â”€â”€â”€
              â”‚    â•±
              â”‚  â•±
            0 â”œâ•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              â”‚â•±
              â”‚  â•²
           -1 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             -âˆ  -2  0  2  +âˆ

ĞŸĞ»ÑÑÑ‹:  â€¢ Ğ¦ĞµĞ½Ñ‚Ñ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ½ÑƒĞ»Ñ
        â€¢ Ğ‘Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ñ‡ĞµĞ¼ sigmoid
        â€¢ Ğ—Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ² Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğµ (-1,1)

ĞœĞ¸Ğ½ÑƒÑÑ‹: â€¢ Ğ’ÑĞµ ĞµÑ‰Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°ÑÑ‰Ğ¸Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²
        â€¢ Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ Ğ·Ğ°Ñ‚Ñ€Ğ°Ñ‚Ğ½Ğ°Ñ
```

#### 3. **ReLU (Rectified Linear Unit)**
```
Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°: f(x) = max(0, x)
Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº:       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              â”‚      â•±
              â”‚    â•±
              â”‚  â•±
            0 â”œâ•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              â”‚
              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             -âˆ  -2  0  2  +âˆ

ĞŸĞ»ÑÑÑ‹:  â€¢ ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ğ¸ Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ
        â€¢ ĞĞµ Ğ½Ğ°ÑÑ‹Ñ‰Ğ°ĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»Ğ¾Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹
        â€¢ Ğ Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¹

ĞœĞ¸Ğ½ÑƒÑÑ‹: â€¢ "ĞœĞµÑ€Ñ‚Ğ²Ñ‹Ğµ" Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ (dying ReLU)
        â€¢ ĞĞµ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ° Ğ² Ğ½ÑƒĞ»Ğµ
```

#### 4. **Leaky ReLU**
```
Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°: f(x) = max(Î±x, x), Ğ³Ğ´Ğµ Î± â‰ˆ 0.01
Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº:       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              â”‚      â•±
              â”‚    â•±
              â”‚  â•±
            0 â”œâ•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              â”‚â•±
              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             -âˆ  -2  0  2  +âˆ

ĞŸĞ»ÑÑÑ‹:  â€¢ Ğ ĞµÑˆĞ°ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñƒ dying ReLU
        â€¢ Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ
        â€¢ ĞŸĞ¾Ğ·Ğ²Ğ¾Ğ»ÑĞµÑ‚ Ğ½ĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ

ĞœĞ¸Ğ½ÑƒÑÑ‹: â€¢ Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€ Î±
```

### ğŸ”¬ ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸

```
Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ     â”‚ ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ        â”‚ ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Sigmoid     â”‚ f'(x) = f(x)(1-f(x))â”‚ ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 0.25
Tanh        â”‚ f'(x) = 1 - f(x)Â²   â”‚ ĞœĞ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ 1
ReLU        â”‚ f'(x) = {1 if x>0   â”‚ ĞšĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ° Ğ¸Ğ»Ğ¸ 0
            â”‚         {0 if xâ‰¤0   â”‚
Leaky ReLU  â”‚ f'(x) = {1 if x>0   â”‚ Ğ’ÑĞµĞ³Ğ´Ğ° > 0
            â”‚         {Î± if xâ‰¤0   â”‚
```

### ğŸ§ª ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ 2.2: Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸

```python
import numpy as np
import matplotlib.pyplot as plt

class ActivationFunctions:
    @staticmethod
    def sigmoid(x):
        """Ğ¡Ğ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ğ°"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    @staticmethod
    def sigmoid_derivative(x):
        """ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ ÑĞ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ñ‹"""
        s = ActivationFunctions.sigmoid(x)
        return s * (1 - s)
    
    @staticmethod
    def tanh(x):
        """Ğ“Ğ¸Ğ¿ĞµÑ€Ğ±Ğ¾Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ Ñ‚Ğ°Ğ½Ğ³ĞµĞ½Ñ"""
        return np.tanh(x)
    
    @staticmethod
    def tanh_derivative(x):
        """ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ tanh"""
        return 1 - np.tanh(x)**2
    
    @staticmethod
    def relu(x):
        """ReLU"""
        return np.maximum(0, x)
    
    @staticmethod
    def relu_derivative(x):
        """ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ ReLU"""
        return (x > 0).astype(float)
    
    @staticmethod
    def leaky_relu(x, alpha=0.01):
        """Leaky ReLU"""
        return np.where(x > 0, x, alpha * x)
    
    @staticmethod
    def leaky_relu_derivative(x, alpha=0.01):
        """ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ Leaky ReLU"""
        return np.where(x > 0, 1, alpha)
    
    @staticmethod
    def plot_comparison():
        """Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸"""
        x = np.linspace(-5, 5, 1000)
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸
        ax1.plot(x, ActivationFunctions.sigmoid(x), 'b-', label='Sigmoid')
        ax1.plot(x, ActivationFunctions.tanh(x), 'r-', label='Tanh')
        ax1.plot(x, ActivationFunctions.relu(x), 'g-', label='ReLU')
        ax1.plot(x, ActivationFunctions.leaky_relu(x), 'm-', label='Leaky ReLU')
        ax1.set_title('Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸')
        ax1.legend()
        ax1.grid(True)
        
        # ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ğµ
        ax2.plot(x, ActivationFunctions.sigmoid_derivative(x), 'b-', label='Sigmoid')
        ax2.plot(x, ActivationFunctions.tanh_derivative(x), 'r-', label='Tanh')
        ax2.plot(x, ActivationFunctions.relu_derivative(x), 'g-', label='ReLU')
        ax2.plot(x, ActivationFunctions.leaky_relu_derivative(x), 'm-', label='Leaky ReLU')
        ax2.set_title('ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸')
        ax2.legend()
        ax2.grid(True)
        
        # ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ·Ğ°Ñ‚ÑƒÑ…Ğ°ÑÑ‰Ğ¸Ñ… Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²
        layers = np.arange(1, 21)
        sigmoid_grad = 0.25 ** layers  # ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ ÑĞ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ñ‹
        tanh_grad = 1.0 ** layers      # ĞœĞ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ tanh
        relu_grad = np.ones_like(layers)  # ReLU ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚
        
        ax3.semilogy(layers, sigmoid_grad, 'b-', label='Sigmoid')
        ax3.semilogy(layers, tanh_grad, 'r-', label='Tanh')
        ax3.semilogy(layers, relu_grad, 'g-', label='ReLU')
        ax3.set_title('Ğ—Ğ°Ñ‚ÑƒÑ…Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² Ğ¿Ğ¾ ÑĞ»Ğ¾ÑĞ¼')
        ax3.set_xlabel('ĞĞ¾Ğ¼ĞµÑ€ ÑĞ»Ğ¾Ñ')
        ax3.set_ylabel('Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ (log scale)')
        ax3.legend()
        ax3.grid(True)
        
        plt.tight_layout()
        plt.show()

# Ğ—Ğ°Ğ¿ÑƒÑĞº ÑÑ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ñ
ActivationFunctions.plot_comparison()
```

### ğŸ¯ Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸

```
Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ°                  â”‚ Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ğ¡ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸ (Ğ¾Ğ±Ñ‰Ğ¸Ğ¹)    â”‚ ReLU / Leaky ReLU
Ğ’Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¹ (Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ°Ñ)â”‚ Sigmoid
Ğ’Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¹ (Ğ¼ÑƒĞ»ÑŒÑ‚Ğ¸)  â”‚ Softmax
Ğ ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ              â”‚ Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ°Ñ / ReLU
ĞĞ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ñ‹           â”‚ Sigmoid / Tanh
```

---

## ğŸ”„ Ğ“Ğ»Ğ°Ğ²Ğ° 2.3: ĞĞ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸

### ğŸ§  Ğ¡ÑƒÑ‚ÑŒ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ°

**ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ (Backpropagation)** - ÑÑ‚Ğ¾ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼ Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ½Ğ¾Ğ³Ğ¾ÑĞ»Ğ¾Ğ¹Ğ½Ñ‹Ñ… Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹.

```
ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ (Forward Pass)
â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”
â”‚Inputâ”‚â”€â”€â”€â–¶â”‚ L1  â”‚â”€â”€â”€â–¶â”‚ L2  â”‚â”€â”€â”€â–¶â”‚Outputâ”‚
â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚  Loss   â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ (Backward Pass)
â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”
â”‚âˆ‚L/âˆ‚Wâ”‚â—€â”€â”€â”€â”‚âˆ‚L/âˆ‚Wâ”‚â—€â”€â”€â”€â”‚âˆ‚L/âˆ‚Wâ”‚â—€â”€â”€â”€â”‚âˆ‚L/âˆ‚yâ”‚
â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜
```

### ğŸ“š ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ²Ñ‹

#### 1. **Ğ¦ĞµĞ¿Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ**

Ğ•ÑĞ»Ğ¸ Ñƒ Ğ½Ğ°Ñ ĞµÑÑ‚ÑŒ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹: `z = f(g(x))`, Ñ‚Ğ¾:
```
dz/dx = (dz/dg) Ã— (dg/dx)
```

#### 2. **ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğº Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞµÑ‚ÑĞ¼**

Ğ”Ğ»Ñ ÑĞµÑ‚Ğ¸ Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸ĞµĞ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ `L` Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸ `W`:
```
âˆ‚L/âˆ‚W = (âˆ‚L/âˆ‚y) Ã— (âˆ‚y/âˆ‚z) Ã— (âˆ‚z/âˆ‚W)

Ğ³Ğ´Ğµ:
- âˆ‚L/âˆ‚y - Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¿Ğ¾ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ñƒ
- âˆ‚y/âˆ‚z - Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸
- âˆ‚z/âˆ‚W - Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚ Ğ¿Ğ¾ Ğ²ĞµÑĞ°Ğ¼
```

### ğŸ”¢ ĞŸĞ¾ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼

```
Ğ¨Ğ°Ğ³ 1: ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ğ’Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¹: aâ½â°â¾ = x
Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ l = 1, 2, ..., L:
  zâ½Ë¡â¾ = Wâ½Ë¡â¾aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾
  aâ½Ë¡â¾ = f(zâ½Ë¡â¾)

Ğ¨Ğ°Ğ³ 2: Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Î´â½á´¸â¾ = âˆ‡â‚L âŠ™ f'(zâ½á´¸â¾)

Ğ¨Ğ°Ğ³ 3: ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ l = L-1, L-2, ..., 1:
  Î´â½Ë¡â¾ = ((Wâ½Ë¡âºÂ¹â¾)áµ€Î´â½Ë¡âºÂ¹â¾) âŠ™ f'(zâ½Ë¡â¾)

Ğ¨Ğ°Ğ³ 4: Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âˆ‚L/âˆ‚Wâ½Ë¡â¾ = Î´â½Ë¡â¾(aâ½Ë¡â»Â¹â¾)áµ€
âˆ‚L/âˆ‚bâ½Ë¡â¾ = Î´â½Ë¡â¾

Ğ¨Ğ°Ğ³ 5: ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Wâ½Ë¡â¾ = Wâ½Ë¡â¾ - Î·âˆ‚L/âˆ‚Wâ½Ë¡â¾
bâ½Ë¡â¾ = bâ½Ë¡â¾ - Î·âˆ‚L/âˆ‚bâ½Ë¡â¾
```

### ğŸ“Š Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„

```
ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸:
x â”€â”€â”€ Wâ‚ â”€â”€â”€ zâ‚ â”€â”€â”€ Ïƒ â”€â”€â”€ aâ‚ â”€â”€â”€ Wâ‚‚ â”€â”€â”€ zâ‚‚ â”€â”€â”€ Ïƒ â”€â”€â”€ aâ‚‚ â”€â”€â”€ L
      â”‚              â”‚              â”‚              â”‚
      â”‚              â”‚              â”‚              â”‚
ĞŸÑ€ÑĞ¼Ğ¾Ğ¹ â”‚              â”‚              â”‚              â”‚ ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğ¹
Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´ â”‚              â”‚              â”‚              â”‚ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´
      â”‚              â”‚              â”‚              â”‚
      â–¼              â–¼              â–¼              â–¼
    âˆ‚L/âˆ‚Wâ‚ â—€â”€â”€â”€ âˆ‚L/âˆ‚zâ‚ â—€â”€â”€â”€ âˆ‚L/âˆ‚aâ‚ â—€â”€â”€â”€ âˆ‚L/âˆ‚zâ‚‚ â—€â”€â”€â”€ âˆ‚L/âˆ‚aâ‚‚
```

### ğŸ’» ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ 2.3: Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ backpropagation

```python
import numpy as np
import matplotlib.pyplot as plt

class NeuralNetwork:
    def __init__(self, layers, learning_rate=0.01):
        """
        Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸
        
        Args:
            layers: ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾Ğ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ² ĞºĞ°Ğ¶Ğ´Ğ¾Ğ¼ ÑĞ»Ğ¾Ğµ
            learning_rate: ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
        """
        self.layers = layers
        self.learning_rate = learning_rate
        self.weights = []
        self.biases = []
        
        # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹
        for i in range(len(layers) - 1):
            w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])
            b = np.zeros((1, layers[i+1]))
            self.weights.append(w)
            self.biases.append(b)
    
    def sigmoid(self, x):
        """Ğ¡Ğ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ğ°"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def sigmoid_derivative(self, x):
        """ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ ÑĞ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ñ‹"""
        s = self.sigmoid(x)
        return s * (1 - s)
    
    def forward(self, X):
        """
        ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
        
        Args:
            X: Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
            
        Returns:
            activations: Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ
            z_values: Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ
        """
        activations = [X]
        z_values = []
        
        for i in range(len(self.weights)):
            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            z_values.append(z)
            a = self.sigmoid(z)
            activations.append(a)
        
        return activations, z_values
    
    def backward(self, X, y, activations, z_values):
        """
        ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
        
        Args:
            X: Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
            y: Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ
            activations: Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ
            z_values: Ğ»Ğ¸Ğ½ĞµĞ¹Ğ½Ñ‹Ğµ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ
        """
        m = X.shape[0]
        
        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ
        delta = activations[-1] - y
        
        # ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸
        deltas = [delta]
        
        for i in range(len(self.weights) - 1, 0, -1):
            delta = np.dot(delta, self.weights[i].T) * self.sigmoid_derivative(z_values[i-1])
            deltas.append(delta)
        
        deltas.reverse()
        
        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ² Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹
        for i in range(len(self.weights)):
            self.weights[i] -= self.learning_rate * np.dot(activations[i].T, deltas[i]) / m
            self.biases[i] -= self.learning_rate * np.sum(deltas[i], axis=0, keepdims=True) / m
    
    def train(self, X, y, epochs=1000):
        """
        ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞµÑ‚Ğ¸
        
        Args:
            X: Ğ²Ñ…Ğ¾Ğ´Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
            y: Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ
            epochs: ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¿Ğ¾Ñ…
        """
        losses = []
        
        for epoch in range(epochs):
            # ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
            activations, z_values = self.forward(X)
            
            # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ
            loss = np.mean((activations[-1] - y) ** 2)
            losses.append(loss)
            
            # ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
            self.backward(X, y, activations, z_values)
            
            if epoch % 100 == 0:
                print(f'Epoch {epoch}, Loss: {loss:.4f}')
        
        return losses
    
    def predict(self, X):
        """ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ"""
        activations, _ = self.forward(X)
        return activations[-1]
    
    def plot_training_curve(self, losses):
        """Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ"""
        plt.figure(figsize=(10, 6))
        plt.plot(losses)
        plt.title('ĞšÑ€Ğ¸Ğ²Ğ°Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ')
        plt.xlabel('Ğ­Ğ¿Ğ¾Ñ…Ğ°')
        plt.ylabel('Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ')
        plt.yscale('log')
        plt.grid(True)
        plt.show()

# Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ½Ğ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… XOR
X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_xor = np.array([[0], [1], [1], [0]])

# Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞµÑ‚Ğ¸
nn = NeuralNetwork([2, 4, 1], learning_rate=1.0)
losses = nn.train(X_xor, y_xor, epochs=5000)

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
predictions = nn.predict(X_xor)
print("\nĞ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹:")
for i in range(len(X_xor)):
    print(f"Input: {X_xor[i]}, Target: {y_xor[i][0]}, Prediction: {predictions[i][0]:.4f}")

# Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
nn.plot_training_curve(losses)
```

### ğŸ¯ ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñ‹ Ğ³Ğ»Ğ°Ğ²Ñ‹ 2.3

| ĞšĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ğ¸Ñ | Ğ’Ğ°Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ | ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ |
|-----------|----------|----------|
| **Ğ¦ĞµĞ¿Ğ½Ğ¾Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾** | â­â­â­â­â­ | ĞÑĞ½Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ² |
| **Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ³Ñ€Ğ°Ñ„** | â­â­â­â­â˜† | ĞŸÑ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ |
| **Ğ—Ğ°Ñ‚ÑƒÑ…Ğ°ÑÑ‰Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹** | â­â­â­â­â˜† | ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ñ… ÑĞµÑ‚ĞµĞ¹ |
| **Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ²** | â­â­â­â­â˜† | ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ´Ğ»Ñ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ |

---

## ğŸ“‰ Ğ“Ğ»Ğ°Ğ²Ğ° 2.4: Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°

### ğŸ¯ Ğ Ğ¾Ğ»ÑŒ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ

```
ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ = ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ

ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ â†’ Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ â†’ Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ â†’ ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ²
     â†‘                                              â†“
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ˜Ñ‚ĞµÑ€Ğ°Ñ†Ğ¸Ñ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“Š ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ

#### 1. **Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° (MSE)**

```
Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°: MSE = 1/n Î£(yáµ¢ - Å·áµ¢)Â²

ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ: Ğ ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ
ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸: â€¢ Ğ§ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ° Ğº Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ°Ğ¼
            â€¢ Ğ“Ğ»Ğ°Ğ´ĞºĞ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ
            â€¢ ĞšĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ ÑˆÑ‚Ñ€Ğ°Ñ„ÑƒĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸

Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ:
     â†‘ Loss
     â”‚    â•±â”‚â•²
     â”‚  â•±  â”‚  â•²
     â”‚ â•±   â”‚   â•²
     â”‚â•±    â”‚    â•²
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’ (y - Å·)
     â”‚     0
```

#### 2. **Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ° (MAE)**

```
Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°: MAE = 1/n Î£|yáµ¢ - Å·áµ¢|

ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ: Ğ ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ (ÑƒÑÑ‚Ğ¾Ğ¹Ñ‡Ğ¸Ğ²Ğ°Ñ Ğº Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ°Ğ¼)
ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸: â€¢ ĞœĞµĞ½ĞµĞµ Ñ‡ÑƒĞ²ÑÑ‚Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ° Ğº Ğ²Ñ‹Ğ±Ñ€Ğ¾ÑĞ°Ğ¼
            â€¢ ĞĞµĞ´Ğ¸Ñ„Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ€ÑƒĞµĞ¼Ğ° Ğ² Ğ½ÑƒĞ»Ğµ
            â€¢ Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ¾ ÑˆÑ‚Ñ€Ğ°Ñ„ÑƒĞµÑ‚ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸

Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ:
     â†‘ Loss
     â”‚     â•±â”‚â•²
     â”‚   â•±  â”‚  â•²
     â”‚ â•±    â”‚    â•²
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’ (y - Å·)
     â”‚     0
```

#### 3. **ĞšÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ (Cross-Entropy)**

```
Ğ‘Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ°Ñ: L = -[y log(Å·) + (1-y) log(1-Å·)]

ĞœĞ½Ğ¾Ğ³Ğ¾ĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ°Ñ: L = -Î£ yáµ¢ log(Å·áµ¢)

ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ: ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ
ĞÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸: â€¢ Ğ¡Ğ¸Ğ»ÑŒĞ½Ğ¾ ÑˆÑ‚Ñ€Ğ°Ñ„ÑƒĞµÑ‚ ÑƒĞ²ĞµÑ€ĞµĞ½Ğ½Ñ‹Ğµ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ñ
            â€¢ Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ñ Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸
            â€¢ Ğ’Ñ‹Ğ¿ÑƒĞºĞ»Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ

Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº (Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹):
     â†‘ Loss
     â”‚ â•±
     â”‚â•±
     â”‚â•²
     â”‚ â•²____
â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’ Å·
     0     1
```

### ğŸ“ˆ ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°

#### ĞšĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ

```
ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº (Confusion Matrix):
                 ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ
                 â”‚  0  â”‚  1  â”‚
Ğ˜ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğµ    â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ     0   â”‚ TN  â”‚ FP  â”‚
             1   â”‚ FN  â”‚ TP  â”‚
```

**ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸:**
- **Accuracy** = (TP + TN) / (TP + TN + FP + FN)
- **Precision** = TP / (TP + FP)
- **Recall** = TP / (TP + FN)
- **F1-Score** = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

#### Ğ ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ

```
ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ°              â”‚ Ğ¤Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ°                    â”‚ Ğ˜Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ°Ñ†Ğ¸Ñ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MSE                  â”‚ 1/n Î£(y - Å·)Â²              â”‚ Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ ĞºĞ². Ğ¾ÑˆĞ¸Ğ±ĞºĞ°
RMSE                 â”‚ âˆš(1/n Î£(y - Å·)Â²)           â”‚ Ğ’ ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ğ°Ñ… Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹
MAE                  â”‚ 1/n Î£|y - Å·|               â”‚ Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ğ°Ğ±Ñ. Ğ¾ÑˆĞ¸Ğ±ĞºĞ°
RÂ²                   â”‚ 1 - SS_res/SS_tot          â”‚ Ğ”Ğ¾Ğ»Ñ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ğ¸
```

### ğŸ’» ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ 2.4: Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

class LossFunctions:
    @staticmethod
    def mse(y_true, y_pred):
        """Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°"""
        return np.mean((y_true - y_pred) ** 2)
    
    @staticmethod
    def mse_derivative(y_true, y_pred):
        """ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ MSE"""
        return 2 * (y_pred - y_true) / len(y_true)
    
    @staticmethod
    def mae(y_true, y_pred):
        """Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ°Ñ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°"""
        return np.mean(np.abs(y_true - y_pred))
    
    @staticmethod
    def mae_derivative(y_true, y_pred):
        """ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ MAE"""
        return np.sign(y_pred - y_true) / len(y_true)
    
    @staticmethod
    def binary_cross_entropy(y_true, y_pred):
        """Ğ‘Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ°Ñ ĞºÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ"""
        epsilon = 1e-15  # Ğ´Ğ»Ñ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    
    @staticmethod
    def binary_cross_entropy_derivative(y_true, y_pred):
        """ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ Ğ±Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ¾Ğ¹ ĞºÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ğ¸"""
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        return (y_pred - y_true) / (y_pred * (1 - y_pred) * len(y_true))
    
    @staticmethod
    def categorical_cross_entropy(y_true, y_pred):
        """ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ ĞºÑ€Ğ¾ÑÑ-ÑĞ½Ñ‚Ñ€Ğ¾Ğ¿Ğ¸Ñ"""
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))
    
    @staticmethod
    def visualize_loss_functions():
        """Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ"""
        # Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
        y_true = 1.0  # Ğ¸ÑÑ‚Ğ¸Ğ½Ğ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ
        y_pred = np.linspace(0.01, 0.99, 100)
        
        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ
        bce_loss = []
        mse_loss = []
        mae_loss = []
        
        for pred in y_pred:
            bce_loss.append(LossFunctions.binary_cross_entropy(
                np.array([y_true]), np.array([pred])
            ))
            mse_loss.append(LossFunctions.mse(
                np.array([y_true]), np.array([pred])
            ))
            mae_loss.append(LossFunctions.mae(
                np.array([y_true]), np.array([pred])
            ))
        
        plt.figure(figsize=(15, 5))
        
        # Binary Cross-Entropy
        plt.subplot(1, 3, 1)
        plt.plot(y_pred, bce_loss, 'b-', linewidth=2)
        plt.axvline(x=y_true, color='r', linestyle='--', label='True value')
        plt.title('Binary Cross-Entropy Loss')
        plt.xlabel('Predicted Value')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        # MSE
        plt.subplot(1, 3, 2)
        plt.plot(y_pred, mse_loss, 'g-', linewidth=2)
        plt.axvline(x=y_true, color='r', linestyle='--', label='True value')
        plt.title('Mean Squared Error Loss')
        plt.xlabel('Predicted Value')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        # MAE
        plt.subplot(1, 3, 3)
        plt.plot(y_pred, mae_loss, 'm-', linewidth=2)
        plt.axvline(x=y_true, color='r', linestyle='--', label='True value')
        plt.title('Mean Absolute Error Loss')
        plt.xlabel('Predicted Value')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.show()

class Metrics:
    @staticmethod
    def accuracy(y_true, y_pred):
        """Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ"""
        return np.mean(y_true == y_pred)
    
    @staticmethod
    def precision(y_true, y_pred, average='binary'):
        """Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ (precision)"""
        if average == 'binary':
            tp = np.sum((y_true == 1) & (y_pred == 1))
            fp = np.sum((y_true == 0) & (y_pred == 1))
            return tp / (tp + fp) if (tp + fp) > 0 else 0
        else:
            # ĞœĞ½Ğ¾Ğ³Ğ¾ĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ
            classes = np.unique(y_true)
            precisions = []
            for cls in classes:
                tp = np.sum((y_true == cls) & (y_pred == cls))
                fp = np.sum((y_true != cls) & (y_pred == cls))
                prec = tp / (tp + fp) if (tp + fp) > 0 else 0
                precisions.append(prec)
            return np.mean(precisions)
    
    @staticmethod
    def recall(y_true, y_pred, average='binary'):
        """ĞŸĞ¾Ğ»Ğ½Ğ¾Ñ‚Ğ° (recall)"""
        if average == 'binary':
            tp = np.sum((y_true == 1) & (y_pred == 1))
            fn = np.sum((y_true == 1) & (y_pred == 0))
            return tp / (tp + fn) if (tp + fn) > 0 else 0
        else:
            # ĞœĞ½Ğ¾Ğ³Ğ¾ĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ°Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ‚Ğ°
            classes = np.unique(y_true)
            recalls = []
            for cls in classes:
                tp = np.sum((y_true == cls) & (y_pred == cls))
                fn = np.sum((y_true == cls) & (y_pred != cls))
                rec = tp / (tp + fn) if (tp + fn) > 0 else 0
                recalls.append(rec)
            return np.mean(recalls)
    
    @staticmethod
    def f1_score(y_true, y_pred, average='binary'):
        """F1-Ğ¼ĞµÑ€Ğ°"""
        prec = Metrics.precision(y_true, y_pred, average)
        rec = Metrics.recall(y_true, y_pred, average)
        return 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0
    
    @staticmethod
    def confusion_matrix_plot(y_true, y_pred, classes=None):
        """Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº"""
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=classes, yticklabels=classes)
        plt.title('Confusion Matrix')
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.show()
        
        return cm

# Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ
LossFunctions.visualize_loss_functions()

# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº
np.random.seed(42)
y_true = np.random.randint(0, 2, 100)
y_pred = np.random.randint(0, 2, 100)

print("ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸:")
print(f"Accuracy: {Metrics.accuracy(y_true, y_pred):.3f}")
print(f"Precision: {Metrics.precision(y_true, y_pred):.3f}")
print(f"Recall: {Metrics.recall(y_true, y_pred):.3f}")
print(f"F1-Score: {Metrics.f1_score(y_true, y_pred):.3f}")
```

### ğŸ¯ Ğ’Ñ‹Ğ±Ğ¾Ñ€ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ

```
Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ°                    â”‚ Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ           â”‚ ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ğ‘Ğ¸Ğ½Ğ°Ñ€Ğ½Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ñ    â”‚ Binary Cross-Entropy    â”‚ Accuracy, F1, AUC
ĞœĞ½Ğ¾Ğ³Ğ¾ĞºĞ»Ğ°ÑÑĞ¾Ğ²Ğ°Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸Ğº. â”‚ Categorical Cross-Entropyâ”‚ Accuracy, F1-macro
Ğ ĞµĞ³Ñ€ĞµÑÑĞ¸Ñ                 â”‚ MSE / MAE               â”‚ RMSE, MAE, RÂ²
ĞĞµÑĞ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ â”‚ Focal Loss              â”‚ F1, Precision, Recall
```

---

## ğŸ”§ Ğ“Ğ»Ğ°Ğ²Ğ° 2.5: Ğ ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ

### ğŸ¯ ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ° Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ

```
ĞĞµĞ´Ğ¾Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ    ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ    ĞŸĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ    â”‚  â”‚   Ğ¡Ğ±Ğ°Ğ»Ğ°Ğ½ÑĞ¸Ñ€.    â”‚  â”‚   Ğ¡Ğ»Ğ¾Ğ¶Ğ½Ğ°Ñ   â”‚
â”‚  Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ     â”‚  â”‚    Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ       â”‚  â”‚   Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ    â”‚
â”‚             â”‚  â”‚                 â”‚  â”‚             â”‚
â”‚ Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹     â”‚  â”‚ ĞĞ¸Ğ·ĞºĞ¸Ğ¹ bias     â”‚  â”‚ ĞĞ¸Ğ·ĞºĞ¸Ğ¹ bias â”‚
â”‚ bias        â”‚  â”‚ ĞĞ¸Ğ·ĞºĞ¸Ğ¹ variance â”‚  â”‚ Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹     â”‚
â”‚ ĞĞ¸Ğ·ĞºĞ¸Ğ¹      â”‚  â”‚                 â”‚  â”‚ variance    â”‚
â”‚ variance    â”‚  â”‚                 â”‚  â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ›¡ï¸ ĞœĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸

#### 1. **L1 Ğ¸ L2 Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ**

```
L1 Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (Lasso):
Loss = MSE + Î» Î£|wáµ¢|

L2 Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (Ridge):
Loss = MSE + Î» Î£wáµ¢Â²

Elastic Net:
Loss = MSE + Î»â‚ Î£|wáµ¢| + Î»â‚‚ Î£wáµ¢Â²

Ğ­Ñ„Ñ„ĞµĞºÑ‚ Ğ½Ğ° Ğ²ĞµÑĞ°:
L1: â”‚    â•±â”‚â•²         L2: â”‚      â•±â”‚â•²
    â”‚  â•±  â”‚  â•²           â”‚    â•±  â”‚  â•²
    â”‚ â•±   â”‚   â•²          â”‚  â•±    â”‚    â•²
â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€      â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€
    â”‚    0               â”‚    0
    
L1 â†’ Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ²ĞµÑĞ°   L2 â†’ Ğ¼Ğ°Ğ»Ñ‹Ğµ Ğ²ĞµÑĞ°
```

#### 2. **Dropout**

```
ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ                    Ğ˜Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  o    o    o    o   â”‚    â”‚  o    o    o    o   â”‚
â”‚  â”‚    â”‚    â”‚    â”‚   â”‚    â”‚  â”‚    â”‚    â”‚    â”‚   â”‚
â”‚  o    Ã—    o    Ã—   â”‚    â”‚  o    o    o    o   â”‚
â”‚  â”‚    Ã—    â”‚    Ã—   â”‚    â”‚  â”‚    â”‚    â”‚    â”‚   â”‚
â”‚  o    Ã—    o    o   â”‚    â”‚  o    o    o    o   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
50% Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ² Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ñ‹     Ğ’ÑĞµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ñ‹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹

ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸:
r ~ Bernoulli(p)
y = r * x / p  (Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ)
```

#### 3. **Batch Normalization**

```
ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ°: Internal Covariate Shift
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ğ¡Ğ»Ğ¾Ğ¹ 1    Ğ¡Ğ»Ğ¾Ğ¹ 2    Ğ¡Ğ»Ğ¾Ğ¹ 3    Ğ¡Ğ»Ğ¾Ğ¹ 4    Ğ¡Ğ»Ğ¾Ğ¹ 5    â”‚
â”‚   â”‚        â”‚         â”‚         â”‚         â”‚        â”‚
â”‚   â–¼        â–¼         â–¼         â–¼         â–¼        â”‚
â”‚ ĞĞ¾Ñ€Ğ¼.   Ğ¡Ğ´Ğ²Ğ¸Ğ³      Ğ¡Ğ´Ğ²Ğ¸Ğ³     Ğ¡Ğ´Ğ²Ğ¸Ğ³     Ğ¡Ğ´Ğ²Ğ¸Ğ³     â”‚
â”‚Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´. Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´.  Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´.  Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´.  Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´.    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ Batch Normalization:
Î¼ = 1/m Î£xáµ¢                    # ÑÑ€ĞµĞ´Ğ½ĞµĞµ Ğ¿Ğ¾ Ğ±Ğ°Ñ‚Ñ‡Ñƒ
ÏƒÂ² = 1/m Î£(xáµ¢ - Î¼)Â²            # Ğ´Ğ¸ÑĞ¿ĞµÑ€ÑĞ¸Ñ Ğ¿Ğ¾ Ğ±Ğ°Ñ‚Ñ‡Ñƒ
xÌ‚áµ¢ = (xáµ¢ - Î¼) / âˆš(ÏƒÂ² + Îµ)      # Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
yáµ¢ = Î³xÌ‚áµ¢ + Î²                   # Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ ÑĞ´Ğ²Ğ¸Ğ³
```

### ğŸ“Š Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸

```
Batch Norm     â”‚ Layer Norm     â”‚ Instance Norm  â”‚ Group Norm
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ĞĞ¾Ñ€Ğ¼. Ğ¿Ğ¾ Ğ±Ğ°Ñ‚Ñ‡Ñƒ â”‚ ĞĞ¾Ñ€Ğ¼. Ğ¿Ğ¾ ÑĞ»Ğ¾Ñ  â”‚ ĞĞ¾Ñ€Ğ¼. Ğ¿Ğ¾ ĞºĞ°Ğ½Ğ°Ğ» â”‚ ĞĞ¾Ñ€Ğ¼. Ğ¿Ğ¾ Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ
               â”‚                â”‚                â”‚
BÃ—HÃ—WÃ—C        â”‚ BÃ—HÃ—WÃ—C        â”‚ BÃ—HÃ—WÃ—C        â”‚ BÃ—HÃ—WÃ—C
   â†“           â”‚    â†“           â”‚    â†“           â”‚    â†“
Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ¿Ğ¾     â”‚ Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ¿Ğ¾     â”‚ Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ¿Ğ¾     â”‚ Ğ¡Ñ€ĞµĞ´Ğ½ĞµĞµ Ğ¿Ğ¾
HÃ—WÃ—C          â”‚ HÃ—WÃ—C          â”‚ HÃ—W            â”‚ Ğ¿Ğ¾Ğ´Ğ³Ñ€ÑƒĞ¿Ğ¿Ğµ ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ²
```

### ğŸ’» ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ 2.5: Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸

```python
import numpy as np
import matplotlib.pyplot as plt

class RegularizedNeuralNetwork:
    def __init__(self, layers, learning_rate=0.01, dropout_rate=0.5, 
                 l1_reg=0.0, l2_reg=0.01, use_batch_norm=True):
        """
        ĞĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹
        
        Args:
            layers: Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ÑĞµÑ‚Ğ¸
            learning_rate: ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
            dropout_rate: Ğ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ°
            l1_reg: ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ L1 Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
            l2_reg: ĞºĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚ L2 Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
            use_batch_norm: Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ batch normalization
        """
        self.layers = layers
        self.learning_rate = learning_rate
        self.dropout_rate = dropout_rate
        self.l1_reg = l1_reg
        self.l2_reg = l2_reg
        self.use_batch_norm = use_batch_norm
        
        # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
        self.weights = []
        self.biases = []
        self.gamma = []  # Ğ´Ğ»Ñ batch norm
        self.beta = []   # Ğ´Ğ»Ñ batch norm
        
        for i in range(len(layers) - 1):
            # Xavier/He Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
            if i == 0:
                w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])
            else:
                w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])
            
            b = np.zeros((1, layers[i+1]))
            
            self.weights.append(w)
            self.biases.append(b)
            
            # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ batch normalization
            if self.use_batch_norm and i < len(layers) - 2:  # Ğ½Ğµ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ
                self.gamma.append(np.ones((1, layers[i+1])))
                self.beta.append(np.zeros((1, layers[i+1])))
    
    def relu(self, x):
        """ReLU Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ"""
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        """ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ ReLU"""
        return (x > 0).astype(float)
    
    def softmax(self, x):
        """Softmax Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ"""
        # Ğ”Ğ»Ñ Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ²Ñ‹Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼
        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=1, keepdims=True)
    
    def dropout(self, x, rate, training=True):
        """Dropout Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ"""
        if not training:
            return x
        
        keep_prob = 1 - rate
        mask = np.random.binomial(1, keep_prob, size=x.shape) / keep_prob
        return x * mask
    
    def forward(self, X, training=True):
        """ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ"""
        activations = [X]
        z_values = []
        dropout_masks = []
        
        for i in range(len(self.weights)):
            # Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ
            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            z_values.append(z)
            
            # ĞĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ
            if i < len(self.weights) - 1:  # ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸
                a = self.relu(z)
                # Dropout Ğ´Ğ»Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ²
                if training:
                    a = self.dropout(a, self.dropout_rate, training)
            else:  # Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¹
                a = self.softmax(z)
            
            activations.append(a)
        
        return activations, z_values
    
    def compute_loss(self, y_true, y_pred):
        """Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ"""
        # Categorical Cross-Entropy
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        ce_loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))
        
        # L2 Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        l2_loss = 0
        for w in self.weights:
            l2_loss += np.sum(w ** 2)
        
        total_loss = ce_loss + self.l2_reg * l2_loss
        return total_loss, ce_loss
    
    def backward(self, X, y, activations, z_values):
        """ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ"""
        m = X.shape[0]
        
        # Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹
        dW = [np.zeros_like(w) for w in self.weights]
        db = [np.zeros_like(b) for b in self.biases]
        
        # ĞÑˆĞ¸Ğ±ĞºĞ° Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ
        delta = activations[-1] - y
        
        # ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
        for i in range(len(self.weights) - 1, -1, -1):
            # Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¸ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹
            dW[i] = np.dot(activations[i].T, delta) / m + self.l2_reg * self.weights[i]
            db[i] = np.sum(delta, axis=0, keepdims=True) / m
            
            # Ğ Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ½Ğ° Ğ¿Ñ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ¸Ğ¹ ÑĞ»Ğ¾Ğ¹
            if i > 0:
                delta = np.dot(delta, self.weights[i].T) * self.relu_derivative(z_values[i-1])
        
        return dW, db
    
    def adam_update(self, dW, db):
        """ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Adam optimizer"""
        self.t += 1
        
        for i in range(len(self.weights)):
            # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ²ĞµÑĞ¾Ğ²
            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dW[i]
            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dW[i] ** 2)
            
            # ĞšĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ
            m_w_corrected = self.m_w[i] / (1 - self.beta1 ** self.t)
            v_w_corrected = self.v_w[i] / (1 - self.beta2 ** self.t)
            
            # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ¾Ğ²
            self.weights[i] -= self.learning_rate * m_w_corrected / (np.sqrt(v_w_corrected) + self.epsilon)
            
            # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹
            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]
            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)
            
            # ĞšĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ñ
            m_b_corrected = self.m_b[i] / (1 - self.beta1 ** self.t)
            v_b_corrected = self.v_b[i] / (1 - self.beta2 ** self.t)
            
            # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¼ĞµÑ‰ĞµĞ½Ğ¸Ğ¹
            self.biases[i] -= self.learning_rate * m_b_corrected / (np.sqrt(v_b_corrected) + self.epsilon)
    
    def accuracy(self, y_true, y_pred):
        """Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸"""
        predictions = np.argmax(y_pred, axis=1)
        true_labels = np.argmax(y_true, axis=1)
        return np.mean(predictions == true_labels)
    
    def train(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):
        """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸"""
        n_batches = len(X_train) // batch_size
        
        print("ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ...")
        print(f"ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¿Ğ¾Ñ…: {epochs}")
        print(f"Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ±Ğ°Ñ‚Ñ‡Ğ°: {batch_size}")
        print(f"ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ±Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ·Ğ° ÑĞ¿Ğ¾Ñ…Ñƒ: {n_batches}")
        print("-" * 50)
        
        for epoch in range(epochs):
            epoch_loss = 0
            epoch_accuracy = 0
            
            # ĞŸĞµÑ€ĞµĞ¼ĞµÑˆĞ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
            indices = np.random.permutation(len(X_train))
            X_train_shuffled = X_train[indices]
            y_train_shuffled = y_train[indices]
            
            # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ Ğ±Ğ°Ñ‚Ñ‡Ğ°Ğ¼
            for batch in range(n_batches):
                start_idx = batch * batch_size
                end_idx = start_idx + batch_size
                
                X_batch = X_train_shuffled[start_idx:end_idx]
                y_batch = y_train_shuffled[start_idx:end_idx]
                
                # ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
                activations, z_values = self.forward(X_batch, training=True)
                
                # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ
                loss, ce_loss = self.compute_loss(y_batch, activations[-1])
                epoch_loss += loss
                
                # Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ
                acc = self.accuracy(y_batch, activations[-1])
                epoch_accuracy += acc
                
                # ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
                dW, db = self.backward(X_batch, y_batch, activations, z_values)
                
                # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ²
                self.adam_update(dW, db)
            
            # Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ·Ğ° ÑĞ¿Ğ¾Ñ…Ñƒ
            avg_loss = epoch_loss / n_batches
            avg_accuracy = epoch_accuracy / n_batches
            
            # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ
            val_activations, _ = self.forward(X_val, training=False)
            val_loss, _ = self.compute_loss(y_val, val_activations[-1])
            val_accuracy = self.accuracy(y_val, val_activations[-1])
            
            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ğ¸
            self.train_losses.append(avg_loss)
            self.train_accuracies.append(avg_accuracy)
            self.val_losses.append(val_loss)
            self.val_accuracies.append(val_accuracy)
            
            # Ğ’Ñ‹Ğ²Ğ¾Ğ´ Ğ¿Ñ€Ğ¾Ğ³Ñ€ĞµÑÑĞ°
            if epoch % 10 == 0 or epoch == epochs - 1:
                print(f"Ğ­Ğ¿Ğ¾Ñ…Ğ° {epoch:3d}: "
                      f"Train Loss: {avg_loss:.4f}, Train Acc: {avg_accuracy:.3f}, "
                      f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.3f}")
        
        print("-" * 50)
        print("ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾!")
        print(f"Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸: {self.val_accuracies[-1]:.3f}")
    
    def predict(self, X):
        """ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ"""
        activations, _ = self.forward(X, training=False)
        return activations[-1]
    
    def plot_training_history(self):
        """Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ
        ax1.plot(self.train_losses, 'b-', label='Train Loss')
        ax1.plot(self.val_losses, 'r-', label='Validation Loss')
        ax1.set_title('Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ')
        ax1.set_xlabel('Ğ­Ğ¿Ğ¾Ñ…Ğ°')
        ax1.set_ylabel('ĞŸĞ¾Ñ‚ĞµÑ€Ñ')
        ax1.legend()
        ax1.grid(True)
        
        # Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ
        ax2.plot(self.train_accuracies, 'b-', label='Train Accuracy')
        ax2.plot(self.val_accuracies, 'r-', label='Validation Accuracy')
        ax2.set_title('Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ')
        ax2.set_xlabel('Ğ­Ğ¿Ğ¾Ñ…Ğ°')
        ax2.set_ylabel('Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ')
        ax2.legend()
        ax2.grid(True)
        
        # Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ (Ğ»Ğ¾Ğ³Ğ°Ñ€Ğ¸Ñ„Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ°Ñ ÑˆĞºĞ°Ğ»Ğ°)
        ax3.semilogy(self.train_losses, 'b-', label='Train Loss')
        ax3.semilogy(self.val_losses, 'r-', label='Validation Loss')
        ax3.set_title('Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ (log scale)')
        ax3.set_xlabel('Ğ­Ğ¿Ğ¾Ñ…Ğ°')
        ax3.set_ylabel('ĞŸĞ¾Ñ‚ĞµÑ€Ñ (log)')
        ax3.legend()
        ax3.grid(True)
        
        # Ğ Ğ°Ğ·Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¼ĞµĞ¶Ğ´Ñƒ train Ğ¸ validation accuracy
        overfitting = np.array(self.train_accuracies) - np.array(self.val_accuracies)
        ax4.plot(overfitting, 'g-', label='Overfitting Gap')
        ax4.axhline(y=0, color='k', linestyle='--', alpha=0.5)
        ax4.set_title('ĞŸĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ (Train Acc - Val Acc)')
        ax4.set_xlabel('Ğ­Ğ¿Ğ¾Ñ…Ğ°')
        ax4.set_ylabel('Ğ Ğ°Ğ·Ğ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸')
        ax4.legend()
        ax4.grid(True)
        
        plt.tight_layout()
        plt.show()
    
    def plot_confusion_matrix(self, X_test, y_test):
        """ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº"""
        predictions = self.predict(X_test)
        pred_labels = np.argmax(predictions, axis=1)
        true_labels = np.argmax(y_test, axis=1)
        
        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ‹ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº
        cm = np.zeros((10, 10))
        for i in range(len(pred_labels)):
            cm[true_labels[i], pred_labels[i]] += 1
        
        # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ¾ĞºĞ°Ğ¼
        cm_normalized = cm / cm.sum(axis=1, keepdims=True)
        
        # Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                   xticklabels=range(10), yticklabels=range(10))
        plt.title('ĞœĞ°Ñ‚Ñ€Ğ¸Ñ†Ğ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº (Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ)')
        plt.xlabel('ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ')
        plt.ylabel('Ğ˜ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ')
        plt.show()
        
        return cm
    
    def visualize_weights(self):
        """Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ"""
        # Ğ’ĞµÑĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ°Ğº Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
        weights = self.weights[0].T  # Ğ¢Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ğ½Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ñ‹
        
        # Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿ĞµÑ€Ğ²Ñ‹Ñ… 16 Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ¾Ğ²
        fig, axes = plt.subplots(4, 4, figsize=(10, 10))
        for i in range(16):
            ax = axes[i // 4, i % 4]
            weight_img = weights[i].reshape(28, 28)
            ax.imshow(weight_img, cmap='RdBu', vmin=-np.max(np.abs(weight_img)), 
                     vmax=np.max(np.abs(weight_img)))
            ax.set_title(f'ĞĞµĞ¹Ñ€Ğ¾Ğ½ {i+1}')
            ax.axis('off')
        
        plt.suptitle('Ğ’ĞµÑĞ° Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ (Ğ´ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ²)')
        plt.tight_layout()
        plt.show()
    
    def test_predictions(self, X_test, y_test, n_samples=10):
        """Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ñ‹Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ñ…"""
        indices = np.random.choice(len(X_test), n_samples, replace=False)
        
        fig, axes = plt.subplots(2, 5, figsize=(15, 6))
        
        for i, idx in enumerate(indices):
            ax = axes[i // 5, i % 5]
            
            # ĞŸĞ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
            image = X_test[idx].reshape(28, 28)
            ax.imshow(image, cmap='gray')
            
            # ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğµ
            prediction = self.predict(X_test[idx:idx+1])
            pred_class = np.argmax(prediction)
            pred_prob = prediction[0, pred_class]
            
            # Ğ˜ÑÑ‚Ğ¸Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ
            true_class = np.argmax(y_test[idx])
            
            # Ğ¦Ğ²ĞµÑ‚ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°: Ğ·ĞµĞ»ĞµĞ½Ñ‹Ğ¹ ĞµÑĞ»Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾, ĞºÑ€Ğ°ÑĞ½Ñ‹Ğ¹ ĞµÑĞ»Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾
            color = 'green' if pred_class == true_class else 'red'
            ax.set_title(f'True: {true_class}, Pred: {pred_class} ({pred_prob:.2f})', 
                        color=color)
            ax.axis('off')
        
        plt.suptitle('ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹ Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ¹')
        plt.tight_layout()
        plt.show()

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MNIST
def load_mnist_data():
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MNIST"""
    print("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… MNIST...")
    
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    mnist = fetch_openml('mnist_784', version=1, as_frame=False)
    X, y = mnist.data, mnist.target.astype(int)
    
    # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    X = X / 255.0
    
    # One-hot ĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ´Ğ»Ñ Ğ¼ĞµÑ‚Ğ¾Ğº
    y_onehot = np.zeros((len(y), 10))
    for i, label in enumerate(y):
        y_onehot[i, label] = 1
    
    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_onehot, test_size=0.2, random_state=42, stratify=y
    )
    
    # Ğ Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ train Ğ½Ğ° train/validation
    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.2, random_state=42, stratify=np.argmax(y_train, axis=1)
    )
    
    print(f"Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¾Ğ±ÑƒÑ‡Ğ°ÑÑ‰ĞµĞ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸: {X_train.shape}")
    print(f"Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸: {X_val.shape}")
    print(f"Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸: {X_test.shape}")
    
    return X_train, X_val, X_test, y_train, y_val, y_test

# Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹
def demonstrate_mnist_training():
    """Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° MNIST"""
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    X_train, X_val, X_test, y_train, y_val, y_test = load_mnist_data()
    
    # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    model = MNISTNeuralNetwork()
    model.train(X_train, y_train, X_val, y_val, epochs=50, batch_size=128)
    
    # ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
    test_predictions = model.predict(X_test)
    test_accuracy = model.accuracy(y_test, test_predictions)
    print(f"\nĞ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {test_accuracy:.3f}")
    
    # Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
    model.plot_training_history()
    model.plot_confusion_matrix(X_test, y_test)
    model.visualize_weights()
    model.test_predictions(X_test, y_test)
    
    return model

# Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸
# model = demonstrate_mnist_training()
```

---

## ğŸ¯ Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ‘Ğ»Ğ¾ĞºĞ° 2

### âœ… Ğ§Ñ‚Ğ¾ Ğ²Ñ‹ Ğ´Ğ¾ÑÑ‚Ğ¸Ğ³Ğ»Ğ¸

```
ğŸ“Š Ğ¢ĞµĞ¾Ñ€ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ:
â”œâ”€â”€ ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹
â”œâ”€â”€ Ğ—Ğ½Ğ°Ğ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¸Ñ… ÑĞ²Ğ¾Ğ¹ÑÑ‚Ğ²
â”œâ”€â”€ Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ backpropagation
â”œâ”€â”€ Ğ—Ğ½Ğ°Ğ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸Ğº
â””â”€â”€ ĞœĞµÑ‚Ğ¾Ğ´Ñ‹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸

ğŸ’» ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸:
â”œâ”€â”€ Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ñ Ğ½ÑƒĞ»Ñ
â”œâ”€â”€ ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
â”œâ”€â”€ Ğ”Ğ¾ÑÑ‚Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¹ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ (95%+)
â”œâ”€â”€ Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
â””â”€â”€ ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²

ğŸ› ï¸ Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹:
â”œâ”€â”€ NumPy Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹
â”œâ”€â”€ Matplotlib Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
â”œâ”€â”€ Gradient descent Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
â”œâ”€â”€ Adam optimizer
â””â”€â”€ Ğ ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ L1/L2 + Dropout
```

### ğŸ† Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ°

| ĞšÑ€Ğ¸Ñ‚ĞµÑ€Ğ¸Ğ¹ | ĞÑ†ĞµĞ½ĞºĞ° | ĞšĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ |
|----------|--------|-------------|
| **Ğ¢ĞµĞ¾Ñ€Ğ¸Ñ** | â­â­â­â­â­ | ĞŸĞ¾Ğ»Ğ½Ğ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ² |
| **ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°** | â­â­â­â­â­ | Ğ Ğ°Ğ±Ğ¾Ñ‡Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Ğ½ÑƒĞ»Ñ |
| **Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚** | â­â­â­â­â­ | Ğ¢Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ 95%+ Ğ½Ğ° MNIST |
| **ĞšĞ¾Ğ´** | â­â­â­â­â˜† | Ğ§Ğ¸ÑÑ‚Ñ‹Ğ¹, Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ |

### ğŸš€ Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ Ğº ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼Ñƒ Ğ±Ğ»Ğ¾ĞºÑƒ

```
âœ… ĞŸĞ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ¾ÑĞ½Ğ¾Ğ² Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹
âœ… ĞĞ°Ğ²Ñ‹ĞºĞ¸ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ½ÑƒĞ»Ñ
âœ… ĞĞ¿Ñ‹Ñ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
âœ… Ğ—Ğ½Ğ°Ğ½Ğ¸Ğµ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ² Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸
âœ… Ğ£Ğ¼ĞµĞ½Ğ¸Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹

â¡ï¸ Ğ“Ğ¾Ñ‚Ğ¾Ğ² Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ CNN (Ğ‘Ğ»Ğ¾Ğº 3)
```

---

## ğŸ“š Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ°Ñ‚ĞµÑ€Ğ¸Ğ°Ğ»Ñ‹

### ğŸ”— Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼Ñ‹Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹

**ĞšĞ½Ğ¸Ğ³Ğ¸:**
- Neural Networks and Deep Learning (Michael Nielsen)
- Deep Learning (Ian Goodfellow, Yoshua Bengio, Aaron Courville)
- Pattern Recognition and Machine Learning (Christopher Bishop)

**ĞĞ½Ğ»Ğ°Ğ¹Ğ½ ĞºÑƒÑ€ÑÑ‹:**
- CS231n: Convolutional Neural Networks for Visual Recognition
- Andrew Ng's Deep Learning Specialization
- Fast.ai Practical Deep Learning

**Ğ˜Ğ½ÑÑ‚Ñ€ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹:**
- TensorFlow/Keras
- PyTorch
- JAX
- Weights & Biases Ğ´Ğ»Ñ tracking

### ğŸ¯ ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ñ

1. **Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€:**
   - ĞĞ²Ñ‚Ğ¾ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ´Ğ»Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
   - ĞœĞ½Ğ¾Ğ³Ğ¾ĞºĞ»Ğ°ÑÑĞ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ CIFAR-10
   - Ğ ĞµĞ³Ñ€ĞµÑÑĞ¸Ğ¾Ğ½Ğ½Ğ°Ñ ÑĞµÑ‚ÑŒ Ğ´Ğ»Ñ Boston Housing

2. **Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚Ñ‹ Ñ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸:**
   - Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ²
   - Ğ’Ğ»Ğ¸ÑĞ½Ğ¸Ğµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ±Ğ°Ñ‚Ñ‡Ğ° Ğ½Ğ° Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
   - Optimal learning rate scheduling

3. **ĞŸÑ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ñ‹Ğµ Ñ‚ĞµÑ…Ğ½Ğ¸ĞºĞ¸:**
   - Batch Normalization Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
   - Ğ Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ²ĞµÑĞ¾Ğ²
   - Early stopping Ğ¸ learning rate decay

---

## ğŸ“ Ğ—Ğ°ĞºĞ»ÑÑ‡ĞµĞ½Ğ¸Ğµ

ĞŸĞ¾Ğ·Ğ´Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼! Ğ’Ñ‹ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ¸Ğ»Ğ¸ **Ğ‘Ğ»Ğ¾Ğº 2: ĞÑĞ½Ğ¾Ğ²Ñ‹ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ñ… ÑĞµÑ‚ĞµĞ¹ Ğ¸ Ğ¸Ñ… Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ**. 

Ğ¢ĞµĞ¿ĞµÑ€ÑŒ Ñƒ Ğ²Ğ°Ñ ĞµÑÑ‚ÑŒ:
- **Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ** Ñ‚Ğ¾Ğ³Ğ¾, ĞºĞ°Ğº Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ÑÑ‚ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğµ ÑĞµÑ‚Ğ¸
- **ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸** ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
- **ĞĞ¿Ñ‹Ñ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹** Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸
- **Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ** Ğº Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ»Ğ¾Ğ¶Ğ½Ñ‹Ñ… Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€

Ğ’ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼ Ğ±Ğ»Ğ¾ĞºĞµ Ğ¼Ñ‹ Ğ¿ĞµÑ€ĞµĞ¹Ğ´ĞµĞ¼ Ğº **ÑĞ²ĞµÑ€Ñ‚Ğ¾Ñ‡Ğ½Ñ‹Ğ¼ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¼ ÑĞµÑ‚ÑĞ¼ (CNN)** Ğ¸ Ğ¸Ñ… Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ Ğ² ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğ¼ Ğ·Ñ€ĞµĞ½Ğ¸Ğ¸. Ğ’Ñ‹ ÑƒĞ·Ğ½Ğ°ĞµÑ‚Ğµ, ĞºĞ°Ğº Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ Ñ€Ğ°ÑĞ¿Ğ¾Ğ·Ğ½Ğ°Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞºÑ‚Ğ¾Ğ².

```
ğŸ¯ Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ğ±Ğ»Ğ¾Ğº: CNN Ğ¸ ĞºĞ¾Ğ¼Ğ¿ÑŒÑÑ‚ĞµÑ€Ğ½Ğ¾Ğµ Ğ·Ñ€ĞµĞ½Ğ¸Ğµ
ğŸ“… ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ: 6-8 Ğ½ĞµĞ´ĞµĞ»ÑŒ
ğŸš€ Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ ÑĞ»Ğ¾Ğ¶Ğ½Ğ¾ÑÑ‚Ğ¸: â­â­â­â­â˜†
```

**Ğ£Ğ´Ğ°Ñ‡Ğ¸ Ğ² Ğ´Ğ°Ğ»ÑŒĞ½ĞµĞ¹ÑˆĞµĞ¼ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ!** ğŸš€
    
    def sigmoid(self, x):
        """Ğ¡Ğ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ğ°"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def sigmoid_derivative(self, x):
        """ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ ÑĞ¸Ğ³Ğ¼Ğ¾Ğ¸Ğ´Ñ‹"""
        s = self.sigmoid(x)
        return s * (1 - s)
    
    def batch_norm(self, x, gamma, beta, epsilon=1e-8):
        """Batch normalization"""
        mu = np.mean(x, axis=0, keepdims=True)
        sigma2 = np.var(x, axis=0, keepdims=True)
        x_norm = (x - mu) / np.sqrt(sigma2 + epsilon)
        return gamma * x_norm + beta, mu, sigma2
    
    def dropout(self, x, rate, training=True):
        """Dropout Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ"""
        if not training:
            return x
        
        keep_prob = 1 - rate
        mask = np.random.binomial(1, keep_prob, size=x.shape)
        return x * mask / keep_prob
    
    def forward(self, X, training=True):
        """ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ"""
        activations = [X]
        z_values = []
        dropout_masks = []
        batch_norm_params = []
        
        for i in range(len(self.weights)):
            # Ğ›Ğ¸Ğ½ĞµĞ¹Ğ½Ğ°Ñ ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ
            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            z_values.append(z)
            
            # Batch normalization (ĞºÑ€Ğ¾Ğ¼Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ)
            if self.use_batch_norm and i < len(self.weights) - 1:
                z_norm, mu, sigma2 = self.batch_norm(z, self.gamma[i], self.beta[i])
                batch_norm_params.append((mu, sigma2))
                z = z_norm
            
            # ĞĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ
            if i < len(self.weights) - 1:  # ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸
                a = self.relu(z)
            else:  # Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑĞ»Ğ¾Ğ¹
                a = self.sigmoid(z)
            
            # Dropout (ĞºÑ€Ğ¾Ğ¼Ğµ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ñ)
            if training and i < len(self.weights) - 1:
                a = self.dropout(a, self.dropout_rate, training)
                dropout_masks.append(np.random.binomial(1, 1-self.dropout_rate, size=a.shape))
            
            activations.append(a)
        
        return activations, z_values, dropout_masks, batch_norm_params
    
    def compute_loss(self, y_true, y_pred):
        """Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹"""
        # ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€Ñ
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        ce_loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
        
        # L1 Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        l1_loss = 0
        if self.l1_reg > 0:
            for w in self.weights:
                l1_loss += np.sum(np.abs(w))
        
        # L2 Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
        l2_loss = 0
        if self.l2_reg > 0:
            for w in self.weights:
                l2_loss += np.sum(w ** 2)
        
        total_loss = ce_loss + self.l1_reg * l1_loss + self.l2_reg * l2_loss
        return total_loss, ce_loss, l1_loss, l2_loss
    
    def train(self, X, y, epochs=1000, validation_data=None):
        """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹"""
        train_losses = []
        val_losses = []
        
        for epoch in range(epochs):
            # ĞŸÑ€ÑĞ¼Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
            activations, z_values, dropout_masks, batch_norm_params = self.forward(X, training=True)
            
            # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ
            total_loss, ce_loss, l1_loss, l2_loss = self.compute_loss(y, activations[-1])
            train_losses.append(total_loss)
            
            # ĞĞ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğµ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ (ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ)
            # ... (Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±Ñ€Ğ°Ñ‚Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑĞ¿Ñ€Ğ¾ÑÑ‚Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ ÑƒÑ‡ĞµÑ‚Ğ¾Ğ¼ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸)
            
            # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ
            if validation_data is not None:
                X_val, y_val = validation_data
                val_activations, _, _, _ = self.forward(X_val, training=False)
                val_loss, _, _, _ = self.compute_loss(y_val, val_activations[-1])
                val_losses.append(val_loss)
            
            if epoch % 100 == 0:
                print(f'Epoch {epoch}: Total Loss = {total_loss:.4f}, '
                      f'CE Loss = {ce_loss:.4f}, L1 = {l1_loss:.4f}, L2 = {l2_loss:.4f}')
        
        return train_losses, val_losses
    
    def plot_regularization_comparison(self):
        """Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ ÑÑ„Ñ„ĞµĞºÑ‚Ğ° Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸"""
        # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ ÑĞ¸Ğ½Ñ‚ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        np.random.seed(42)
        X = np.random.randn(100, 2)
        y = (X[:, 0] + X[:, 1] > 0).astype(int).reshape(-1, 1)
        
        # ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ñ Ñ€Ğ°Ğ·Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹
        models = {
            'Ğ‘ĞµĞ· Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸': {'l1_reg': 0, 'l2_reg': 0, 'dropout_rate': 0},
            'L1 Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ': {'l1_reg': 0.01, 'l2_reg': 0, 'dropout_rate': 0},
            'L2 Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ': {'l1_reg': 0, 'l2_reg': 0.01, 'dropout_rate': 0},
            'Dropout': {'l1_reg': 0, 'l2_reg': 0, 'dropout_rate': 0.5}
        }
        
        plt.figure(figsize=(15, 10))
        
        for i, (name, params) in enumerate(models.items(), 1):
            plt.subplot(2, 2, i)
            
            # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            model = RegularizedNeuralNetwork(
                [2, 10, 1], 
                learning_rate=0.01,
                **params
            )
            
            # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
            losses, _ = model.train(X, y, epochs=500)
            
            # Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
            plt.plot(losses)
            plt.title(f'{name}')
            plt.xlabel('Ğ­Ğ¿Ğ¾Ñ…Ğ°')
            plt.ylabel('ĞŸĞ¾Ñ‚ĞµÑ€Ñ')
            plt.grid(True)
        
        plt.tight_layout()
        plt.show()

# Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ
model = RegularizedNeuralNetwork([2, 10, 5, 1])
model.plot_regularization_comparison()
```

### ğŸ¯ Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ

```
ĞœĞµÑ‚Ğ¾Ğ´              â”‚ ĞšĞ¾Ğ³Ğ´Ğ° Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ              â”‚ Ğ“Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
L1 Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ   â”‚ ĞÑƒĞ¶Ğ½Ğ° Ñ€Ğ°Ğ·Ñ€ĞµĞ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²ĞµÑĞ¾Ğ²      â”‚ Î» = 0.001-0.1
L2 Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ   â”‚ Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ    â”‚ Î» = 0.0001-0.01
Dropout           â”‚ Ğ‘Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ ÑĞµÑ‚Ğ¸, Ğ¿ĞµÑ€ĞµĞ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ     â”‚ p = 0.2-0.5
Batch Norm        â”‚ Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¸Ğµ ÑĞµÑ‚Ğ¸, Ğ½ĞµÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ  â”‚ Îµ = 1e-8, momentum = 0.9
```

---

## ğŸ† Ğ“Ğ»Ğ°Ğ²Ğ° 2.6: Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚ Ğ±Ğ»Ğ¾ĞºĞ°

### ğŸ¯ Ğ—Ğ°Ğ´Ğ°Ñ‡Ğ°: Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ñ Ğ½ÑƒĞ»Ñ

**Ğ¦ĞµĞ»ÑŒ:** Ğ ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑĞ²ÑĞ·Ğ½ÑƒÑ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½ÑƒÑ ÑĞµÑ‚ÑŒ Ğ½Ğ° Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ¼ NumPy Ğ¸ Ğ¾Ğ±ÑƒÑ‡Ğ¸Ñ‚ÑŒ ĞµÑ‘ Ğ½Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ MNIST.

**Ğ¢Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ:**
- ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°: 784 â†’ 128 â†’ 64 â†’ 10
- Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸: ReLU Ğ´Ğ»Ñ ÑĞºÑ€Ñ‹Ñ‚Ñ‹Ñ… ÑĞ»Ğ¾ĞµĞ², Softmax Ğ´Ğ»Ñ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾
- Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ: Categorical Cross-Entropy
- Ğ ĞµĞ³ÑƒĞ»ÑÑ€Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ: L2 + Dropout
- ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€: Adam
- Ğ¦ĞµĞ»ĞµĞ²Ğ°Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ: 95%+

### ğŸ’» ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import seaborn as sns

class MNISTNeuralNetwork:
    def __init__(self):
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½ĞµĞ¹Ñ€Ğ¾Ğ½Ğ½Ğ¾Ğ¹ ÑĞµÑ‚Ğ¸ Ğ´Ğ»Ñ MNIST"""
        self.layers = [784, 128, 64, 10]
        self.learning_rate = 0.001
        self.dropout_rate = 0.3
        self.l2_reg = 0.0001
        
        # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ² (He initialization)
        self.weights = []
        self.biases = []
        
        for i in range(len(self.layers) - 1):
            w = np.random.randn(self.layers[i], self.layers[i+1]) * np.sqrt(2.0 / self.layers[i])
            b = np.zeros((1, self.layers[i+1]))
            self.weights.append(w)
            self.biases.append(b)
        
        # ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´Ğ»Ñ Adam optimizer
        self.m_w = [np.zeros_like(w) for w in self.weights]
        self.v_w = [np.zeros_like(w) for w in self.weights]
        self.m_b = [np.zeros_like(b) for b in self.biases]
        self.v_b = [np.zeros_like(b) for b in self.biases]
        self.beta1 = 0.9
        self.beta2 = 0.999
        self.epsilon = 1e-8
        self.t = 0  # Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ğ¾Ğ¹ ÑˆĞ°Ğ³
        
        # Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ
        self.train_losses = []
        self.train_accuracies = []
        self.val_losses = []
        self.val_accuracies = []
    
    def relu(self, x):
        """ReLU Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ"""
        return np.maximum(0, x)
    
    def relu_derivative(self, x):
        """ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ ReLU"""
        return (x > 0).astype(float)