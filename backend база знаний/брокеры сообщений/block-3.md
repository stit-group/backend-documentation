# Ğ‘Ğ›ĞĞš 3: ĞŸĞ ĞĞšĞ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ ĞĞ—Ğ ĞĞ‘ĞĞ¢ĞšĞ

---

## ğŸ¯ **Ğ¦Ğ•Ğ›Ğ¬ Ğ‘Ğ›ĞĞšĞ**
**ĞÑĞ²Ğ¾ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ğ²Ñ‹ĞºĞ¾Ğ² Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ñ Ğ±Ñ€Ğ¾ĞºĞµÑ€Ğ°Ğ¼Ğ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹**

**ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ:** 5-6 Ğ½ĞµĞ´ĞµĞ»ÑŒ  
**Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚:** Ğ£Ğ¼ĞµĞ½Ğ¸Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ±Ñ€Ğ¾ĞºĞµÑ€Ğ¾Ğ² ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹

---

## ğŸ“‹ **Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ Ğ‘Ğ›ĞĞšĞ**

```
Ğ‘Ğ›ĞĞš 3: ĞŸĞ ĞĞšĞ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ Ğ ĞĞ—Ğ ĞĞ‘ĞĞ¢ĞšĞ
â”œâ”€â”€ 3.1 Ğ“Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğµ Ğ¿Ğ¾Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ² Apache Kafka (1.5 Ğ½ĞµĞ´ĞµĞ»Ğ¸)
â”œâ”€â”€ 3.2 Ğ Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Producer Ğ¸ Consumer Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹ (2 Ğ½ĞµĞ´ĞµĞ»Ğ¸)
â”œâ”€â”€ 3.3 ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ° Ñ RabbitMQ (1 Ğ½ĞµĞ´ĞµĞ»Ñ)
â”œâ”€â”€ 3.4 Ğ¡ĞµÑ€Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… (1 Ğ½ĞµĞ´ĞµĞ»Ñ)
â””â”€â”€ 3.5 ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚ÑŒ (0.5 Ğ½ĞµĞ´ĞµĞ»Ğ¸)
```

---

## âš¡ **Ğ“Ğ›ĞĞ’Ğ 3.1: Ğ“Ğ›Ğ£Ğ‘ĞĞšĞĞ• ĞŸĞĞ“Ğ Ğ£Ğ–Ğ•ĞĞ˜Ğ• Ğ’ APACHE KAFKA**

### **ğŸ—ï¸ Production-Ready Kafka Cluster Setup**

#### **ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRODUCTION KAFKA CLUSTER                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                   ZOOKEEPER ENSEMBLE                       â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚â”‚
â”‚  â”‚  â”‚ZooKeeper 1  â”‚  â”‚ZooKeeper 2  â”‚  â”‚ZooKeeper 3  â”‚        â”‚â”‚
â”‚  â”‚  â”‚   (Leader)  â”‚  â”‚ (Follower)  â”‚  â”‚ (Follower)  â”‚        â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                               â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚             KAFKA BROKERS   â”‚                               â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚â”‚
â”‚  â”‚  â”‚  Broker 1   â”‚  â”‚  Broker 2   â”‚  â”‚  Broker 3   â”‚        â”‚â”‚
â”‚  â”‚  â”‚  (ID: 1)    â”‚  â”‚  (ID: 2)    â”‚  â”‚  (ID: 3)    â”‚        â”‚â”‚
â”‚  â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚        â”‚â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚â”‚
â”‚  â”‚  â”‚ â”‚Topic A  â”‚ â”‚  â”‚ â”‚Topic A  â”‚ â”‚  â”‚ â”‚Topic A  â”‚ â”‚        â”‚â”‚
â”‚  â”‚  â”‚ â”‚Part 0(L)â”‚ â”‚  â”‚ â”‚Part 1(L)â”‚ â”‚  â”‚ â”‚Part 2(L)â”‚ â”‚        â”‚â”‚
â”‚  â”‚  â”‚ â”‚Part 1(F)â”‚ â”‚  â”‚ â”‚Part 2(F)â”‚ â”‚  â”‚ â”‚Part 0(F)â”‚ â”‚        â”‚â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

### **ğŸ“ Confluent Schema Registry Integration**

#### **Producer Ñ Schema Registry**
```java
// AvroProducerExample.java
import org.apache.kafka.clients.producer.*;
import io.confluent.kafka.serializers.KafkaAvroSerializer;
import org.apache.avro.Schema;
import org.apache.avro.generic.GenericData;
import org.apache.avro.generic.GenericRecord;

public class AvroProducerExample {
    
    public static void main(String[] args) throws Exception {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);
        props.put("schema.registry.url", "http://localhost:8081");
        
        // Define Avro schema
        String schemaString = """
            {
              "type": "record",
              "name": "User",
              "fields": [
                {"name": "id", "type": "long"},
                {"name": "name", "type": "string"},
                {"name": "email", "type": "string"}
              ]
            }
        """;
        
        Schema schema = new Schema.Parser().parse(schemaString);
        
        try (KafkaProducer<String, GenericRecord> producer = new KafkaProducer<>(props)) {
            
            for (int i = 0; i < 100; i++) {
                // Create Avro record
                GenericRecord user = new GenericData.Record(schema);
                user.put("id", (long) i);
                user.put("name", "User " + i);
                user.put("email", "user" + i + "@example.com");
                
                ProducerRecord<String, GenericRecord> record = 
                    new ProducerRecord<>("user-topic", "user-" + i, user);
                
                producer.send(record, (metadata, exception) -> {
                    if (exception != null) {
                        System.err.println("Error: " + exception.getMessage());
                    } else {
                        System.out.println("Sent user " + i + " to " + 
                            metadata.topic() + ":" + metadata.partition() + 
                            " at offset " + metadata.offset());
                    }
                });
            }
        }
    }
}
```

#### **Consumer Ñ Schema Registry**
```java
// AvroConsumerExample.java
import org.apache.kafka.clients.consumer.*;
import io.confluent.kafka.serializers.KafkaAvroDeserializer;
import org.apache.avro.generic.GenericRecord;
import java.time.Duration;
import java.util.Collections;

public class AvroConsumerExample {
    
    public static void main(String[] args) {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "avro-consumer-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class);
        props.put("schema.registry.url", "http://localhost:8081");
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        
        try (KafkaConsumer<String, GenericRecord> consumer = new KafkaConsumer<>(props)) {
            consumer.subscribe(Collections.singletonList("user-topic"));
            
            while (true) {
                ConsumerRecords<String, GenericRecord> records = 
                    consumer.poll(Duration.ofMillis(1000));
                
                for (ConsumerRecord<String, GenericRecord> record : records) {
                    GenericRecord user = record.value();
                    
                    System.out.println("Received user: " +
                        "id=" + user.get("id") +
                        ", name=" + user.get("name") +
                        ", email=" + user.get("email"));
                }
            }
        }
    }
}
```

---

### **ğŸ›¡ï¸ Protocol Buffers Implementation**

#### **protobuf Schema Definition**
```protobuf
// user.proto
syntax = "proto3";

package com.company.user;

option java_package = "com.company.user";
option java_outer_classname = "UserProtos";

message User {
  int64 id = 1;
  string name = 2;
  string email = 3;
  string phone = 4;
  int64 created_at = 5;
  
  enum Status {
    UNKNOWN = 0;
    ACTIVE = 1;
    INACTIVE = 2;
    SUSPENDED = 3;
  }
  
  Status status = 6;
  
  message Address {
    string street = 1;
    string city = 2;
    string country = 3;
    string postal_code = 4;
  }
  
  repeated Address addresses = 7;
}
```

#### **Protocol Buffers Producer**
```java
// ProtobufProducerExample.java
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.ByteArraySerializer;
import org.apache.kafka.common.serialization.StringSerializer;
import com.company.user.UserProtos.User;

public class ProtobufProducerExample {
    
    public static void main(String[] args) throws Exception {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
        
        try (KafkaProducer<String, byte[]> producer = new KafkaProducer<>(props)) {
            
            for (int i = 0; i < 100; i++) {
                // Build protobuf message
                User user = User.newBuilder()
                    .setId(i)
                    .setName("User " + i)
                    .setEmail("user" + i + "@example.com")
                    .setPhone("+1-555-000-" + String.format("%04d", i))
                    .setCreatedAt(System.currentTimeMillis())
                    .setStatus(User.Status.ACTIVE)
                    .addAddresses(User.Address.newBuilder()
                        .setStreet(i + " Main St")
                        .setCity("Anytown")
                        .setCountry("US")
                        .setPostalCode(String.format("%05d", 10000 + i)))
                    .build();
                
                // Serialize to bytes
                byte[] userBytes = user.toByteArray();
                
                ProducerRecord<String, byte[]> record = 
                    new ProducerRecord<>("user-protobuf-topic", "user-" + i, userBytes);
                
                producer.send(record);
                System.out.println("Sent protobuf user " + i + " (" + userBytes.length + " bytes)");
            }
        }
    }
}
```

---

### **âš¡ Performance Comparison**

#### **Serialization Benchmark**
```java
// SerializationBenchmark.java
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.avro.generic.GenericRecord;
import org.apache.avro.io.*;
import org.apache.avro.specific.SpecificDatumWriter;
import org.apache.avro.specific.SpecificDatumReader;
import com.company.user.UserProtos.User;

public class SerializationBenchmark {
    
    private static final int ITERATIONS = 100_000;
    private static final ObjectMapper JSON_MAPPER = new ObjectMapper();
    
    public static void main(String[] args) throws Exception {
        System.out.println("=== SERIALIZATION BENCHMARK ===");
        System.out.println("Iterations: " + ITERATIONS);
        System.out.println();
        
        // Test data
        UserData userData = createTestUser();
        
        // JSON Benchmark
        long jsonStart = System.currentTimeMillis();
        int jsonSize = 0;
        for (int i = 0; i < ITERATIONS; i++) {
            byte[] jsonBytes = JSON_MAPPER.writeValueAsBytes(userData);
            jsonSize = jsonBytes.length;
        }
        long jsonTime = System.currentTimeMillis() - jsonStart;
        
        // Avro Benchmark
        long avroStart = System.currentTimeMillis();
        int avroSize = 0;
        for (int i = 0; i < ITERATIONS; i++) {
            byte[] avroBytes = serializeAvro(userData);
            avroSize = avroBytes.length;
        }
        long avroTime = System.currentTimeMillis() - avroStart;
        
        // Protocol Buffers Benchmark
        long protobufStart = System.currentTimeMillis();
        int protobufSize = 0;
        for (int i = 0; i < ITERATIONS; i++) {
            byte[] protobufBytes = serializeProtobuf(userData);
            protobufSize = protobufBytes.length;
        }
        long protobufTime = System.currentTimeMillis() - protobufStart;
        
        // Results
        printResults("JSON", jsonTime, jsonSize);
        printResults("Avro", avroTime, avroSize);
        printResults("Protobuf", protobufTime, protobufSize);
        
        // Compression ratios
        System.out.println("\n=== COMPRESSION RATIOS ===");
        System.out.printf("Avro vs JSON: %.1fx smaller\n", (double)jsonSize / avroSize);
        System.out.printf("Protobuf vs JSON: %.1fx smaller\n", (double)jsonSize / protobufSize);
    }
    
    private static void printResults(String format, long time, int size) {
        double throughput = (double) ITERATIONS / time * 1000;
        System.out.printf("%-10s: %4dms, %3d bytes, %8.0f ops/sec\n", 
            format, time, size, throughput);
    }
}
```

#### **Benchmark Results Example**
```
=== SERIALIZATION BENCHMARK ===
Iterations: 100,000

JSON      :  245ms, 151 bytes,   408,163 ops/sec
Avro      :   89ms,  42 bytes, 1,123,596 ops/sec  
Protobuf  :   67ms,  38 bytes, 1,492,537 ops/sec

=== COMPRESSION RATIOS ===
Avro vs JSON: 3.6x smaller
Protobuf vs JSON: 4.0x smaller

=== THROUGHPUT COMPARISON ===
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JSON:     â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 27% (408K ops/sec)           â”‚
â”‚ Avro:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ 75% (1.1M ops/sec)           â”‚
â”‚ Protobuf: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 100% (1.5M ops/sec)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ›¡ï¸ **Ğ“Ğ›ĞĞ’Ğ 3.5: ĞĞ‘Ğ ĞĞ‘ĞĞ¢ĞšĞ ĞĞ¨Ğ˜Ğ‘ĞĞš Ğ˜ ĞĞĞ”Ğ•Ğ–ĞĞĞ¡Ğ¢Ğ¬**

### **ğŸ”„ Retry Policies Ğ¸ Exponential Backoff**

#### **Retry Strategy Implementation**
```java
// RetryableKafkaProducer.java
import org.apache.kafka.clients.producer.*;
import java.util.concurrent.ThreadLocalRandom;
import java.util.concurrent.TimeUnit;

public class RetryableKafkaProducer {
    private final KafkaProducer<String, String> producer;
    private final RetryPolicy retryPolicy;
    
    public RetryableKafkaProducer(Properties props, RetryPolicy retryPolicy) {
        this.producer = new KafkaProducer<>(props);
        this.retryPolicy = retryPolicy;
    }
    
    public void sendWithRetry(String topic, String key, String value) {
        ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
        
        int attempt = 0;
        Exception lastException = null;
        
        while (attempt < retryPolicy.getMaxRetries()) {
            try {
                RecordMetadata metadata = producer.send(record).get(
                    retryPolicy.getTimeoutMs(), TimeUnit.MILLISECONDS);
                
                System.out.println("Message sent successfully on attempt " + (attempt + 1) +
                    ": topic=" + metadata.topic() + 
                    ", partition=" + metadata.partition() + 
                    ", offset=" + metadata.offset());
                return;
                
            } catch (Exception e) {
                lastException = e;
                attempt++;
                
                if (attempt < retryPolicy.getMaxRetries()) {
                    long delayMs = calculateDelay(attempt);
                    System.err.println("Attempt " + attempt + " failed, retrying in " + 
                        delayMs + "ms: " + e.getMessage());
                    
                    try {
                        Thread.sleep(delayMs);
                    } catch (InterruptedException ie) {
                        Thread.currentThread().interrupt();
                        throw new RuntimeException("Interrupted during retry", ie);
                    }
                }
            }
        }
        
        throw new RuntimeException("Failed to send message after " + 
            retryPolicy.getMaxRetries() + " attempts", lastException);
    }
    
    private long calculateDelay(int attempt) {
        switch (retryPolicy.getBackoffStrategy()) {
            case FIXED:
                return retryPolicy.getBaseDelayMs();
                
            case LINEAR:
                return retryPolicy.getBaseDelayMs() * attempt;
                
            case EXPONENTIAL:
                long delay = retryPolicy.getBaseDelayMs() * (1L << (attempt - 1));
                return Math.min(delay, retryPolicy.getMaxDelayMs());
                
            case EXPONENTIAL_JITTER:
                long exponentialDelay = retryPolicy.getBaseDelayMs() * (1L << (attempt - 1));
                exponentialDelay = Math.min(exponentialDelay, retryPolicy.getMaxDelayMs());
                
                // Add jitter (Â±25%)
                double jitterFactor = 0.75 + (ThreadLocalRandom.current().nextDouble() * 0.5);
                return (long) (exponentialDelay * jitterFactor);
                
            default:
                return retryPolicy.getBaseDelayMs();
        }
    }
}

// RetryPolicy.java
public class RetryPolicy {
    public enum BackoffStrategy {
        FIXED, LINEAR, EXPONENTIAL, EXPONENTIAL_JITTER
    }
    
    private final int maxRetries;
    private final long baseDelayMs;
    private final long maxDelayMs;
    private final long timeoutMs;
    private final BackoffStrategy backoffStrategy;
    
    public RetryPolicy(int maxRetries, long baseDelayMs, long maxDelayMs, 
                      long timeoutMs, BackoffStrategy backoffStrategy) {
        this.maxRetries = maxRetries;
        this.baseDelayMs = baseDelayMs;
        this.maxDelayMs = maxDelayMs;
        this.timeoutMs = timeoutMs;
        this.backoffStrategy = backoffStrategy;
    }
    
    // Static factory methods
    public static RetryPolicy exponentialBackoff() {
        return new RetryPolicy(5, 1000, 30000, 10000, BackoffStrategy.EXPONENTIAL_JITTER);
    }
    
    public static RetryPolicy fixedDelay() {
        return new RetryPolicy(3, 2000, 2000, 5000, BackoffStrategy.FIXED);
    }
    
    // Getters...
}
```

#### **Backoff Strategies Visualization**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKOFF STRATEGIES                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  FIXED DELAY:                                                   â”‚
â”‚  Delay â”‚                                                        â”‚
â”‚   (ms) â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              â”‚
â”‚   2000 â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              â”‚
â”‚   1000 â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              â”‚
â”‚      0 â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â–¶ Attempt            â”‚
â”‚          1     2     3     4     5     6                       â”‚
â”‚                                                                 â”‚
â”‚  EXPONENTIAL BACKOFF:                                           â”‚
â”‚  Delay â”‚                                                        â”‚
â”‚   (ms) â”‚                               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â”‚
â”‚   8000 â”‚                               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â”‚
â”‚   4000 â”‚                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â”‚
â”‚   2000 â”‚           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â”‚
â”‚   1000 â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  â”‚
â”‚      0 â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â–¶ Attempt            â”‚
â”‚          1     2     3     4     5     6                       â”‚
â”‚                                                                 â”‚
â”‚  EXPONENTIAL WITH JITTER:                                       â”‚
â”‚  Delay â”‚                                                        â”‚
â”‚   (ms) â”‚                           â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆ      â”‚
â”‚   8000 â”‚                           â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆ      â”‚
â”‚   4000 â”‚                 â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆ    â”‚
â”‚   2000 â”‚       â–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆ    â”‚
â”‚   1000 â”‚ â–‘â–‘â–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  â”‚
â”‚      0 â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â–¶ Attempt            â”‚
â”‚          1     2     3     4     5     6                       â”‚
â”‚                                                                 â”‚
â”‚  â–‘ = Jitter variation (Â±25%)                                   â”‚
â”‚  â–ˆ = Base exponential delay                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **âš¡ Circuit Breaker Pattern**

#### **Circuit Breaker Implementation**
```java
// CircuitBreaker.java
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.atomic.AtomicReference;

public class CircuitBreaker {
    
    public enum State {
        CLOSED,     // Normal operation
        OPEN,       // Failing fast
        HALF_OPEN   // Testing if service recovered
    }
    
    private final int failureThreshold;
    private final int successThreshold;
    private final long timeoutMs;
    private final AtomicInteger failureCount = new AtomicInteger(0);
    private final AtomicInteger successCount = new AtomicInteger(0);
    private final AtomicLong lastFailureTime = new AtomicLong(0);
    private final AtomicReference<State> state = new AtomicReference<>(State.CLOSED);
    
    public CircuitBreaker(int failureThreshold, int successThreshold, long timeoutMs) {
        this.failureThreshold = failureThreshold;
        this.successThreshold = successThreshold;
        this.timeoutMs = timeoutMs;
    }
    
    public boolean allowRequest() {
        State currentState = state.get();
        
        switch (currentState) {
            case CLOSED:
                return true;
                
            case OPEN:
                if (System.currentTimeMillis() - lastFailureTime.get() > timeoutMs) {
                    // Try to transition to HALF_OPEN
                    if (state.compareAndSet(State.OPEN, State.HALF_OPEN)) {
                        successCount.set(0);
                        return true;
                    }
                }
                return false;
                
            case HALF_OPEN:
                return successCount.get() < successThreshold;
                
            default:
                return false;
        }
    }
    
    public void recordSuccess() {
        State currentState = state.get();
        
        if (currentState == State.HALF_OPEN) {
            int successes = successCount.incrementAndGet();
            if (successes >= successThreshold) {
                // Service recovered, transition to CLOSED
                state.set(State.CLOSED);
                failureCount.set(0);
                System.out.println("Circuit breaker: HALF_OPEN -> CLOSED (service recovered)");
            }
        } else if (currentState == State.CLOSED) {
            // Reset failure count on success
            failureCount.set(0);
        }
    }
    
    public void recordFailure() {
        lastFailureTime.set(System.currentTimeMillis());
        
        State currentState = state.get();
        
        if (currentState == State.HALF_OPEN) {
            // Failed during testing, go back to OPEN
            state.set(State.OPEN);
            System.out.println("Circuit breaker: HALF_OPEN -> OPEN (test failed)");
        } else if (currentState == State.CLOSED) {
            int failures = failureCount.incrementAndGet();
            if (failures >= failureThreshold) {
                // Too many failures, open circuit
                state.set(State.OPEN);
                System.out.println("Circuit breaker: CLOSED -> OPEN (failure threshold reached)");
            }
        }
    }
    
    public State getState() {
        return state.get();
    }
    
    public int getFailureCount() {
        return failureCount.get();
    }
}
```

#### **Circuit Breaker Ğ² Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ğ¸**
```java
// CircuitBreakerKafkaProducer.java
public class CircuitBreakerKafkaProducer {
    private final KafkaProducer<String, String> producer;
    private final CircuitBreaker circuitBreaker;
    
    public CircuitBreakerKafkaProducer(Properties props) {
        this.producer = new KafkaProducer<>(props);
        this.circuitBreaker = new CircuitBreaker(5, 2, 60000); // 5 failures, 2 successes, 60s timeout
    }
    
    public void send(String topic, String key, String value) throws Exception {
        if (!circuitBreaker.allowRequest()) {
            throw new RuntimeException("Circuit breaker is OPEN - failing fast");
        }
        
        try {
            RecordMetadata metadata = producer.send(
                new ProducerRecord<>(topic, key, value)).get();
            
            circuitBreaker.recordSuccess();
            System.out.println("Message sent successfully: " + metadata);
            
        } catch (Exception e) {
            circuitBreaker.recordFailure();
            System.err.println("Failed to send message: " + e.getMessage());
            throw e;
        }
    }
}
```

#### **Circuit Breaker State Transitions**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                CIRCUIT BREAKER STATE MACHINE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   CLOSED    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚         â”‚         â”‚ (Normal)    â”‚           â”‚                   â”‚
â”‚         â”‚         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚                   â”‚
â”‚         â”‚               â”‚                   â”‚                   â”‚
â”‚         â”‚     failures â‰¥â”‚ threshold         â”‚ successes â‰¥      â”‚
â”‚         â”‚     threshold â”‚                   â”‚ threshold         â”‚
â”‚         â”‚               â–¼                   â”‚                   â”‚
â”‚         â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚                   â”‚
â”‚  timeoutâ”‚         â”‚    OPEN     â”‚           â”‚                   â”‚
â”‚  elapsedâ”‚         â”‚ (Fail Fast) â”‚           â”‚                   â”‚
â”‚         â”‚         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚                   â”‚
â”‚         â”‚               â”‚                   â”‚                   â”‚
â”‚         â”‚     timeout   â”‚ elapsed           â”‚                   â”‚
â”‚         â”‚     elapsed   â”‚                   â”‚                   â”‚
â”‚         â”‚               â–¼                   â”‚                   â”‚
â”‚         â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚                   â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  HALF_OPEN  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚          failure  â”‚ (Testing)   â”‚ success                       â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                                                                 â”‚
â”‚  Behavior in each state:                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ CLOSED:    Allow all requests, count failures              â”‚â”‚
â”‚  â”‚ OPEN:      Reject all requests (fail fast)                 â”‚â”‚
â”‚  â”‚ HALF_OPEN: Allow limited requests to test service health   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **ğŸ” Message Deduplication**

#### **Idempotent Consumer Pattern**
```java
// IdempotentConsumer.java
import java.util.concurrent.ConcurrentHashMap;
import java.util.Set;
import java.util.concurrent.ConcurrentSkipListSet;

public class IdempotentConsumer {
    private final Set<String> processedMessages = new ConcurrentSkipListSet<>();
    private final KafkaConsumer<String, String> consumer;
    
    // Cleanup old message IDs periodically
    private final long MESSAGE_ID_TTL_MS = 3600000; // 1 hour
    private final ConcurrentHashMap<String, Long> messageTimestamps = new ConcurrentHashMap<>();
    
    public IdempotentConsumer(Properties props) {
        this.consumer = new KafkaConsumer<>(props);
        
        // Start cleanup thread
        startCleanupTask();
    }
    
    public void consume(List<String> topics) {
        consumer.subscribe(topics);
        
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
            
            for (ConsumerRecord<String, String> record : records) {
                String messageId = extractMessageId(record);
                
                if (messageId != null && isAlreadyProcessed(messageId)) {
                    System.out.println("Skipping duplicate message: " + messageId);
                    continue;
                }
                
                try {
                    processMessage(record);
                    
                    if (messageId != null) {
                        markAsProcessed(messageId);
                    }
                    
                } catch (Exception e) {
                    System.err.println("Failed to process message: " + e.getMessage());
                    // Don't mark as processed if failed
                }
            }
            
            consumer.commitSync();
        }
    }
    
    private String extractMessageId(ConsumerRecord<String, String> record) {
        // Try to extract message ID from headers
        Header messageIdHeader = record.headers().lastHeader("message-id");
        if (messageIdHeader != null) {
            return new String(messageIdHeader.value());
        }
        
        // Fallback: use key + offset as unique ID
        return record.key() + "-" + record.partition() + "-" + record.offset();
    }
    
    private boolean isAlreadyProcessed(String messageId) {
        return processedMessages.contains(messageId);
    }
    
    private void markAsProcessed(String messageId) {
        processedMessages.add(messageId);
        messageTimestamps.put(messageId, System.currentTimeMillis());
    }
    
    private void processMessage(ConsumerRecord<String, String> record) {
        // Simulate message processing
        System.out.println("Processing message: " + record.value());
        
        // Your business logic here
        // This should be idempotent by nature
    }
    
    private void startCleanupTask() {
        Thread cleanupThread = new Thread(() -> {
            while (true) {
                try {
                    Thread.sleep(300000); // Cleanup every 5 minutes
                    cleanupOldMessageIds();
                } catch (InterruptedException e) {
                    Thread.currentThread().interrupt();
                    break;
                }
            }
        });
        
        cleanupThread.setDaemon(true);
        cleanupThread.start();
    }
    
    private void cleanupOldMessageIds() {
        long now = System.currentTimeMillis();
        int removedCount = 0;
        
        for (Map.Entry<String, Long> entry : messageTimestamps.entrySet()) {
            if (now - entry.getValue() > MESSAGE_ID_TTL_MS) {
                String messageId = entry.getKey();
                processedMessages.remove(messageId);
                messageTimestamps.remove(messageId);
                removedCount++;
            }
        }
        
        if (removedCount > 0) {
            System.out.println("Cleaned up " + removedCount + " old message IDs");
        }
    }
}
```

---

### **ğŸ’¾ Transactional Messaging**

#### **Kafka Transactions Example**
```java
// TransactionalKafkaExample.java
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.TopicPartition;
import java.util.Map;
import java.util.HashMap;

public class TransactionalKafkaExample {
    
    public static class TransactionalProducer {
        private final KafkaProducer<String, String> producer;
        
        public TransactionalProducer(String transactionalId) {
            Properties props = new Properties();
            props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
            props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
            props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
            
            // Transactional settings
            props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, transactionalId);
            props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);
            props.put(ProducerConfig.ACKS_CONFIG, "all");
            props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
            props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1);
            
            this.producer = new KafkaProducer<>(props);
            this.producer.initTransactions();
        }
        
        public void processAndForward(ConsumerRecords<String, String> records, 
                                     Map<TopicPartition, OffsetAndMetadata> offsets) {
            try {
                producer.beginTransaction();
                
                // Process messages and send to output topic
                for (ConsumerRecord<String, String> record : records) {
                    String processedValue = processMessage(record.value());
                    
                    ProducerRecord<String, String> outputRecord = 
                        new ProducerRecord<>("processed-orders", record.key(), processedValue);
                    
                    producer.send(outputRecord);
                }
                
                // Commit consumer offsets as part of transaction
                producer.sendOffsetsToTransaction(offsets, "order-processor-group");
                
                // Commit transaction
                producer.commitTransaction();
                System.out.println("Transaction committed successfully");
                
            } catch (Exception e) {
                System.err.println("Transaction failed, aborting: " + e.getMessage());
                producer.abortTransaction();
                throw new RuntimeException(e);
            }
        }
        
        private String processMessage(String input) {
            // Simulate message processing
            return "PROCESSED: " + input;
        }
        
        public void close() {
            producer.close();
        }
    }
    
    public static class TransactionalConsumer {
        private final KafkaConsumer<String, String> consumer;
        
        public TransactionalConsumer() {
            Properties props = new Properties();
            props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
            props.put(ConsumerConfig.GROUP_ID_CONFIG, "order-processor-group");
            props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
            props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
            
            // Read only committed messages
            props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
            props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
            
            this.consumer = new KafkaConsumer<>(props);
        }
        
        public void consumeAndProcess() {
            consumer.subscribe(Collections.singletonList("orders"));
            TransactionalProducer producer = new TransactionalProducer("order-processor");
            
            try {
                while (true) {
                    ConsumerRecords<String, String> records = 
                        consumer.poll(Duration.ofMillis(1000));
                    
                    if (!records.isEmpty()) {
                        // Prepare offset commit map
                        Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
                        for (ConsumerRecord<String, String> record : records) {
                            TopicPartition partition = 
                                new TopicPartition(record.topic(), record.partition());
                            offsets.put(partition, 
                                new OffsetAndMetadata(record.offset() + 1));
                        }
                        
                        // Process in transaction
                        producer.processAndForward(records, offsets);
                    }
                }
            } finally {
                producer.close();
                consumer.close();
            }
        }
    }
}
```

#### **Transaction Flow Diagram**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA TRANSACTIONS FLOW                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. BEGIN TRANSACTION:                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Producer â”€â”€beginTransaction()â”€â”€â–¶ Transaction Coordinator    â”‚â”‚
â”‚  â”‚                                        â”‚                    â”‚â”‚
â”‚  â”‚                                        â–¼                    â”‚â”‚
â”‚  â”‚                                 Assign Transaction ID       â”‚â”‚
â”‚  â”‚                                 Create Transaction Log      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  2. SEND MESSAGES:                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Producer â”€â”€send(record)â”€â”€â–¶ Topic Partition                  â”‚â”‚
â”‚  â”‚                                  â”‚                          â”‚â”‚
â”‚  â”‚                                  â–¼                          â”‚â”‚
â”‚  â”‚                           Mark as part of TX               â”‚â”‚
â”‚  â”‚                           (not visible to consumers yet)    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  3. COMMIT OFFSETS (optional):                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Producer â”€â”€sendOffsetsToTransaction()â”€â”€â–¶ __consumer_offsets â”‚â”‚
â”‚  â”‚                                               â”‚             â”‚â”‚
â”‚  â”‚                                               â–¼             â”‚â”‚
â”‚  â”‚                                        Mark as part of TX   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  4. COMMIT TRANSACTION:                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Producer â”€â”€commitTransaction()â”€â”€â–¶ Transaction Coordinator   â”‚â”‚
â”‚  â”‚                                        â”‚                    â”‚â”‚
â”‚  â”‚                                        â–¼                    â”‚â”‚
â”‚  â”‚                                 Mark TX as committed        â”‚â”‚
â”‚  â”‚                                        â”‚                    â”‚â”‚
â”‚  â”‚                                        â–¼                    â”‚â”‚
â”‚  â”‚                                 Messages become visible     â”‚â”‚
â”‚  â”‚                                 to read_committed consumers â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  ABORT SCENARIO:                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Producer â”€â”€abortTransaction()â”€â”€â–¶ Transaction Coordinator    â”‚â”‚
â”‚  â”‚                                        â”‚                    â”‚â”‚
â”‚  â”‚                                        â–¼                    â”‚â”‚
â”‚  â”‚                                 Mark TX as aborted          â”‚â”‚
â”‚  â”‚                                        â”‚                    â”‚â”‚
â”‚  â”‚                                        â–¼                    â”‚â”‚
â”‚  â”‚                                 Messages are discarded      â”‚â”‚
â”‚  â”‚                                 (never visible to consumers)â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ **Ğ˜Ğ¢ĞĞ“Ğ˜ Ğ‘Ğ›ĞĞšĞ 3**

### **âœ… Ğ§Ñ‚Ğ¾ Ğ²Ñ‹ Ğ¾ÑĞ²Ğ¾Ğ¸Ğ»Ğ¸:**

1. **Production Kafka Setup** â€” Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° enterprise-ready ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
2. **Advanced Producer/Consumer** â€” Ğ²Ñ‹ÑĞ¾ĞºĞ¾Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ
3. **RabbitMQ Advanced Features** â€” ÑĞ»Ğ¾Ğ¶Ğ½Ğ°Ñ Ğ¼Ğ°Ñ€ÑˆÑ€ÑƒÑ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¸ DLX
4. **Serialization Formats** â€” Avro, Protocol Buffers, Schema Registry
5. **Error Handling & Reliability** â€” retry, circuit breaker, idempotency, transactions

### **ğŸ› ï¸ ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ½Ğ°Ğ²Ñ‹ĞºĞ¸:**

- ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° production-ready Ğ¸Ğ½Ñ„Ñ€Ğ°ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñ‹
- ĞĞ°Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ fault-tolerant Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğ¹
- ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
- ĞĞ±ĞµÑĞ¿ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ½Ğ°Ğ´ĞµĞ¶Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ¸ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹
- Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ñ schema evolution

### **ğŸ“Š Performance Best Practices:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PERFORMANCE SUMMARY                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  KAFKA PRODUCER TUNING:                                         â”‚
â”‚  âœ… batch.size = 65536 (64KB)                                  â”‚
â”‚  âœ… linger.ms = 10                                             â”‚
â”‚  âœ… compression.type = lz4                                     â”‚
â”‚  âœ… acks = all (for reliability)                               â”‚
â”‚                                                                 â”‚
â”‚  KAFKA CONSUMER TUNING:                                         â”‚
â”‚  âœ… fetch.min.bytes = 1024                                     â”‚
â”‚  âœ… max.poll.records = 1000                                    â”‚
â”‚  âœ… Parallel processing with thread pools                      â”‚
â”‚  âœ… Manual offset commits                                       â”‚
â”‚                                                                 â”‚
â”‚  SERIALIZATION:                                                 â”‚
â”‚  âœ… Use Avro/Protobuf for production                           â”‚
â”‚  âœ… Schema Registry for schema evolution                       â”‚
â”‚  âœ… Avoid JSON for high-throughput scenarios                   â”‚
â”‚                                                                 â”‚
â”‚  RELIABILITY:                                                   â”‚
â”‚  âœ… Implement retry with exponential backoff                   â”‚
â”‚  âœ… Use circuit breakers for external dependencies             â”‚
â”‚  âœ… Ensure idempotent message processing                       â”‚
â”‚  âœ… Use transactions for exactly-once semantics                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **ğŸš€ Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ ÑˆĞ°Ğ³Ğ¸:**

ĞŸĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ñ‚Ğµ Ğº **Ğ‘Ğ»Ğ¾ĞºÑƒ 4: ĞŸÑ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°**, Ğ³Ğ´Ğµ Ğ²Ñ‹ Ğ¸Ğ·ÑƒÑ‡Ğ¸Ñ‚Ğµ Enterprise Integration Patterns, Event Sourcing, CQRS Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ distributed ÑĞ¸ÑÑ‚ĞµĞ¼!

---

## ğŸ“š **ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°ĞºÑ€ĞµĞ¿Ğ»ĞµĞ½Ğ¸Ñ**

### **ğŸ¯ Ğ—Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ 1: High-Throughput Producer**
Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹Ñ‚Ğµ Kafka producer, ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑÑ‚ÑŒ 100K+ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹ Ğ² ÑĞµĞºÑƒĞ½Ğ´Ñƒ Ñ Ğ³Ğ°Ñ€Ğ°Ğ½Ñ‚Ğ¸ĞµĞ¹ Ğ´Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ¸.

### **ğŸ¯ Ğ—Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ 2: Fault-Tolerant Consumer**
Ğ ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞ¹Ñ‚Ğµ consumer Ñ circuit breaker, retry Ğ»Ğ¾Ğ³Ğ¸ĞºĞ¾Ğ¹ Ğ¸ idempotent processing.

### **ğŸ¯ Ğ—Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ 3: Schema Evolution**
Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ¹Ñ‚Ğµ Avro schema Ğ¸ Ğ¿Ñ€Ğ¾Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ ÑĞ²Ğ¾Ğ»ÑÑ†Ğ¸Ğ¹, Ñ‚ĞµÑÑ‚Ğ¸Ñ€ÑƒÑ backward/forward compatibility.

### **ğŸ¯ Ğ—Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ 4: Transaction Processing**
Ğ ĞµĞ°Ğ»Ğ¸Ğ·ÑƒĞ¹Ñ‚Ğµ exactly-once processing Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ´Ğ²ÑƒĞ¼Ñ Kafka Ñ‚Ğ¾Ğ¿Ğ¸ĞºĞ°Ğ¼Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ğ¸Ğ¹.

---

**Ğ’Ñ€ĞµĞ¼Ñ Ğ½Ğ° Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ğ»Ğ¾ĞºĞ°: 5-6 Ğ½ĞµĞ´ĞµĞ»ÑŒ**  
**Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ğ±Ğ»Ğ¾Ğº: ĞŸÑ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°** ğŸ—ï¸
     â”‚                       â”‚                       â”‚
     â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Client   â”‚          â”‚ Client   â”‚          â”‚ Client   â”‚
â”‚Apps/APIs â”‚          â”‚Apps/APIs â”‚          â”‚Apps/APIs â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

L = Leader, F = Follower
```

---

### **âš™ï¸ Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ°Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Kafka**

#### **server.properties - Production Configuration**
```properties
############################ Server Basics ############################
broker.id=1
listeners=PLAINTEXT://0.0.0.0:9092
advertised.listeners=PLAINTEXT://kafka-1.internal:9092
num.network.threads=8
num.io.threads=16
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600

############################ Log Basics ############################
log.dirs=/var/kafka-logs
num.partitions=6
default.replication.factor=3
min.insync.replicas=2
log.retention.hours=168
log.retention.bytes=1073741824
log.segment.bytes=1073741824
log.cleanup.policy=delete

############################ Zookeeper ############################
zookeeper.connect=zk-1.internal:2181,zk-2.internal:2181,zk-3.internal:2181
zookeeper.connection.timeout.ms=6000

############################ Performance Tuning ############################
# Producer config
compression.type=lz4
batch.size=16384
linger.ms=5
buffer.memory=33554432

# Consumer config
fetch.min.bytes=1
fetch.max.wait.ms=500
max.partition.fetch.bytes=1048576

# Replication
replica.fetch.max.bytes=1048576
replica.socket.timeout.ms=30000
replica.socket.receive.buffer.bytes=65536
```

#### **JVM Ğ¸ OS Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸**
```bash
#!/bin/bash
# kafka-server-start-tuned.sh

# JVM Heap Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ 6GB Ğ´Ğ»Ñ production)
export KAFKA_HEAP_OPTS="-Xmx6g -Xms6g"

# GC Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ»Ñ Ğ½Ğ¸Ğ·ĞºĞ¾Ğ¹ latency
export KAFKA_JVM_PERFORMANCE_OPTS="-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 \
    -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent \
    -XX:MaxInlineLevel=15 -Djava.awt.headless=true"

# OS level Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸
echo 'vm.swappiness=1' >> /etc/sysctl.conf
echo 'vm.dirty_ratio=80' >> /etc/sysctl.conf
echo 'vm.dirty_background_ratio=5' >> /etc/sysctl.conf
echo 'net.core.rmem_default = 262144' >> /etc/sysctl.conf
echo 'net.core.rmem_max = 16777216' >> /etc/sysctl.conf

# File descriptor limits
echo 'kafka soft nofile 100000' >> /etc/security/limits.conf
echo 'kafka hard nofile 100000' >> /etc/security/limits.conf

# Start Kafka
kafka-server-start.sh /opt/kafka/config/server.properties
```

---

### **ğŸ“Š Kafka Manager Ğ¸ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³**

#### **Prometheus Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ**
```yaml
# prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'kafka'
    static_configs:
      - targets: ['kafka-1:9308', 'kafka-2:9308', 'kafka-3:9308']
    metrics_path: /metrics
    scrape_interval: 10s
    
  - job_name: 'kafka-jmx'
    static_configs:
      - targets: ['kafka-1:9999', 'kafka-2:9999', 'kafka-3:9999']
    metrics_path: /metrics
    scrape_interval: 30s
```

#### **Key Performance Indicators (KPIs)**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      KAFKA METRICS DASHBOARD                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  THROUGHPUT METRICS:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ â€¢ Messages in/sec:  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 85K msg/s                â”‚â”‚
â”‚  â”‚ â€¢ Bytes in/sec:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 120 MB/s                 â”‚â”‚
â”‚  â”‚ â€¢ Bytes out/sec:    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘ 200 MB/s                 â”‚â”‚
â”‚  â”‚ â€¢ Requests/sec:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘ 12K req/s                â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  LATENCY METRICS:                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ â€¢ Producer latency: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 5ms (p99)                 â”‚â”‚
â”‚  â”‚ â€¢ Consumer lag:     â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 100ms (max)               â”‚â”‚
â”‚  â”‚ â€¢ Request latency:  â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 3ms (avg)                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  RESOURCE METRICS:                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ â€¢ CPU Usage:        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘ 70%                       â”‚â”‚
â”‚  â”‚ â€¢ Memory Usage:     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘ 60%                       â”‚â”‚
â”‚  â”‚ â€¢ Disk Usage:       â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 30%                       â”‚â”‚
â”‚  â”‚ â€¢ Network I/O:      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 80%                       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ°Ğ»ĞµÑ€Ñ‚Ñ‹**
```yaml
# alerts.yml
groups:
  - name: kafka-alerts
    rules:
    - alert: KafkaConsumerLag
      expr: kafka_consumer_lag_max > 1000
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High consumer lag detected"
        
    - alert: KafkaBrokerDown
      expr: up{job="kafka"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Kafka broker is down"
        
    - alert: KafkaUnderReplicatedPartitions
      expr: kafka_server_replicamanager_underreplicatedpartitions > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Under-replicated partitions detected"
```

---

### **ğŸ—‚ï¸ Schema Registry**

#### **Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Schema Registry?**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCHEMA REGISTRY ARCHITECTURE                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              CONFLUENT SCHEMA REGISTRY                     â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚â”‚
â”‚  â”‚  â”‚ Schema v1   â”‚  â”‚ Schema v2   â”‚  â”‚ Schema v3   â”‚        â”‚â”‚
â”‚  â”‚  â”‚ (user.avsc) â”‚  â”‚ (user.avsc) â”‚  â”‚ (user.avsc) â”‚        â”‚â”‚
â”‚  â”‚  â”‚             â”‚  â”‚ + phone     â”‚  â”‚ + address   â”‚        â”‚â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚        â”‚â”‚
â”‚  â”‚  â”‚ â”‚id: 1    â”‚ â”‚  â”‚ â”‚id: 2    â”‚ â”‚  â”‚ â”‚id: 3    â”‚ â”‚        â”‚â”‚
â”‚  â”‚  â”‚ â”‚name     â”‚ â”‚  â”‚ â”‚name     â”‚ â”‚  â”‚ â”‚name     â”‚ â”‚        â”‚â”‚
â”‚  â”‚  â”‚ â”‚email    â”‚ â”‚  â”‚ â”‚email    â”‚ â”‚  â”‚ â”‚email    â”‚ â”‚        â”‚â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â”‚phone    â”‚ â”‚  â”‚ â”‚phone    â”‚ â”‚        â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â”‚address  â”‚ â”‚        â”‚â”‚
â”‚  â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚        â”‚â”‚
â”‚  â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                               â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚              KAFKA CLUSTER  â”‚                               â”‚â”‚
â”‚  â”‚                             â”‚                               â”‚â”‚
â”‚  â”‚     Producer â”€â”€[schema_id]â”€â”€â”¼â”€â”€â–¶ Topic â”€â”€[schema_id]â”€â”€â–¶ Consumerâ”‚â”‚
â”‚  â”‚         â”‚                   â”‚              â”‚             â”‚  â”‚â”‚
â”‚  â”‚         â–¼                   â”‚              â–¼             â–¼  â”‚â”‚
â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚â”‚
â”‚  â”‚   â”‚ Validate â”‚             â”‚        â”‚ Validate â”‚ â”‚Deserializâ”‚â”‚â”‚
â”‚  â”‚   â”‚& Serializâ”‚             â”‚        â”‚&Deserial.â”‚ â”‚   Data   â”‚â”‚â”‚
â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Avro Schema Example**
```json
{
  "type": "record",
  "name": "User",
  "namespace": "com.company.user",
  "fields": [
    {
      "name": "id",
      "type": "long",
      "doc": "Unique user identifier"
    },
    {
      "name": "name",
      "type": "string",
      "doc": "User full name"
    },
    {
      "name": "email",
      "type": "string",
      "doc": "User email address"
    },
    {
      "name": "phone",
      "type": ["null", "string"],
      "default": null,
      "doc": "Optional phone number"
    },
    {
      "name": "created_at",
      "type": {
        "type": "long",
        "logicalType": "timestamp-millis"
      },
      "doc": "Account creation timestamp"
    }
  ]
}
```

#### **Schema Evolution Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCHEMA EVOLUTION COMPATIBILITY               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  BACKWARD COMPATIBLE (default):                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ âœ… Delete fields                                            â”‚â”‚
â”‚  â”‚ âœ… Add optional fields (with defaults)                     â”‚â”‚
â”‚  â”‚ âŒ Add required fields                                      â”‚â”‚
â”‚  â”‚ âŒ Change field types                                       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  FORWARD COMPATIBLE:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ âœ… Add fields                                               â”‚â”‚
â”‚  â”‚ âœ… Delete optional fields                                   â”‚â”‚
â”‚  â”‚ âŒ Delete required fields                                   â”‚â”‚
â”‚  â”‚ âŒ Change field types                                       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  FULL COMPATIBLE:                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ âœ… Add optional fields (with defaults)                     â”‚â”‚
â”‚  â”‚ âœ… Delete optional fields                                   â”‚â”‚
â”‚  â”‚ âŒ Add/delete required fields                               â”‚â”‚
â”‚  â”‚ âŒ Change field types                                       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **ğŸ› ï¸ ĞŸÑ€Ğ°ĞºÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ: Production Kafka Setup**

#### **Docker Compose Ğ´Ğ»Ñ Full Stack**
```yaml
version: '3.8'
services:
  zookeeper-1:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SERVERS: zookeeper-1:2888:3888;zookeeper-2:2888:3888;zookeeper-3:2888:3888
    volumes:
      - zk1-data:/var/lib/zookeeper/data
      - zk1-logs:/var/lib/zookeeper/log

  kafka-1:
    image: confluentinc/cp-kafka:latest
    depends_on: [zookeeper-1, zookeeper-2, zookeeper-3]
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:9092
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: false
      KAFKA_DELETE_TOPIC_ENABLE: true
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: kafka-1
    volumes:
      - kafka1-data:/var/lib/kafka/data

  schema-registry:
    image: confluentinc/cp-schema-registry:latest
    depends_on: [kafka-1, kafka-2, kafka-3]
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka-1:9092,kafka-2:9092,kafka-3:9092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    ports:
      - "8081:8081"

  kafka-manager:
    image: hlebalbau/kafka-manager:stable
    depends_on: [kafka-1, kafka-2, kafka-3]
    environment:
      ZK_HOSTS: zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181
      APPLICATION_SECRET: "random-secret"
    ports:
      - "9000:9000"

volumes:
  zk1-data:
  zk1-logs:
  kafka1-data:
```

---

## ğŸš€ **Ğ“Ğ›ĞĞ’Ğ 3.2: Ğ ĞĞ—Ğ ĞĞ‘ĞĞ¢ĞšĞ PRODUCER Ğ˜ CONSUMER ĞŸĞ Ğ˜Ğ›ĞĞ–Ğ•ĞĞ˜Ğ™**

### **ğŸ“¤ Advanced Producer Development**

#### **High-Performance Producer Configuration**
```java
// HighPerformanceKafkaProducer.java
import org.apache.kafka.clients.producer.*;
import org.apache.kafka.common.serialization.StringSerializer;
import java.util.Properties;
import java.util.concurrent.Future;

public class HighPerformanceKafkaProducer {
    private final KafkaProducer<String, String> producer;
    
    public HighPerformanceKafkaProducer() {
        Properties props = new Properties();
        
        // Connection settings
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, 
                 "kafka-1:9092,kafka-2:9092,kafka-3:9092");
        
        // Performance tuning
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 65536);        // 64KB batches
        props.put(ProducerConfig.LINGER_MS_CONFIG, 10);            // Wait 10ms for batching
        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 67108864);  // 64MB buffer
        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, "lz4");   // Fast compression
        
        // Reliability settings
        props.put(ProducerConfig.ACKS_CONFIG, "all");              // Wait for all replicas
        props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE); // Infinite retries
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 1); // Order guarantee
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true); // Exactly-once semantics
        
        // Timeouts
        props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 30000);
        props.put(ProducerConfig.DELIVERY_TIMEOUT_MS_CONFIG, 120000);
        
        // Serializers
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        
        this.producer = new KafkaProducer<>(props);
    }
    
    public Future<RecordMetadata> sendAsync(String topic, String key, String value) {
        ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);
        
        return producer.send(record, new Callback() {
            @Override
            public void onCompletion(RecordMetadata metadata, Exception exception) {
                if (exception != null) {
                    // Log error and implement retry logic
                    System.err.println("Failed to send message: " + exception.getMessage());
                } else {
                    System.out.println("Message sent successfully: " + 
                        "topic=" + metadata.topic() + 
                        ", partition=" + metadata.partition() + 
                        ", offset=" + metadata.offset());
                }
            }
        });
    }
    
    public void close() {
        producer.close();
    }
}
```

#### **Partitioning Strategies**
```java
// CustomPartitioner.java
import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import java.util.Map;

public class CustomPartitioner implements Partitioner {
    
    @Override
    public int partition(String topic, Object key, byte[] keyBytes, 
                        Object value, byte[] valueBytes, Cluster cluster) {
        
        int numPartitions = cluster.partitionCountForTopic(topic);
        
        if (key == null) {
            // Round-robin Ğ´Ğ»Ñ null keys
            return (int) (System.currentTimeMillis() % numPartitions);
        }
        
        String keyStr = (String) key;
        
        // Ğ¡Ğ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ´Ğ»Ñ VIP Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹
        if (keyStr.startsWith("VIP_")) {
            return 0; // Ğ’ÑĞµĞ³Ğ´Ğ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² partition 0
        }
        
        // Ğ“ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¿Ğ°Ñ€Ñ‚Ğ¸Ñ†Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
        if (keyStr.contains("_US_")) {
            return 1;
        } else if (keyStr.contains("_EU_")) {
            return 2;
        } else if (keyStr.contains("_ASIA_")) {
            return 3;
        }
        
        // Default hash partitioning
        return Math.abs(keyStr.hashCode()) % numPartitions;
    }
    
    @Override
    public void close() {}
    
    @Override
    public void configure(Map<String, ?> configs) {}
}
```

#### **Producer Patterns**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PRODUCER PATTERNS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  FIRE AND FORGET:                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Producer â”€â”€send()â”€â”€â–¶ Kafka â”€â”€â–¶ â“ (don't care about result) â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚ + Highest throughput                                        â”‚â”‚
â”‚  â”‚ - No delivery guarantee                                     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  SYNCHRONOUS SEND:                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Producer â”€â”€send().get()â”€â”€â–¶ Kafka â”€â”€â–¶ âœ…/âŒ â—€â”€â”€ wait for ACK â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚ + Delivery guarantee                                        â”‚â”‚
â”‚  â”‚ - Lowest throughput                                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  ASYNCHRONOUS SEND:                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Producer â”€â”€send(callback)â”€â”€â–¶ Kafka â”€â”€â–¶ âœ…/âŒ â”€â”€â–¶ callback   â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚ + High throughput + Error handling                          â”‚â”‚
â”‚  â”‚ - More complex logic                                        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **ğŸ“¥ Advanced Consumer Development**

#### **High-Performance Consumer Configuration**
```java
// HighPerformanceKafkaConsumer.java
import org.apache.kafka.clients.consumer.*;
import org.apache.kafka.common.serialization.StringDeserializer;
import java.time.Duration;
import java.util.*;
import java.util.concurrent.*;

public class HighPerformanceKafkaConsumer {
    private final KafkaConsumer<String, String> consumer;
    private final ExecutorService executor;
    private volatile boolean running = true;
    
    public HighPerformanceKafkaConsumer(String groupId) {
        Properties props = new Properties();
        
        // Connection settings
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, 
                 "kafka-1:9092,kafka-2:9092,kafka-3:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        
        // Performance tuning
        props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 1024);      // Min 1KB per fetch
        props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500);     // Max wait 500ms
        props.put(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, 1048576); // 1MB per partition
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 1000);     // Process 1000 records at once
        
        // Offset management
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);  // Manual offset commit
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        
        // Session management
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 30000);
        props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 10000);
        props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, 300000); // 5 minutes
        
        // Deserializers
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        
        this.consumer = new KafkaConsumer<>(props);
        this.executor = Executors.newFixedThreadPool(10); // Thread pool for processing
    }
    
    public void consume(List<String> topics) {
        consumer.subscribe(topics);
        
        while (running) {
            try {
                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
                
                if (!records.isEmpty()) {
                    // Process records in parallel
                    List<Future<?>> futures = new ArrayList<>();
                    
                    for (ConsumerRecord<String, String> record : records) {
                        Future<?> future = executor.submit(() -> processRecord(record));
                        futures.add(future);
                    }
                    
                    // Wait for all processing to complete
                    for (Future<?> future : futures) {
                        future.get(); // This will throw exception if processing failed
                    }
                    
                    // Commit offsets only after successful processing
                    consumer.commitSync();
                }
                
            } catch (Exception e) {
                System.err.println("Error in consumer loop: " + e.getMessage());
                // Implement retry logic or dead letter queue
            }
        }
    }
    
    private void processRecord(ConsumerRecord<String, String> record) {
        try {
            // Simulate processing time
            Thread.sleep(10);
            
            System.out.println("Processed message: " + 
                "key=" + record.key() + 
                ", value=" + record.value() + 
                ", partition=" + record.partition() + 
                ", offset=" + record.offset());
                
        } catch (Exception e) {
            System.err.println("Failed to process record: " + e.getMessage());
            // Send to dead letter queue or retry
            throw new RuntimeException(e);
        }
    }
    
    public void shutdown() {
        running = false;
        consumer.close();
        executor.shutdown();
    }
}
```

#### **Consumer Groups Ğ¸ Partition Assignment**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONSUMER GROUP REBALANCING                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  INITIAL STATE (3 consumers, 6 partitions):                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Topic: user-events                                          â”‚â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”â”‚â”‚
â”‚  â”‚ â”‚ Part 0  â”‚â”‚ Part 1  â”‚â”‚ Part 2  â”‚â”‚ Part 3  â”‚â”‚ Part 4  â”‚â”‚P5 â”‚â”‚â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”˜â”‚â”‚
â”‚  â”‚     â”‚          â”‚          â”‚          â”‚          â”‚        â”‚  â”‚â”‚
â”‚  â”‚     â–¼          â–¼          â–¼          â–¼          â–¼        â–¼  â”‚â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚  â”‚ â”‚  Consumer A    â”‚  â”‚  Consumer B    â”‚  â”‚  Consumer C    â”‚ â”‚â”‚
â”‚  â”‚ â”‚  (Part 0,1)    â”‚  â”‚  (Part 2,3)    â”‚  â”‚  (Part 4,5)    â”‚ â”‚â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  AFTER CONSUMER B FAILURE (rebalancing):                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”â”‚â”‚
â”‚  â”‚ â”‚ Part 0  â”‚â”‚ Part 1  â”‚â”‚ Part 2  â”‚â”‚ Part 3  â”‚â”‚ Part 4  â”‚â”‚P5 â”‚â”‚â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”˜â”‚â”‚
â”‚  â”‚     â”‚          â”‚          â”‚          â”‚          â”‚        â”‚  â”‚â”‚
â”‚  â”‚     â–¼          â–¼          â–¼          â–¼          â–¼        â–¼  â”‚â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚â”‚
â”‚  â”‚ â”‚  Consumer A    â”‚              â”‚  Consumer C    â”‚         â”‚â”‚
â”‚  â”‚ â”‚ (Part 0,1,2)   â”‚              â”‚ (Part 3,4,5)   â”‚         â”‚â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Offset Management Strategies**
```java
// OffsetManagementStrategies.java
public class OffsetManagementStrategies {
    
    // Strategy 1: Auto-commit (Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹, Ğ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ²ĞµÑÑ‚Ğ¸ Ğº Ğ¿Ğ¾Ñ‚ĞµÑ€Ğµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹)
    public void autoCommitExample() {
        Properties props = new Properties();
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 5000);
        
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        // Consumer Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ ĞºĞ¾Ğ¼Ğ¼Ğ¸Ñ‚Ğ¸Ñ‚ offsets ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 5 ÑĞµĞºÑƒĞ½Ğ´
    }
    
    // Strategy 2: Manual commit after processing (Ğ½Ğ°Ğ´ĞµĞ¶Ğ½ĞµĞµ)
    public void manualCommitExample() {
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
            
            for (ConsumerRecord<String, String> record : records) {
                processRecord(record); // Process first
            }
            
            try {
                consumer.commitSync(); // Then commit
            } catch (CommitFailedException e) {
                // Handle commit failure
                System.err.println("Commit failed: " + e.getMessage());
            }
        }
    }
    
    // Strategy 3: Commit specific offsets
    public void specificOffsetCommitExample() {
        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
        
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
            
            for (ConsumerRecord<String, String> record : records) {
                processRecord(record);
                
                // Track offset for each partition
                TopicPartition partition = new TopicPartition(record.topic(), record.partition());
                OffsetAndMetadata offset = new OffsetAndMetadata(record.offset() + 1);
                offsets.put(partition, offset);
            }
            
            // Commit specific offsets
            consumer.commitSync(offsets);
            offsets.clear();
        }
    }
}
```

---

### **âš–ï¸ Consumer Load Balancing Ğ¸ Scaling**

#### **Dynamic Consumer Scaling**
```java
// DynamicConsumerScaler.java
import java.util.concurrent.atomic.AtomicInteger;
import java.util.List;
import java.util.ArrayList;

public class DynamicConsumerScaler {
    private final List<Thread> consumerThreads = new ArrayList<>();
    private final AtomicInteger consumerCount = new AtomicInteger(0);
    private final String groupId;
    private final List<String> topics;
    
    public DynamicConsumerScaler(String groupId, List<String> topics) {
        this.groupId = groupId;
        this.topics = topics;
    }
    
    public void scaleUp(int additionalConsumers) {
        for (int i = 0; i < additionalConsumers; i++) {
            String consumerId = groupId + "-consumer-" + consumerCount.incrementAndGet();
            
            Thread consumerThread = new Thread(() -> {
                HighPerformanceKafkaConsumer consumer = 
                    new HighPerformanceKafkaConsumer(groupId);
                consumer.consume(topics);
            });
            
            consumerThread.setName(consumerId);
            consumerThread.start();
            consumerThreads.add(consumerThread);
            
            System.out.println("Started consumer: " + consumerId);
        }
    }
    
    public void scaleDown(int consumersToRemove) {
        int removed = 0;
        for (int i = consumerThreads.size() - 1; i >= 0 && removed < consumersToRemove; i--) {
            Thread thread = consumerThreads.get(i);
            thread.interrupt(); // Gracefully shutdown
            consumerThreads.remove(i);
            removed++;
            
            System.out.println("Stopped consumer: " + thread.getName());
        }
    }
    
    public int getCurrentConsumerCount() {
        return consumerThreads.size();
    }
}
```

#### **Consumer Lag Monitoring**
```java
// ConsumerLagMonitor.java
import org.apache.kafka.clients.admin.*;
import org.apache.kafka.clients.consumer.*;
import java.util.*;
import java.util.concurrent.ExecutionException;

public class ConsumerLagMonitor {
    private final AdminClient adminClient;
    private final Properties consumerProps;
    
    public ConsumerLagMonitor(String bootstrapServers) {
        Properties adminProps = new Properties();
        adminProps.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        this.adminClient = AdminClient.create(adminProps);
        
        consumerProps = new Properties();
        consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
    }
    
    public Map<String, Long> getConsumerLag(String groupId) 
            throws ExecutionException, InterruptedException {
        
        Map<String, Long> lagPerPartition = new HashMap<>();
        
        // Get consumer group description
        DescribeConsumerGroupsResult groupResult = 
            adminClient.describeConsumerGroups(Collections.singletonList(groupId));
        ConsumerGroupDescription groupDescription = 
            groupResult.describedGroups().get(groupId).get();
        
        // Get committed offsets
        ListConsumerGroupOffsetsResult offsetsResult = 
            adminClient.listConsumerGroupOffsets(groupId);
        Map<TopicPartition, OffsetAndMetadata> committedOffsets = 
            offsetsResult.partitionsToOffsetAndMetadata().get();
        
        // Get end offsets for each partition
        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(consumerProps)) {
            Set<TopicPartition> partitions = committedOffsets.keySet();
            Map<TopicPartition, Long> endOffsets = consumer.endOffsets(partitions);
            
            // Calculate lag
            for (Map.Entry<TopicPartition, OffsetAndMetadata> entry : committedOffsets.entrySet()) {
                TopicPartition partition = entry.getKey();
                long committedOffset = entry.getValue().offset();
                long endOffset = endOffsets.get(partition);
                long lag = endOffset - committedOffset;
                
                lagPerPartition.put(partition.toString(), lag);
            }
        }
        
        return lagPerPartition;
    }
    
    public void printLagReport(String groupId) {
        try {
            Map<String, Long> lag = getConsumerLag(groupId);
            
            System.out.println("=== Consumer Lag Report for Group: " + groupId + " ===");
            System.out.println("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
            System.out.println("â”‚ Partition               â”‚ Lag         â”‚");
            System.out.println("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
            
            long totalLag = 0;
            for (Map.Entry<String, Long> entry : lag.entrySet()) {
                System.out.printf("â”‚ %-23s â”‚ %11d â”‚%n", entry.getKey(), entry.getValue());
                totalLag += entry.getValue();
            }
            
            System.out.println("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");
            System.out.printf("â”‚ TOTAL LAG               â”‚ %11d â”‚%n", totalLag);
            System.out.println("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");
            
        } catch (Exception e) {
            System.err.println("Failed to get lag information: " + e.getMessage());
        }
    }
}
```

---

## ğŸ° **Ğ“Ğ›ĞĞ’Ğ 3.3: ĞŸĞ ĞĞšĞ¢Ğ˜ĞšĞ Ğ¡ RABBITMQ**

### **ğŸ”€ Advanced Routing Ñ Headers Exchange**

#### **Headers Exchange Implementation**
```java
// RabbitMQHeadersExchangeExample.java
import com.rabbitmq.client.*;
import java.util.HashMap;
import java.util.Map;

public class RabbitMQHeadersExchangeExample {
    
    private static final String EXCHANGE_NAME = "notification_headers";
    
    public static void setupExchangeAndQueues() throws Exception {
        ConnectionFactory factory = new ConnectionFactory();
        factory.setHost("localhost");
        
        try (Connection connection = factory.newConnection();
             Channel channel = connection.createChannel()) {
            
            // Declare headers exchange
            channel.exchangeDeclare(EXCHANGE_NAME, "headers", true);
            
            // Declare queues
            channel.queueDeclare("email_queue", true, false, false, null);
            channel.queueDeclare("sms_queue", true, false, false, null);
            channel.queueDeclare("push_queue", true, false, false, null);
            channel.queueDeclare("high_priority_queue", true, false, false, null);
            
            // Binding for email notifications
            Map<String, Object> emailHeaders = new HashMap<>();
            emailHeaders.put("x-match", "all");
            emailHeaders.put("type", "email");
            channel.queueBind("email_queue", EXCHANGE_NAME, "", emailHeaders);
            
            // Binding for SMS notifications
            Map<String, Object> smsHeaders = new HashMap<>();
            smsHeaders.put("x-match", "all");
            smsHeaders.put("type", "sms");
            channel.queueBind("sms_queue", EXCHANGE_NAME, "", smsHeaders);
            
            // Binding for high priority (any type)
            Map<String, Object> highPriorityHeaders = new HashMap<>();
            highPriorityHeaders.put("x-match", "any");
            highPriorityHeaders.put("priority", "high");
            channel.queueBind("high_priority_queue", EXCHANGE_NAME, "", highPriorityHeaders);
        }
    }
    
    public static void publishMessage(String type, String priority, String content) throws Exception {
        ConnectionFactory factory = new ConnectionFactory();
        factory.setHost("localhost");
        
        try (Connection connection = factory.newConnection();
             Channel channel = connection.createChannel()) {
            
            // Set message headers
            Map<String, Object> headers = new HashMap<>();
            headers.put("type", type);
            headers.put("priority", priority);
            headers.put("timestamp", System.currentTimeMillis());
            
            AMQP.BasicProperties props = new AMQP.BasicProperties.Builder()
                .headers(headers)
                .deliveryMode(2) // Persistent message
                .build();
            
            channel.basicPublish(EXCHANGE_NAME, "", props, content.getBytes());
            
            System.out.println("Published message: " + content + 
                " with headers: " + headers);
        }
    }
}
```

#### **Complex Routing Scenarios**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   COMPLEX ROUTING EXAMPLE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  Message Headers: {type: "email", priority: "high", region: "US"}â”‚
â”‚                                                                 â”‚
â”‚                           â”‚                                     â”‚
â”‚                           â–¼                                     â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                  â”‚ Headers Exchangeâ”‚                           â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                           â”‚                                     â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚          â”‚                â”‚                â”‚                   â”‚
â”‚          â–¼                â–¼                â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Email Queue  â”‚ â”‚ High Priorityâ”‚ â”‚ US Region    â”‚           â”‚
â”‚  â”‚              â”‚ â”‚ Queue        â”‚ â”‚ Queue        â”‚           â”‚
â”‚  â”‚ Binding:     â”‚ â”‚              â”‚ â”‚              â”‚           â”‚
â”‚  â”‚ type=email   â”‚ â”‚ Binding:     â”‚ â”‚ Binding:     â”‚           â”‚
â”‚  â”‚ (x-match=all)â”‚ â”‚ priority=highâ”‚ â”‚ region=US    â”‚           â”‚
â”‚  â”‚              â”‚ â”‚ (x-match=any)â”‚ â”‚ (x-match=all)â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚          â”‚                â”‚                â”‚                   â”‚
â”‚          â–¼                â–¼                â–¼                   â”‚
â”‚    âœ… Matches       âœ… Matches       âœ… Matches              â”‚
â”‚   (type=email)     (priority=high)   (region=US)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **ğŸ’€ Dead Letter Exchanges**

#### **DLX Setup Ğ¸ Configuration**
```java
// DeadLetterExchangeSetup.java
public class DeadLetterExchangeSetup {
    
    public static void setupDLX() throws Exception {
        ConnectionFactory factory = new ConnectionFactory();
        factory.setHost("localhost");
        
        try (Connection connection = factory.newConnection();
             Channel channel = connection.createChannel()) {
            
            // Main exchange and queue
            channel.exchangeDeclare("orders_exchange", "direct");
            
            Map<String, Object> mainQueueArgs = new HashMap<>();
            mainQueueArgs.put("x-dead-letter-exchange", "orders_dlx");
            mainQueueArgs.put("x-dead-letter-routing-key", "failed");
            mainQueueArgs.put("x-message-ttl", 300000); // 5 minutes TTL
            mainQueueArgs.put("x-max-retries", 3);
            
            channel.queueDeclare("orders_queue", true, false, false, mainQueueArgs);
            channel.queueBind("orders_queue", "orders_exchange", "new");
            
            // Dead Letter Exchange and Queue
            channel.exchangeDeclare("orders_dlx", "direct");
            channel.queueDeclare("orders_dlq", true, false, false, null);
            channel.queueBind("orders_dlq", "orders_dlx", "failed");
            
            // Retry mechanism setup
            setupRetryMechanism(channel);
        }
    }
    
    private static void setupRetryMechanism(Channel channel) throws Exception {
        // Retry queue with TTL
        Map<String, Object> retryQueueArgs = new HashMap<>();
        retryQueueArgs.put("x-message-ttl", 60000); // 1 minute delay
        retryQueueArgs.put("x-dead-letter-exchange", "orders_exchange");
        retryQueueArgs.put("x-dead-letter-routing-key", "retry");
        
        channel.queueDeclare("orders_retry_queue", true, false, false, retryQueueArgs);
        channel.queueBind("orders_retry_queue", "orders_dlx", "retry");
    }
}
```

#### **DLX Message Flow**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEAD LETTER EXCHANGE FLOW                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. NORMAL PROCESSING:                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Producer â”€â”€â–¶ orders_exchange â”€â”€â–¶ orders_queue â”€â”€â–¶ Consumer  â”‚â”‚
â”‚  â”‚                                      â”‚                      â”‚â”‚
â”‚  â”‚                                      â–¼                      â”‚â”‚
â”‚  â”‚                               âœ… Success                    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  2. FAILURE SCENARIO:                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Producer â”€â”€â–¶ orders_exchange â”€â”€â–¶ orders_queue â”€â”€â–¶ Consumer  â”‚â”‚
â”‚  â”‚                                      â”‚              â”‚       â”‚â”‚
â”‚  â”‚                                      â”‚              â–¼       â”‚â”‚
â”‚  â”‚                                      â”‚          âŒ NACK     â”‚â”‚
â”‚  â”‚                                      â”‚              â”‚       â”‚â”‚
â”‚  â”‚                                      â–¼              â”‚       â”‚â”‚
â”‚  â”‚                              x-dead-letter-exchange â”‚       â”‚â”‚
â”‚  â”‚                                      â”‚              â”‚       â”‚â”‚
â”‚  â”‚                                      â–¼              â”‚       â”‚â”‚
â”‚  â”‚                               orders_dlx â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚â”‚
â”‚  â”‚                                      â”‚                      â”‚â”‚
â”‚  â”‚                                      â–¼                      â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚â”‚
â”‚  â”‚  â”‚ RETRY LOGIC:                                            â”‚â”‚â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” TTL â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” retry â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚â”‚
â”‚  â”‚  â”‚ â”‚retry_queue  â”‚â”€â”€â”€â”€â–¶â”‚orders_exchangeâ”‚â”€â”€â”€â”€â–¶â”‚main_queueâ”‚ â”‚â”‚â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚â”‚
â”‚  â”‚  â”‚                                                       â”‚â”‚â”‚
â”‚  â”‚  â”‚ IF still fails after 3 retries:                      â”‚â”‚â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚â”‚â”‚
â”‚  â”‚  â”‚ â”‚  DLQ        â”‚ â—€â”€â”€ Permanent failure storage        â”‚â”‚â”‚
â”‚  â”‚  â”‚ â”‚ (manual     â”‚                                       â”‚â”‚â”‚
â”‚  â”‚  â”‚ â”‚ review)     â”‚                                       â”‚â”‚â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                       â”‚â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **â° Message TTL Ğ¸ Queue Length Limits**

#### **TTL Ğ¸ Priority Queues**
```java
// TTLAndPriorityQueues.java
public class TTLAndPriorityQueues {
    
    public static void setupPriorityQueueWithTTL() throws Exception {
        ConnectionFactory factory = new ConnectionFactory();
        factory.setHost("localhost");
        
        try (Connection connection = factory.newConnection();
             Channel channel = connection.createChannel()) {
            
            // Priority queue Ñ TTL
            Map<String, Object> queueArgs = new HashMap<>();
            queueArgs.put("x-max-priority", 10);           // Priority levels 0-10
            queueArgs.put("x-message-ttl", 3600000);       // 1 hour TTL
            queueArgs.put("x-max-length", 10000);          // Max 10K messages
            queueArgs.put("x-max-length-bytes", 10485760); // Max 10MB
            queueArgs.put("x-overflow", "reject-publish"); // Reject when full
            
            channel.queueDeclare("priority_queue", true, false, false, queueArgs);
            
            // Publish message Ñ priority
            publishPriorityMessage(channel, "High priority task", 8);
            publishPriorityMessage(channel, "Medium priority task", 5);
            publishPriorityMessage(channel, "Low priority task", 2);
        }
    }
    
    private static void publishPriorityMessage(Channel channel, String message, int priority) 
            throws Exception {
        
        AMQP.BasicProperties props = new AMQP.BasicProperties.Builder()
            .priority(priority)
            .expiration("300000") // 5 minutes per-message TTL
            .timestamp(new Date())
            .build();
        
        channel.basicPublish("", "priority_queue", props, message.getBytes());
        System.out.println("Published: " + message + " (priority: " + priority + ")");
    }
}
```

#### **Queue Length Management**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   QUEUE LENGTH MANAGEMENT                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  OVERFLOW BEHAVIORS:                                            â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ DROP-HEAD (x-overflow: "drop-head"):                       â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚ Queue: [msg1][msg2][msg3][msg4][msg5] â† maxlength=5        â”‚â”‚
â”‚  â”‚ New msg arrives: [msg6]                                     â”‚â”‚
â”‚  â”‚ Result: [msg2][msg3][msg4][msg5][msg6] (msg1 dropped)      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ REJECT-PUBLISH (x-overflow: "reject-publish"):             â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚ Queue: [msg1][msg2][msg3][msg4][msg5] â† maxlength=5        â”‚â”‚
â”‚  â”‚ New msg arrives: [msg6]                                     â”‚â”‚
â”‚  â”‚ Result: [msg1][msg2][msg3][msg4][msg5] (msg6 rejected)     â”‚â”‚
â”‚  â”‚ Publisher receives: basic.nack                              â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ REJECT-PUBLISH-DLX (with dead letter exchange):            â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚ Queue: [msg1][msg2][msg3][msg4][msg5] â† maxlength=5        â”‚â”‚
â”‚  â”‚ New msg arrives: [msg6]                                     â”‚â”‚
â”‚  â”‚ Result: [msg1][msg2][msg3][msg4][msg5]                     â”‚â”‚
â”‚  â”‚ msg6 â”€â”€â–¶ Dead Letter Exchange                              â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### **ğŸ”Œ RabbitMQ Plugins**

#### **Management Plugin Features**
```bash
# Enable management plugin
rabbitmq-plugins enable rabbitmq_management

# Access web interface at http://localhost:15672
# Default credentials: guest/guest
```

#### **Useful Plugins Setup**
```bash
# Federation plugin Ğ´Ğ»Ñ multi-datacenter
rabbitmq-plugins enable rabbitmq_federation
rabbitmq-plugins enable rabbitmq_federation_management

# Shovel plugin Ğ´Ğ»Ñ message transfer
rabbitmq-plugins enable rabbitmq_shovel
rabbitmq-plugins enable rabbitmq_shovel_management

# MQTT plugin Ğ´Ğ»Ñ IoT devices
rabbitmq-plugins enable rabbitmq_mqtt

# STOMP plugin Ğ´Ğ»Ñ web applications
rabbitmq-plugins enable rabbitmq_stomp
rabbitmq-plugins enable rabbitmq_web_stomp

# Delayed message plugin
# Download from: https://github.com/rabbitmq/rabbitmq-delayed-message-exchange
rabbitmq-plugins enable rabbitmq_delayed_message_exchange
```

#### **Federation Configuration**
```bash
# Setup federation link
rabbitmqctl set_parameter federation-upstream datacenter2 \
  '{"uri":"amqp://guest:guest@rabbit2.datacenter2.com","expires":3600000}'

# Create federated exchange
rabbitmqctl set_policy federate-exchanges "^federated\." \
  '{"federation-upstream-set":"all"}'
```

---

## ğŸ“¦ **Ğ“Ğ›ĞĞ’Ğ 3.4: Ğ¡Ğ•Ğ Ğ˜ĞĞ›Ğ˜Ğ—ĞĞ¦Ğ˜Ğ¯ Ğ˜ Ğ¤ĞĞ ĞœĞĞ¢Ğ« Ğ”ĞĞĞĞ«Ğ¥**

### **ğŸ”„ JSON vs Avro vs Protocol Buffers**

#### **Format Comparison**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SERIALIZATION FORMATS COMPARISON            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Aspect      â”‚   JSON   â”‚   Avro   â”‚ Protocol â”‚  MessagePack â”‚ â”‚
â”‚ â”‚             â”‚          â”‚          â”‚ Buffers  â”‚              â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ Size        â”‚    âŒ     â”‚    âœ…     â”‚    âœ…     â”‚      âœ…       â”‚ â”‚
â”‚ â”‚             â”‚  Largest â”‚ Compact  â”‚ Smallest â”‚   Compact    â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ Speed       â”‚    âŒ     â”‚    âœ…     â”‚    âœ…     â”‚      âœ…       â”‚ â”‚
â”‚ â”‚             â”‚   Slow   â”‚   Fast   â”‚ Fastest  â”‚     Fast     â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ Schema      â”‚    âŒ     â”‚    âœ…     â”‚    âœ…     â”‚      âŒ       â”‚ â”‚
â”‚ â”‚ Evolution   â”‚   None   â”‚ Advanced â”‚ Limited  â”‚    None      â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ Human       â”‚    âœ…     â”‚    âŒ     â”‚    âŒ     â”‚      âŒ       â”‚ â”‚
â”‚ â”‚ Readable    â”‚  Perfect â”‚   None   â”‚   None   â”‚    None      â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ Language    â”‚    âœ…     â”‚    âœ…     â”‚    âœ…     â”‚      âœ…       â”‚ â”‚
â”‚ â”‚ Support     â”‚ Universalâ”‚   Good   â”‚   Good   â”‚    Good      â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **Size Comparison Example**
```json
// JSON (151 bytes)
{
  "id": 12345,
  "name": "John Doe",
  "email": "john.doe@example.com",
  "age": 30,
  "active": true,
  "created_at": "2024-01-15T10:30:00Z"
}

// Avro (Binary, ~40 bytes)
// Schema defined separately, data is just binary values

// Protocol Buffers (~35 bytes)
// Schema compiled into code, most compact binary format

// MessagePack (~42 bytes)
// JSON-like but binary, good middle ground
```

---

### **ğŸ”§ Avro Schema Evolution**

#### **Schema Evolution Examples**
```json
// Original Schema (v1)
{
  "type": "record",
  "name": "User",
  "namespace": "com.company.user",
  "fields": [
    {"name": "id", "type": "long"},
    {"name": "name", "type": "string"},
    {"name": "email", "type": "string"}
  ]
}

// Evolution v2 - Adding optional field (BACKWARD compatible)
{
  "type": "record",
  "name": "User",
  "namespace": "com.company.user",
  "fields": [
    {"name": "id", "type": "long"},
    {"name": "name", "type": "string"},
    {"name": "email", "type": "string"},
    {"name": "phone", "type": ["null", "string"], "default": null}
  ]
}

// Evolution v3 - Adding field with default (BACKWARD compatible)
{
  "type": "record",
  "name": "User",
  "namespace": "com.company.user",
  "fields": [
    {"name": "id", "type": "long"},
    {"name": "name", "type": "string"},
    {"name": "email", "type": "string"},
    {"name": "phone", "type": ["null", "string"], "default": null},
    {"name": "status", "type": "string", "default": "active"}
  ]
}
```

#### **Schema Evolution Patterns**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCHEMA EVOLUTION PATTERNS                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  BACKWARD COMPATIBILITY:                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ New Schema â”€â”€can readâ”€â”€â–¶ Old Data                          â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚ Producer (old) â”€â”€v1 dataâ”€â”€â–¶ Consumer (new)                 â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚ âœ… Add optional fields with defaults                       â”‚â”‚
â”‚  â”‚ âœ… Remove fields                                            â”‚â”‚
â”‚  â”‚ âŒ Add required fields                                      â”‚â”‚
â”‚  â”‚ âŒ Change field types                                       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  FORWARD COMPATIBILITY:                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Old Schema â”€â”€can readâ”€â”€â–¶ New Data                          â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚ Producer (new) â”€â”€v2 dataâ”€â”€â–¶ Consumer (old)                 â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚ âœ… Add fields                                               â”‚â”‚
â”‚  â”‚ âœ… Remove optional fields                                   â”‚â”‚
â”‚  â”‚ âŒ Remove required fields                                   â”‚â”‚
â”‚  â”‚ âŒ Change field types                                       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                 â”‚
â”‚  FULL COMPATIBILITY:                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Both directions work (BACKWARD + FORWARD)                   â”‚â”‚
â”‚  â”‚                                                             â”‚â”‚
â”‚  â”‚ âœ… Add/remove optional fields with defaults only           â”‚â”‚
â”‚  â”‚ âŒ Any other changes                                        â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜