# Полное руководство по кэшированию для Backend разработчиков

## От нуля до эксперта: Глубокое погружение в мир кэширования

---

## Введение: Почему кэширование критично для современных систем

В эпоху микросекундных ожиданий пользователей и экспоненциального роста данных, кэширование стало не просто оптимизацией, а необходимостью выживания для любой серьезной backend системы.

### Современные вызовы производительности

```
Ожидания пользователей:
┌─────────────────────────────────────────┐
│ Web страница: < 3 секунды               │
│ API ответ: < 100ms                      │
│ Database query: < 50ms                  │
│ Cache hit: < 5ms                        │
│ Memory access: < 1ms                    │
└─────────────────────────────────────────┘

Реальность без кэширования:
Database -----> Disk I/O (10-50ms)
Network -----> Latency (50-200ms)
Processing --> CPU cycles (1-10ms)
Result: 61-260ms (НЕПРИЕМЛЕМО)
```

### ROI кэширования

**Без кэша**: 1000 RPS × 200ms = перегрузка сервера
**С кэшем**: 1000 RPS × 5ms = комфортная работа

**Экономика**:
- Снижение нагрузки на DB на 80-95%
- Уменьшение времени отклика в 10-50 раз
- Экономия на инфраструктуре до 60%
- Улучшение конверсии на 1% за каждые 100ms ускорения

---

## Модуль 1: Фундаментальные основы кэширования

### Глубокое понимание принципов локальности

#### Временная локальность (Temporal Locality)
Если данные были запрошены недавно, вероятность их повторного запроса в ближайшее время высока.

```
Реальный пример - социальная сеть:
Timeline пользователя:
Запрос в 10:00 -----> Загрузка постов из DB
Запрос в 10:01 -----> Те же посты из кэша (99% вероятность)
Запрос в 10:05 -----> Те же посты из кэша (90% вероятность)
Запрос в 11:00 -----> Новые посты из DB (10% вероятность старых)

Типичная кривая затухания:
Вероятность повторного доступа
100% |*
     | *
 80% |  *
     |   *
 60% |    *
     |     **
 40% |       ***
     |          ****
 20% |              ******
     |                    **********
  0% +---------------------------------> Время
     0  1m  5m  15m 1h   6h   24h   7d
```

#### Пространственная локальность (Spatial Locality)
Данные, расположенные рядом в памяти или логически связанные, часто запрашиваются вместе.

```
Пример - профиль пользователя:
GET /user/123/profile
├── user.basic_info     (имя, email)
├── user.preferences    (настройки)
├── user.stats         (статистика)
└── user.recent_activity (последние действия)

Паттерн доступа:
80% запросов профиля -----> также запрашивают preferences
60% запросов профиля -----> также запрашивают stats
40% запросов профиля -----> также запрашивают recent_activity

Оптимизация:
Кэшируем как single object:
cache_key = "user_full_profile:123"
cache_value = {basic_info, preferences, stats, recent_activity}
```

### CAP теорема и кэширование

```
Традиционная CAP теорема:
Consistency ←→ Availability ←→ Partition Tolerance
(Можно выбрать только 2 из 3)

CAP для кэширования:
┌─────────────────────────────────────────┐
│ Consistency: Кэш всегда актуален        │
│ Availability: Кэш всегда доступен       │
│ Performance: Кэш всегда быстрый         │
└─────────────────────────────────────────┘

Компромиссы:
High Consistency + High Availability = Low Performance
(Частая синхронизация с источником)

High Performance + High Availability = Low Consistency  
(Eventual consistency, возможны stale данные)

High Performance + High Consistency = Low Availability
(Кэш недоступен при проблемах с источником)
```

### Математика кэширования

#### Формула эффективности кэша

```
Effective Access Time = Hit Rate × Cache Time + Miss Rate × (Cache Time + Miss Penalty)

Пример:
Cache Hit Rate: 90%
Cache Access Time: 5ms
Database Access Time: 100ms

Without Cache: 100ms
With Cache: 0.9 × 5ms + 0.1 × (5ms + 100ms) = 4.5ms + 10.5ms = 15ms

Improvement: 100ms → 15ms = 6.7x faster
```

#### Закон Парето в кэшировании

```
80/20 Rule применительно к кэшированию:
- 20% данных отвечают за 80% запросов
- 20% пользователей генерируют 80% трафика  
- 20% функций используют 80% ресурсов

Практическое применение:
Cache Size = 20% of Total Data Size
Cache Hit Rate ≈ 80%

Пример:
Total Data: 1TB
Cache Size: 200GB (20%)
Expected Hit Rate: 80%
Memory Savings: 80% reduction in DB load
```

### Типы cache miss и их стоимость

```
Cold Miss (Compulsory Miss):
Данных никогда не было в кэше
Cost = Full database query time
Frequency = При первом доступе

Capacity Miss:
Данные были вытеснены из-за нехватки места
Cost = Full database query time  
Frequency = Зависит от размера кэша и LRU эффективности

Coherence Miss (Invalidation Miss):
Данные были принудительно удалены из-за обновления
Cost = Full database query time
Frequency = Зависит от частоты обновлений

Conflict Miss:
Данные вытеснены из-за хэш-коллизий
Cost = Full database query time
Frequency = Зависит от качества хэш-функции
```

---

## Модуль 2: Детальная классификация кэшей

### CPU кэши: Фундамент иерархии

```
CPU Cache Hierarchy:
                    Size        Latency     Bandwidth
L1 Cache (Data)     32KB        1 cycle     ~400GB/s
L1 Cache (Instruction) 32KB     1 cycle     ~400GB/s  
L2 Cache            256KB       3-5 cycles  ~200GB/s
L3 Cache (LLC)      8-32MB      15-30 cycles ~100GB/s
Main Memory (DRAM)  8-64GB      100-300 cycles ~50GB/s
SSD Storage         1TB+        10,000+ cycles ~3GB/s
HDD Storage         4TB+        100,000+ cycles ~150MB/s

Программирование для CPU кэша:
// Плохо - cache unfriendly
for (int j = 0; j < cols; j++) {
    for (int i = 0; i < rows; i++) {
        matrix[i][j] = computation();  // Прыжки по памяти
    }
}

// Хорошо - cache friendly  
for (int i = 0; i < rows; i++) {
    for (int j = 0; j < cols; j++) {
        matrix[i][j] = computation();  // Последовательный доступ
    }
}
```

### Browser кэширование: Первая линия обороны

```
Browser Cache Mechanics:
┌─────────────────────────────────────────┐
│ Memory Cache (RAM)                      │
│ ├── Images, CSS, JS                     │
│ ├── Size: ~50-200MB                     │
│ └── Lifetime: До перезагрузки вкладки   │
│                                         │
│ Disk Cache (Storage)                    │
│ ├── Все типы ресурсов                   │
│ ├── Size: ~300MB-1GB                    │
│ └── Lifetime: Настраиваемо (days/weeks) │
│                                         │
│ Service Worker Cache                    │
│ ├── Программируемый кэш                 │
│ ├── Size: Quota API (~50% доступного)   │
│ └── Lifetime: Полный контроль           │
└─────────────────────────────────────────┘

HTTP Cache Headers в деталях:
Cache-Control: max-age=31536000, immutable
// Кэшировать на год, контент никогда не изменится

Cache-Control: max-age=0, must-revalidate  
// Всегда проверять актуальность с сервером

Cache-Control: private, no-store
// Не кэшировать вообще (sensitive data)

Vary: Accept-Encoding, User-Agent
// Разные версии для разных браузеров/сжатия
```

### CDN: Глобальное распределение кэша

```
CDN Edge Locations:
User в Tokyo -----> Tokyo Edge (15ms)
                    ├── Hit: возврат контента
                    └── Miss: запрос к Origin (200ms)

User в London ----> London Edge (10ms)  
                    ├── Hit: возврат контента
                    └── Miss: запрос к Origin (150ms)

CDN Cache Hierarchy:
Edge Cache (POP) -----> Regional Cache -----> Origin Shield -----> Origin Server
50+ locations          5-10 locations        1-2 locations       Your DC

Типы CDN кэширования:
Static Content:
- Images, CSS, JS: TTL = 1 year
- HTML pages: TTL = 1 hour
- API responses: TTL = 5-30 minutes

Dynamic Content:
- Personalized pages: Edge Side Includes (ESI)
- API responses: Conditional caching
- Real-time data: No caching or very short TTL
```

### Reverse Proxy: Интеллектуальное кэширование

```
Nginx Cache Configuration:
http {
    proxy_cache_path /var/cache/nginx 
                     levels=1:2 
                     keys_zone=api_cache:10m 
                     max_size=10g 
                     inactive=60m 
                     use_temp_path=off;
    
    server {
        location /api/users/ {
            proxy_cache api_cache;
            proxy_cache_valid 200 302 10m;
            proxy_cache_valid 404 1m;
            proxy_cache_key "$scheme$request_method$host$request_uri";
            proxy_cache_bypass $http_cache_control;
            
            add_header X-Cache-Status $upstream_cache_status;
            proxy_pass http://backend;
        }
    }
}

Varnish VCL Example:
vcl 4.0;

backend default {
    .host = "127.0.0.1";
    .port = "8080";
}

sub vcl_recv {
    # Remove cookies for static content
    if (req.url ~ "\.(css|js|png|gif|jp(e)?g|swf|ico)$") {
        unset req.http.cookie;
    }
}

sub vcl_backend_response {
    # Cache static content for 1 day
    if (bereq.url ~ "\.(css|js|png|gif|jp(e)?g|swf|ico)$") {
        set beresp.ttl = 1d;
    }
    
    # Cache API responses for 5 minutes
    if (bereq.url ~ "^/api/") {
        set beresp.ttl = 5m;
    }
}
```

---

## Модуль 3: Продвинутые стратегии кэширования

### Cache-Aside: Детальная реализация

```python
class CacheAsidePattern:
    def __init__(self, cache, database):
        self.cache = cache
        self.database = database
        self.stats = {
            'hits': 0,
            'misses': 0,
            'errors': 0
        }
    
    def get(self, key):
        try:
            # Попытка получить из кэша
            value = self.cache.get(key)
            if value is not None:
                self.stats['hits'] += 1
                return self._deserialize(value)
            
            # Cache miss - обращение к БД
            self.stats['misses'] += 1
            value = self.database.get(key)
            
            if value is not None:
                # Асинхронное обновление кэша
                self._async_cache_set(key, value)
            
            return value
            
        except Exception as e:
            self.stats['errors'] += 1
            # Fallback to database on cache errors
            return self.database.get(key)
    
    def set(self, key, value):
        # Сначала обновляем БД
        self.database.set(key, value)
        
        # Затем инвалидируем кэш
        self.cache.delete(key)
        
        # Опционально: немедленно загружаем в кэш
        # self.cache.set(key, self._serialize(value), ttl=3600)
    
    def _async_cache_set(self, key, value):
        # Неблокирующее обновление кэша
        threading.Thread(
            target=lambda: self.cache.set(
                key, 
                self._serialize(value), 
                ttl=3600
            )
        ).start()

# Продвинутые паттерны Cache-Aside
class SmartCacheAside:
    def __init__(self):
        self.cache_levels = [
            LocalCache(size=100),      # L1: Local memory
            RedisCache(host='redis'),  # L2: Distributed  
            DatabaseCache()            # L3: Persistent
        ]
    
    def get(self, key):
        for level, cache in enumerate(self.cache_levels):
            value = cache.get(key)
            if value is not None:
                # Promote to higher levels (cache warming)
                for upper_level in range(level):
                    self.cache_levels[upper_level].set(key, value)
                return value
        
        return None
```

### Write-Through vs Write-Behind: Глубокий анализ

```python
class WriteThroughCache:
    """
    Синхронная запись в кэш и БД
    Плюсы: Консистентность, простота
    Минусы: Высокая латентность записи
    """
    def set(self, key, value):
        start_time = time.time()
        
        try:
            # Одновременная запись (можно распараллелить)
            futures = [
                executor.submit(self.database.set, key, value),
                executor.submit(self.cache.set, key, value, ttl=3600)
            ]
            
            # Ждем завершения обеих операций
            concurrent.futures.wait(futures)
            
            # Проверяем успешность
            for future in futures:
                if future.exception():
                    raise future.exception()
                    
        except Exception as e:
            # Rollback при ошибке
            self.cache.delete(key)
            raise e
        
        finally:
            latency = time.time() - start_time
            self.metrics.record_write_latency(latency)

class WriteBehindCache:
    """
    Асинхронная запись в БД, синхронная в кэш
    Плюсы: Низкая латентность, batch операции
    Минусы: Риск потери данных, сложность
    """
    def __init__(self):
        self.write_queue = queue.Queue(maxsize=10000)
        self.batch_size = 100
        self.flush_interval = 5  # seconds
        self.start_background_writer()
    
    def set(self, key, value):
        # Быстрая запись в кэш
        self.cache.set(key, value, ttl=3600)
        
        # Добавление в очередь для записи в БД
        try:
            self.write_queue.put_nowait({
                'key': key,
                'value': value,
                'timestamp': time.time()
            })
        except queue.Full:
            # Fallback: синхронная запись при переполнении
            self.database.set(key, value)
    
    def start_background_writer(self):
        def writer_worker():
            batch = []
            last_flush = time.time()
            
            while True:
                try:
                    # Собираем batch или ждем timeout
                    timeout = self.flush_interval - (time.time() - last_flush)
                    item = self.write_queue.get(timeout=max(0.1, timeout))
                    batch.append(item)
                    
                    # Flush conditions
                    should_flush = (
                        len(batch) >= self.batch_size or
                        time.time() - last_flush >= self.flush_interval
                    )
                    
                    if should_flush:
                        self._flush_batch(batch)
                        batch = []
                        last_flush = time.time()
                        
                except queue.Empty:
                    if batch:  # Flush remaining items
                        self._flush_batch(batch)
                        batch = []
                        last_flush = time.time()
        
        threading.Thread(target=writer_worker, daemon=True).start()
    
    def _flush_batch(self, batch):
        try:
            # Batch write to database
            self.database.batch_set([
                (item['key'], item['value']) for item in batch
            ])
            
            self.metrics.record_batch_write(len(batch))
            
        except Exception as e:
            # Error handling: retry, dead letter queue, etc.
            self._handle_write_error(batch, e)
```

### Refresh-Ahead: Проактивное обновление

```python
class RefreshAheadCache:
    """
    Упреждающее обновление кэша до истечения TTL
    Предотвращает cache stampede и обеспечивает стабильную производительность
    """
    def __init__(self, refresh_ratio=0.8):
        self.refresh_ratio = refresh_ratio  # Обновлять при 80% TTL
        self.refresh_executor = ThreadPoolExecutor(max_workers=5)
        self.refresh_locks = {}  # Предотвращение дублирования обновлений
    
    def get(self, key):
        cache_item = self.cache.get_with_metadata(key)
        
        if cache_item is None:
            # Cache miss - синхронная загрузка
            return self._load_and_cache(key)
        
        # Проверяем необходимость refresh
        if self._should_refresh(cache_item):
            self._async_refresh(key)
        
        return cache_item.value
    
    def _should_refresh(self, cache_item):
        age = time.time() - cache_item.created_at
        ttl = cache_item.ttl
        return age >= (ttl * self.refresh_ratio)
    
    def _async_refresh(self, key):
        # Используем lock для предотвращения множественных обновлений
        if key not in self.refresh_locks:
            self.refresh_locks[key] = threading.Lock()
        
        if self.refresh_locks[key].acquire(blocking=False):
            self.refresh_executor.submit(self._refresh_worker, key)
    
    def _refresh_worker(self, key):
        try:
            # Загружаем свежие данные
            fresh_value = self.database.get(key)
            if fresh_value is not None:
                self.cache.set(key, fresh_value, ttl=3600)
            
            self.metrics.record_refresh(key)
            
        except Exception as e:
            self.metrics.record_refresh_error(key, e)
        
        finally:
            self.refresh_locks[key].release()
            del self.refresh_locks[key]

# Продвинутый refresh-ahead с ML предсказанием
class PredictiveRefreshCache:
    def __init__(self):
        self.access_patterns = {}  # История доступа к ключам
        self.ml_model = AccessPatternPredictor()
    
    def record_access(self, key):
        if key not in self.access_patterns:
            self.access_patterns[key] = []
        
        self.access_patterns[key].append(time.time())
        
        # Ограничиваем историю (скользящее окно)
        cutoff = time.time() - 86400  # 24 часа
        self.access_patterns[key] = [
            t for t in self.access_patterns[key] if t > cutoff
        ]
    
    def predict_next_access(self, key):
        if key not in self.access_patterns:
            return None
        
        pattern = self.access_patterns[key]
        return self.ml_model.predict_next_access_time(pattern)
    
    def smart_refresh_schedule(self):
        for key in self.access_patterns:
            predicted_access = self.predict_next_access(key)
            cache_expiry = self.cache.get_expiry_time(key)
            
            if predicted_access and predicted_access < cache_expiry:
                # Запланировать refresh перед предсказанным доступом
                refresh_time = predicted_access - 60  # 1 минута буфер
                self.schedule_refresh(key, refresh_time)
```

---

## Модуль 4: Алгоритмы вытеснения - глубокая детализация

### LRU: Оптимальная реализация

```python
class OptimizedLRU:
    """
    Высокоэффективная реализация LRU с O(1) операциями
    Использует комбинацию HashMap + Doubly Linked List
    """
    class Node:
        def __init__(self, key=None, value=None):
            self.key = key
            self.value = value
            self.prev = None
            self.next = None
            self.frequency = 1
            self.timestamp = time.time()
    
    def __init__(self, capacity):
        self.capacity = capacity
        self.cache = {}  # key -> node
        
        # Dummy head and tail для упрощения операций
        self.head = self.Node()
        self.tail = self.Node()
        self.head.next = self.tail
        self.tail.prev = self.head
        
        # Статистика
        self.hits = 0
        self.misses = 0
        self.evictions = 0
    
    def get(self, key):
        if key in self.cache:
            node = self.cache[key]
            self._move_to_head(node)
            self.hits += 1
            return node.value
        
        self.misses += 1
        return None
    
    def put(self, key, value):
        if key in self.cache:
            # Обновление существующего
            node = self.cache[key]
            node.value = value
            self._move_to_head(node)
        else:
            # Новый элемент
            new_node = self.Node(key, value)
            
            if len(self.cache) >= self.capacity:
                # Вытеснение LRU элемента
                lru_node = self.tail.prev
                self._remove_node(lru_node)
                del self.cache[lru_node.key]
                self.evictions += 1
            
            self.cache[key] = new_node
            self._add_to_head(new_node)
    
    def _move_to_head(self, node):
        self._remove_node(node)
        self._add_to_head(node)
    
    def _remove_node(self, node):
        node.prev.next = node.next
        node.next.prev = node.prev
    
    def _add_to_head(self, node):
        node.prev = self.head
        node.next = self.head.next
        self.head.next.prev = node
        self.head.next = node

# Thread-Safe LRU для многопоточной среды
class ThreadSafeLRU:
    def __init__(self, capacity):
        self.lru = OptimizedLRU(capacity)
        self.lock = threading.RWLock()  # Читатели-писатели
    
    def get(self, key):
        with self.lock.reader():
            return self.lru.get(key)
    
    def put(self, key, value):
        with self.lock.writer():
            self.lru.put(key, value)

# Сегментированный LRU для минимизации lock contention
class SegmentedLRU:
    def __init__(self, capacity, segments=16):
        self.segments = [
            ThreadSafeLRU(capacity // segments) 
            for _ in range(segments)
        ]
        self.segment_mask = segments - 1
    
    def _get_segment(self, key):
        return hash(key) & self.segment_mask
    
    def get(self, key):
        segment_id = self._get_segment(key)
        return self.segments[segment_id].get(key)
    
    def put(self, key, value):
        segment_id = self._get_segment(key)
        self.segments[segment_id].put(key, value)
```

### LFU: Частотно-ориентированное вытеснение

```python
class AdvancedLFU:
    """
    LFU с aging для избежания pollution от старых популярных элементов
    """
    def __init__(self, capacity, aging_factor=0.1):
        self.capacity = capacity
        self.cache = {}  # key -> (value, frequency, last_access)
        self.frequency_buckets = defaultdict(OrderedDict)  # freq -> {key: True}
        self.min_frequency = 0
        self.aging_factor = aging_factor
        self.aging_interval = 3600  # 1 час
        self.last_aging = time.time()
    
    def get(self, key):
        if key not in self.cache:
            return None
        
        self._check_aging()
        
        value, freq, _ = self.cache[key]
        self._update_frequency(key, freq + 1)
        return value
    
    def put(self, key, value):
        if self.capacity <= 0:
            return
        
        self._check_aging()
        
        if key in self.cache:
            _, freq, _ = self.cache[key]
            self.cache[key] = (value, freq, time.time())
            self._update_frequency(key, freq + 1)
        else:
            if len(self.cache) >= self.capacity:
                self._evict_lfu()
            
            self.cache[key] = (value, 1, time.time())
            self.frequency_buckets[1][key] = True
            self.min_frequency = 1
    
    def _update_frequency(self, key, new_freq):
        _, old_freq, _ = self.cache[key]
        
        # Удаляем из старого bucket
        del self.frequency_buckets[old_freq][key]
        if not self.frequency_buckets[old_freq] and old_freq == self.min_frequency:
            self.min_frequency += 1
        
        # Добавляем в новый bucket
        self.frequency_buckets[new_freq][key] = True
        
        # Обновляем кэш
        value, _, _ = self.cache[key]
        self.cache[key] = (value, new_freq, time.time())
    
    def _evict_lfu(self):
        # Находим LFU ключ
        lfu_key = next(iter(self.frequency_buckets[self.min_frequency]))
        
        # Удаляем
        del self.frequency_buckets[self.min_frequency][lfu_key]
        del self.cache[lfu_key]
        
        # Обновляем min_frequency если bucket пустой
        if not self.frequency_buckets[self.min_frequency]:
            self.min_frequency += 1
    
    def _check_aging(self):
        """Aging: периодически уменьшаем частоты для адаптации к изменениям"""
        now = time.time()
        if now - self.last_aging > self.aging_interval:
            self._apply_aging()
            self.last_aging = now
    
    def _apply_aging(self):
        new_cache = {}
        new_frequency_buckets = defaultdict(OrderedDict)
        new_min_frequency = float('inf')
        
        for key, (value, freq, last_access) in self.cache.items():
            # Уменьшаем частоту
            new_freq = max(1, int(freq * (1 - self.aging_factor)))
            new_cache[key] = (value, new_freq, last_access)
            new_frequency_buckets[new_freq][key] = True
            new_min_frequency = min(new_min_frequency, new_freq)
        
        self.cache = new_cache
        self.frequency_buckets = new_frequency_buckets
        self.min_frequency = new_min_frequency
```

### Adaptive Replacement Cache (ARC)

```python
class AdaptiveReplacementCache:
    """
    ARC алгоритм, самоадаптирующийся между LRU и LFU
    Балансирует между recency и frequency автоматически
    """
    def __init__(self, capacity):
        self.capacity = capacity
        self.p = 0  # Adaptive parameter
        
        # Четыре списка ARC
        self.t1 = OrderedDict()  # Recent cache entries
        self.t2 = OrderedDict()  # Frequent cache entries  
        self.b1 = OrderedDict()  # Ghost entries for t1
        self.b2 = OrderedDict()  # Ghost entries for t2
        
        self.stats = {
            'hits_t1': 0, 'hits_t2': 0,
            'misses': 0, 'adaptations': 0
        }
    
    def get(self, key):
        # Case 1: Hit in T1 (recent)
        if key in self.t1:
            value = self.t1.pop(key)
            self.t2[key] = value  # Promote to frequent
            self.stats['hits_t1'] += 1
            return value
        
        # Case 2: Hit in T2 (frequent)
        if key in self.t2:
            value = self.t2.pop(key)
            self.t2[key] = value  # Move to end (most recent in T2)
            self.stats['hits_t2'] += 1
            return value
        
        self.stats['misses'] += 1
        return None
    
    def put(self, key, value):
        # Case 1: Hit in T1 or T2 (handled in get)
        if key in self.t1 or key in self.t2:
            # Update handled by get() method
            return
        
        # Case 2: Hit in B1 (was recently evicted from T1)
        if key in self.b1:
            self._adapt(delta=1)  # Increase preference for recent
            self.b1.pop(key)
            self._replace(key)
            self.t2[key] = value
            return
        
        # Case 3: Hit in B2 (was recently evicted from T2)
        if key in self.b2:
            self._adapt(delta=-1)  # Increase preference for frequent
            self.b2.pop(key)
            self._replace(key)
            self.t2[key] = value
            return
        
        # Case 4: Complete miss
        # Add to T1 (recent cache)
        if len(self.t1) + len(self.t2) >= self.capacity:
            self._replace(key)
        
        self.t1[key] = value
        
        # Manage ghost lists size
        total_ghosts = len(self.b1) + len(self.b2)
        if total_ghosts > self.capacity:
            if len(self.b1) > self.capacity // 2:
                self.b1.popitem(last=False)
            else:
                self.b2.popitem(last=False)
    
    def _adapt(self, delta):
        """Адаптация параметра p для балансировки recency vs frequency"""
        old_p = self.p
        
        if delta > 0:  # Favor recent (increase T1 target size)
            self.p = min(self.capacity, self.p + max(1, len(self.b2) // len(self.b1)))
        else:  # Favor frequent (decrease T1 target size)  
            self.p = max(0, self.p - max(1, len(self.b1) // len(self.b2)))
        
        if abs(self.p - old_p) > 0:
            self.stats['adaptations'] += 1
    
    def _replace(self, key):
        """Вытеснение элемента согласно ARC политике"""
        t1_size = len(self.t1)
        
        if t1_size > 0 and (t1_size > self.p or (key in self.b2 and t1_size == self.p)):
            # Evict from T1
            evicted_key, evicted_value = self.t1.popitem(last=False)
            self.b1[evicted_key] = True  # Add to ghost list
        else:
            # Evict from T2
            evicted_key, evicted_value = self.t2.popitem(last=False)
            self.b2[evicted_key] = True  # Add to ghost list
```

---

## Модуль 5: In-Memory кэширование на production уровне

### Управление памятью и GC оптимизации

```python
class MemoryEfficientCache:
    """
    Кэш с оптимизированным использованием памяти
    Минимизирует GC pressure и фрагментацию
    """
    def __init__(self, max_memory_mb=1024):
        self.max_memory = max_memory_mb * 1024 * 1024
        self.current_memory = 0
        
        # Пулы объектов для минимизации аллокаций
        self.node_pool = ObjectPool(self._create_node, max_size=1000)
        self.value_pool = ObjectPool(self._create_value, max_size=1000)
        
        # Сегментированное хранение для уменьшения GC pause
        self.segments = [LRUSegment() for _ in range(16)]
        
        # Off-heap storage для больших объектов
        self.off_heap = OffHeapStorage()
        
        # Мониторинг памяти
        self.memory_monitor = MemoryMonitor()
        self.memory_monitor.start()
    
    def put(self, key, value):
        value_size = self._estimate_size(value)
        
        # Большие объекты храним off-heap
        if value_size > 1024 * 1024:  # 1MB threshold
            return self._put_off_heap(key, value)
        
        # Проверка лимита памяти
        if self.current_memory + value_size > self.max_memory:
            self._evict_to_fit(value_size)
        
        segment = self._get_segment(key)
        segment.put(key, value, value_size)
        self.current_memory += value_size
    
    def _evict_to_fit(self, required_size):
        """Intelligent eviction учитывающий access patterns"""
        evicted_size = 0
        
        # Сначала пытаемся освободить место в least active segments
        segment_activity = [(i, s.get_activity_score()) 
                           for i, s in enumerate(self.segments)]
        segment_activity.sort(key=lambda x: x[1])  # Сортируем по активности
        
        for segment_id, _ in segment_activity:
            if evicted_size >= required_size:
                break
                
            evicted_size += self.segments[segment_id].evict_lru_items(
                required_size - evicted_size
            )
        
        self.current_memory -= evicted_size
    
    def _put_off_heap(self, key, value):
        """Хранение больших объектов вне Java heap"""
        serialized = self._serialize(value)
        off_heap_address = self.off_heap.allocate(len(serialized))
        self.off_heap.write(off_heap_address, serialized)
        
        # В основном кэше храним только ссылку
        metadata = OffHeapReference(off_heap_address, len(serialized))
        segment = self._get_segment(key)
        segment.put(key, metadata, 64)  # Размер metadata

class OffHeapStorage:
    """
    Off-heap хранилище с использованием memory-mapped файлов
    Позволяет избежать GC pressure для больших объектов
    """
    def __init__(self, initial_size_mb=100):
        self.size = initial_size_mb * 1024 * 1024
        self.file_path = f"/tmp/cache_offheap_{os.getpid()}.dat"
        
        # Создаем memory-mapped file
        with open(self.file_path, 'wb') as f:
            f.write(b'\x00' * self.size)
        
        self.mmap = mmap.mmap(
            open(self.file_path, 'r+b').fileno(),
            self.size,
            access=mmap.ACCESS_WRITE
        )
        
        # Free space management
        self.free_blocks = [(0, self.size)]  # (offset, size)
        self.allocated_blocks = {}  # address -> size
        self.lock = threading.Lock()
    
    def allocate(self, size):
        """Аллокация блока памяти, возвращает offset"""
        with self.lock:
            # Ищем подходящий свободный блок
            for i, (offset, block_size) in enumerate(self.free_blocks):
                if block_size >= size:
                    # Удаляем из свободных
                    del self.free_blocks[i]
                    
                    # Если блок больше нужного, добавляем остаток обратно
                    if block_size > size:
                        remaining_offset = offset + size
                        remaining_size = block_size - size
                        self.free_blocks.append((remaining_offset, remaining_size))
                        self.free_blocks.sort()  # Поддерживаем сортировку
                    
                    self.allocated_blocks[offset] = size
                    return offset
            
            # Нет места - расширяем файл
            return self._expand_and_allocate(size)
    
    def deallocate(self, offset):
        """Освобождение блока памяти"""
        with self.lock:
            if offset not in self.allocated_blocks:
                return
            
            size = self.allocated_blocks.pop(offset)
            
            # Добавляем в свободные блоки
            self.free_blocks.append((offset, size))
            self.free_blocks.sort()
            
            # Объединяем соседние свободные блоки
            self._coalesce_free_blocks()
    
    def write(self, offset, data):
        """Запись данных по offset"""
        self.mmap[offset:offset + len(data)] = data
    
    def read(self, offset, size):
        """Чтение данных по offset"""
        return self.mmap[offset:offset + size]
```

### Распределенное in-memory кэширование

```python
class DistributedCache:
    """
    Распределенный кэш с consistency guarantees
    Поддерживает различные топологии и consistency models
    """
    def __init__(self, nodes, consistency_model='eventual'):
        self.nodes = nodes
        self.consistency_model = consistency_model
        self.local_cache = LocalCache()
        self.vector_clock = VectorClock(len(nodes))
        self.partition_manager = ConsistentHashRing(nodes)
        
        # Для strong consistency
        self.coordinator = ConsensusCoordinator(nodes) if consistency_model == 'strong' else None
        
        # Мониторинг network partitions
        self.network_monitor = NetworkPartitionDetector(nodes)
        self.network_monitor.start()
    
    def get(self, key):
        if self.consistency_model == 'eventual':
            return self._get_eventual(key)
        elif self.consistency_model == 'strong':
            return self._get_strong(key)
        else:
            return self._get_weak(key)
    
    def _get_eventual(self, key):
        """Eventually consistent чтение с read repair"""
        # Сначала проверяем локальный кэш
        local_value = self.local_cache.get(key)
        if local_value and not self._is_stale(local_value):
            return local_value.data
        
        # Читаем с нескольких реплик
        replicas = self.partition_manager.get_replicas(key, count=3)
        responses = []
        
        for replica in replicas:
            try:
                response = replica.get(key, include_metadata=True)
                if response:
                    responses.append(response)
            except NetworkException:
                continue  # Skip unavailable replicas
        
        if not responses:
            return None
        
        # Выбираем самую свежую версию
        latest_response = max(responses, key=lambda r: r.vector_clock)
        
        # Read repair - обновляем отстающие реплики
        self._async_read_repair(key, latest_response, responses)
        
        # Обновляем локальный кэш
        self.local_cache.put(key, latest_response)
        
        return latest_response.data
    
    def _get_strong(self, key):
        """Strongly consistent чтение через consensus"""
        # Используем Raft/PBFT для consensus
        read_quorum = len(self.nodes) // 2 + 1
        
        # Отправляем read request на все узлы
        read_id = self._generate_read_id()
        votes = []
        
        for node in self.nodes:
            try:
                vote = node.read_vote(key, read_id)
                votes.append(vote)
                
                if len(votes) >= read_quorum:
                    break
            except NetworkException:
                continue
        
        if len(votes) < read_quorum:
            raise QuorumNotAvailableException()
        
        # Выбираем значение с highest committed version
        committed_value = max(votes, key=lambda v: v.commit_version)
        return committed_value.data
    
    def put(self, key, value):
        if self.consistency_model == 'eventual':
            return self._put_eventual(key, value)
        elif self.consistency_model == 'strong':
            return self._put_strong(key, value)
        else:
            return self._put_weak(key, value)
    
    def _put_eventual(self, key, value):
        """Eventually consistent запись с vector clocks"""
        # Обновляем vector clock
        self.vector_clock.increment(self.node_id)
        
        versioned_value = VersionedValue(
            data=value,
            vector_clock=self.vector_clock.copy(),
            timestamp=time.time()
        )
        
        # Записываем локально
        self.local_cache.put(key, versioned_value)
        
        # Асинхронно propagate на другие узлы
        replicas = self.partition_manager.get_replicas(key)
        self._async_propagate(key, versioned_value, replicas)
        
        return True
    
    def _put_strong(self, key, value):
        """Strongly consistent запись через 2PC"""
        transaction_id = self._generate_transaction_id()
        write_quorum = len(self.nodes) // 2 + 1
        
        # Phase 1: Prepare
        prepare_votes = []
        for node in self.nodes:
            try:
                vote = node.prepare_write(transaction_id, key, value)
                prepare_votes.append((node, vote))
                
                if len(prepare_votes) >= write_quorum:
                    break
            except NetworkException:
                continue
        
        if len(prepare_votes) < write_quorum:
            # Abort transaction
            self._abort_transaction(transaction_id, prepare_votes)
            raise QuorumNotAvailableException()
        
        # Phase 2: Commit
        commit_success = 0
        for node, vote in prepare_votes:
            try:
                if node.commit_write(transaction_id):
                    commit_success += 1
            except NetworkException:
                pass  # Continue with other nodes
        
        if commit_success >= write_quorum:
            return True
        else:
            # Inconsistent state - need recovery
            self._initiate_recovery(transaction_id)
            raise TransactionFailedException()

class ConsistentHashRing:
    """
    Consistent hashing для равномерного распределения данных
    С поддержкой virtual nodes для лучшего баланса
    """
    def __init__(self, nodes, virtual_nodes_per_physical=150):
        self.virtual_nodes_per_physical = virtual_nodes_per_physical
        self.ring = {}  # hash -> physical_node
        self.sorted_hashes = []
        
        for node in nodes:
            self.add_node(node)
    
    def add_node(self, node):
        """Добавление узла в ring"""
        for i in range(self.virtual_nodes_per_physical):
            virtual_key = f"{node.id}:{i}"
            hash_value = self._hash(virtual_key)
            self.ring[hash_value] = node
        
        self.sorted_hashes = sorted(self.ring.keys())
    
    def remove_node(self, node):
        """Удаление узла из ring"""
        to_remove = []
        for hash_value, ring_node in self.ring.items():
            if ring_node.id == node.id:
                to_remove.append(hash_value)
        
        for hash_value in to_remove:
            del self.ring[hash_value]
        
        self.sorted_hashes = sorted(self.ring.keys())
    
    def get_node(self, key):
        """Получение узла для ключа"""
        if not self.ring:
            return None
        
        hash_value = self._hash(key)
        
        # Находим первый узел по часовой стрелке
        for ring_hash in self.sorted_hashes:
            if hash_value <= ring_hash:
                return self.ring[ring_hash]
        
        # Wrap around - возвращаем первый узел
        return self.ring[self.sorted_hashes[0]]
    
    def get_replicas(self, key, count=3):
        """Получение списка узлов-реплик для ключа"""
        if not self.ring or count <= 0:
            return []
        
        hash_value = self._hash(key)
        replicas = []
        seen_nodes = set()
        
        # Находим стартовую позицию
        start_idx = 0
        for i, ring_hash in enumerate(self.sorted_hashes):
            if hash_value <= ring_hash:
                start_idx = i
                break
        
        # Собираем уникальные физические узлы
        idx = start_idx
        while len(replicas) < count and len(seen_nodes) < len(set(self.ring.values())):
            ring_hash = self.sorted_hashes[idx % len(self.sorted_hashes)]
            node = self.ring[ring_hash]
            
            if node.id not in seen_nodes:
                replicas.append(node)
                seen_nodes.add(node.id)
            
            idx += 1
        
        return replicas
    
    def _hash(self, key):
        """Хэш функция для consistent hashing"""
        return int(hashlib.md5(str(key).encode()).hexdigest(), 16)
```

---

## Модуль 6: Redis - Мастер-класс по производительности

### Архитектура Redis и оптимизации памяти

```bash
# Redis Memory Layout Analysis
redis-cli MEMORY USAGE user:12345
# Output: (integer) 152

# Оптимизация структур данных
# Плохо: отдельные строки для каждого поля
SET user:12345:name "John Doe" 
SET user:12345:email "john@example.com"
SET user:12345:age "30"
# Memory: ~200 bytes per field = 600 bytes total

# Хорошо: Hash для связанных данных  
HSET user:12345 name "John Doe" email "john@example.com" age 30
# Memory: ~80 bytes total

# Анализ памяти по типам данных
redis-cli --bigkeys
redis-cli --memkeys --memkeys-samples 1000

# Memory optimization конфигурация
CONFIG SET hash-max-ziplist-entries 512
CONFIG SET hash-max-ziplist-value 64
CONFIG SET list-max-ziplist-size -2
CONFIG SET set-max-intset-entries 512
CONFIG SET zset-max-ziplist-entries 128
CONFIG SET zset-max-ziplist-value 64
```

### Advanced Redis Data Structures

```python
class RedisOptimizedOperations:
    def __init__(self, redis_client):
        self.redis = redis_client
        
    def efficient_counter(self, key, increment=1):
        """
        Эффективный счетчик с batch операциями
        """
        # Используем pipeline для batch operations
        pipe = self.redis.pipeline()
        pipe.incr(key, increment)
        pipe.expire(key, 3600)  # TTL 1 час
        results = pipe.execute()
        return results[0]
    
    def sliding_window_counter(self, key, window_seconds=300):
        """
        Sliding window counter используя sorted sets
        Подсчет событий за последние N секунд
        """
        now = time.time()
        cutoff = now - window_seconds
        
        pipe = self.redis.pipeline()
        # Удаляем старые записи
        pipe.zremrangebyscore(key, 0, cutoff)
        # Добавляем новую запись
        pipe.zadd(key, {str(uuid.uuid4()): now})
        # Получаем текущий count
        pipe.zcard(key)
        # Устанавливаем TTL
        pipe.expire(key, window_seconds * 2)
        
        results = pipe.execute()
        return results[2]  # Count result
    
    def rate_limiter(self, user_id, limit=100, window=3600):
        """
        Rate limiting using sliding window log
        """
        key = f"rate_limit:{user_id}"
        now = time.time()
        cutoff = now - window
        
        pipe = self.redis.pipeline()
        # Очищаем старые записи
        pipe.zremrangebyscore(key, 0, cutoff)
        # Получаем текущий count
        pipe.zcard(key)
        
        results = pipe.execute()
        current_count = results[1]
        
        if current_count >= limit:
            return False, limit - current_count
        
        # Добавляем новую запись
        pipe = self.redis.pipeline()
        pipe.zadd(key, {str(uuid.uuid4()): now})
        pipe.expire(key, window)
        pipe.execute()
        
        return True, limit - current_count - 1
    
    def distributed_lock(self, lock_key, timeout=10, retry_interval=0.1):
        """
        Distributed lock с автоматическим освобождением
        """
        identifier = str(uuid.uuid4())
        lock_timeout = time.time() + timeout
        
        while time.time() < lock_timeout:
            # Пытаемся получить блокировку
            if self.redis.set(lock_key, identifier, nx=True, ex=timeout):
                return Lock(self.redis, lock_key, identifier)
            
            time.sleep(retry_interval)
        
        return None
    
    def leaderboard_operations(self, leaderboard_key):
        """
        Эффективные операции с leaderboard используя sorted sets
        """
        return LeaderboardManager(self.redis, leaderboard_key)

class LeaderboardManager:
    def __init__(self, redis_client, key):
        self.redis = redis_client
        self.key = key
    
    def add_score(self, user_id, score):
        """Добавить очки пользователю"""
        return self.redis.zincrby(self.key, score, user_id)
    
    def get_user_rank(self, user_id):
        """Получить ранг пользователя (0-based)"""
        rank = self.redis.zrevrank(self.key, user_id)
        return rank + 1 if rank is not None else None
    
    def get_top_users(self, count=10):
        """Получить топ пользователей"""
        return self.redis.zrevrange(self.key, 0, count-1, withscores=True)
    
    def get_users_around(self, user_id, radius=5):
        """Получить пользователей вокруг данного пользователя"""
        rank = self.redis.zrevrank(self.key, user_id)
        if rank is None:
            return []
        
        start = max(0, rank - radius)
        end = rank + radius
        
        return self.redis.zrevrange(self.key, start, end, withscores=True)
    
    def get_percentile(self, user_id):
        """Получить percentile пользователя"""
        total_users = self.redis.zcard(self.key)
        if total_users == 0:
            return None
        
        rank = self.redis.zrevrank(self.key, user_id)
        if rank is None:
            return None
        
        return (1 - rank / total_users) * 100

class RedisClusterOptimizations:
    """
    Оптимизации для Redis Cluster
    """
    def __init__(self, cluster_nodes):
        self.cluster = RedisCluster(startup_nodes=cluster_nodes)
        self.local_cache = {}  # L1 cache
        self.batch_operations = defaultdict(list)
        
    def smart_get(self, key):
        """
        Многоуровневое получение с L1 кэшем
        """
        # L1: Local cache
        if key in self.local_cache:
            entry = self.local_cache[key]
            if time.time() < entry['expires']:
                return entry['value']
            else:
                del self.local_cache[key]
        
        # L2: Redis cluster
        value = self.cluster.get(key)
        
        # Кэшируем локально на короткое время
        if value is not None:
            self.local_cache[key] = {
                'value': value,
                'expires': time.time() + 30  # 30 секунд L1 TTL
            }
        
        return value
    
    def batch_pipeline_operations(self):
        """
        Batch операции по слотам для минимизации network roundtrips
        """
        # Группируем операции по слотам кластера
        slot_operations = defaultdict(list)
        
        for operation in self.batch_operations:
            key = operation['key']
            slot = self.cluster.keyslot(key)
            slot_operations[slot].append(operation)
        
        results = {}
        
        # Выполняем pipeline для каждого слота
        for slot, operations in slot_operations.items():
            node = self.cluster.get_node_by_slot(slot)
            pipe = node.pipeline()
            
            for op in operations:
                if op['type'] == 'get':
                    pipe.get(op['key'])
                elif op['type'] == 'set':
                    pipe.set(op['key'], op['value'], ex=op.get('ttl'))
                # ... other operations
            
            slot_results = pipe.execute()
            
            # Сопоставляем результаты с операциями
            for i, op in enumerate(operations):
                results[op['key']] = slot_results[i]
        
        self.batch_operations.clear()
        return results
```

### Redis Persistence и Performance Tuning

```bash
# RDB Configuration для high-performance
# redis.conf
save 900 1      # Snapshot если минимум 1 ключ изменился за 15 минут
save 300 10     # Snapshot если минимум 10 ключей изменилось за 5 минут  
save 60 10000   # Snapshot если минимум 10000 ключей изменилось за 1 минуту

# Отключение RDB для максимальной производительности (только AOF)
save ""

# AOF Configuration для durability
appendonly yes
appendfsync everysec    # Compromise между производительностью и durability
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# Memory optimization
maxmemory 2gb
maxmemory-policy allkeys-lru  # Вытеснение LRU для всех ключей

# Network optimizations  
tcp-keepalive 300
timeout 0
tcp-backlog 511

# Disable expensive commands in production
rename-command FLUSHDB ""
rename-command FLUSHALL ""
rename-command SHUTDOWN SHUTDOWN_SECRET_KEY

# Performance monitoring
redis-cli --latency-history -i 1
redis-cli --stat
redis-cli --bigkeys --bigkeys-samples 1000000

# Memory analysis
redis-cli --memkeys
redis-cli MEMORY DOCTOR
```

### Redis Lua Scripting для Atomic Operations

```lua
-- Complex atomic increment with limits
-- KEYS[1] = counter key
-- ARGV[1] = increment value  
-- ARGV[2] = max value
-- ARGV[3] = TTL seconds
local current = redis.call('GET', KEYS[1])
if not current then
    current = 0
else
    current = tonumber(current)
end

local increment = tonumber(ARGV[1])
local max_value = tonumber(ARGV[2])
local ttl = tonumber(ARGV[3])

if current + increment > max_value then
    return {current, false}  -- Exceeded limit
end

local new_value = current + increment
redis.call('SET', KEYS[1], new_value)
redis.call('EXPIRE', KEYS[1], ttl)

return {new_value, true}  -- Success

-- Usage:
-- redis.eval(script, 1, "user:123:api_calls", "1", "1000", "3600")
```

```lua
-- Distributed cache-aside pattern with race condition prevention
-- KEYS[1] = cache key
-- KEYS[2] = lock key  
-- ARGV[1] = TTL for cache
-- ARGV[2] = TTL for lock
-- ARGV[3] = lock identifier

local cache_value = redis.call('GET', KEYS[1])
if cache_value then
    return {cache_value, 'hit'}
end

-- Try to acquire lock
local lock_acquired = redis.call('SET', KEYS[2], ARGV[3], 'NX', 'EX', ARGV[2])
if not lock_acquired then
    -- Another process is loading, wait briefly and check cache again
    redis.call('PEXPIRE', KEYS[2], 100)  -- Short wait
    cache_value = redis.call('GET', KEYS[1])
    if cache_value then
        return {cache_value, 'hit_after_wait'}
    else
        return {nil, 'miss_locked'}
    end
end

return {nil, 'miss_lock_acquired'}

-- После загрузки из БД:
-- SET cache_key value EX ttl
-- DEL lock_key
```

---

## Модуль 7: Database кэширование - продвинутые техники

### Query Plan Caching и Optimization

```sql
-- MySQL Query Cache Analysis (deprecated в 8.0+)
SHOW VARIABLES LIKE 'query_cache%';
SHOW STATUS LIKE 'Qcache%';

-- Анализ эффективности query cache
SELECT 
    ROUND(Qcache_hits / (Qcache_hits + Qcache_inserts) * 100, 2) AS hit_ratio,
    Qcache_hits AS hits,
    Qcache_inserts AS misses,
    Qcache_lowmem_prunes AS memory_prunes
FROM (
    SELECT VARIABLE_VALUE AS Qcache_hits 
    FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Qcache_hits'
) hits,
(
    SELECT VARIABLE_VALUE AS Qcache_inserts 
    FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Qcache_inserts'  
) inserts,
(
    SELECT VARIABLE_VALUE AS Qcache_lowmem_prunes
    FROM performance_schema.global_status WHERE VARIABLE_NAME = 'Qcache_lowmem_prunes'
) prunes;

-- PostgreSQL Query Plan Caching
-- Prepared statements автоматически кэшируют планы
PREPARE user_lookup (int) AS 
    SELECT * FROM users WHERE id = $1;

-- Анализ plan cache
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows
FROM pg_stat_statements 
WHERE calls > 100
ORDER BY mean_time DESC;

-- Принудительное использование prepared statements
SET plan_cache_mode = force_generic_plan;
```

### Advanced Connection Pooling

```python
class IntelligentConnectionPool:
    """
    Продвинутый connection pool с адаптивным размером
    и intelligent routing
    """
    def __init__(self, config):
        self.config = config
        self.pools = {}  # database -> pool
        self.metrics = ConnectionMetrics()
        self.load_balancer = DatabaseLoadBalancer()
        
        # Adaptive pool sizing
        self.pool_adjuster = PoolSizeAdjuster()
        self.pool_adjuster.start()
        
        # Health monitoring
        self.health_monitor = DatabaseHealthMonitor()
        self.health_monitor.start()
    
    def get_connection(self, query_type='read', consistency='eventual'):
        """
        Intelligent connection routing основанный на query type
        """
        # Определяем подходящую базу данных
        target_db = self.load_balancer.choose_database(
            query_type=query_type,
            consistency=consistency,
            current_load=self.metrics.get_current_load()
        )
        
        # Получаем connection из соответствующего pool
        pool = self._get_or_create_pool(target_db)
        
        start_time = time.time()
        try:
            connection = pool.get_connection(timeout=self.config.get_timeout)
            
            # Оборачиваем connection для метрик
            wrapped_conn = MetricConnection(
                connection, 
                target_db, 
                self.metrics,
                start_time
            )
            
            return wrapped_conn
            
        except PoolExhaustedException:
            # Попытка получить connection из другой реплики
            fallback_db = self.load_balancer.get_fallback_database(target_db)
            if fallback_db:
                fallback_pool = self._get_or_create_pool(fallback_db)
                return fallback_pool.get_connection(timeout=1)  # Короткий timeout
            
            raise
    
    def _get_or_create_pool(self, database):
        """Ленивое создание pools"""
        if database not in self.pools:
            pool_config = self._calculate_optimal_pool_size(database)
            self.pools[database] = ConnectionPoolImpl(database, pool_config)
        
        return self.pools[database]
    
    def _calculate_optimal_pool_size(self, database):
        """
        Расчет оптимального размера pool на основе метрик
        """
        historical_load = self.metrics.get_historical_load(database)
        current_capacity = self.metrics.get_database_capacity(database)
        
        # Формула: base_size + (peak_load * safety_factor)
        base_size = 5
        peak_load = max(historical_load) if historical_load else 10
        safety_factor = 1.5
        
        optimal_size = base_size + int(peak_load * safety_factor)
        
        # Ограничения
        min_size = 2
        max_size = current_capacity * 0.8  # Не более 80% capacity БД
        
        return max(min_size, min(optimal_size, max_size))

class PoolSizeAdjuster:
    """
    Автоматическая настройка размера connection pools
    """
    def __init__(self, adjustment_interval=300):  # 5 минут
        self.adjustment_interval = adjustment_interval
        self.running = False
        
    def start(self):
        self.running = True
        threading.Thread(target=self._adjustment_loop, daemon=True).start()
    
    def _adjustment_loop(self):
        while self.running:
            try:
                self._adjust_pool_sizes()
            except Exception as e:
                logger.error(f"Pool adjustment error: {e}")
            
            time.sleep(self.adjustment_interval)
    
    def _adjust_pool_sizes(self):
        """
        Анализ метрик и корректировка размеров pools
        """
        for database, pool in connection_pool.pools.items():
            metrics = pool.get_metrics()
            
            # Анализ паттернов использования
            utilization = metrics.average_utilization
            wait_time = metrics.average_wait_time
            timeout_rate = metrics.timeout_rate
            
            current_size = pool.size
            new_size = current_size
            
            # Увеличиваем размер при высокой утилизации
            if utilization > 0.8 and wait_time > 100:  # >100ms wait
                new_size = min(current_size * 1.2, pool.max_size)
                logger.info(f"Increasing pool size for {database}: {current_size} -> {new_size}")
            
            # Уменьшаем размер при низкой утилизации
            elif utilization < 0.3 and wait_time < 10:  # <10ms wait
                new_size = max(current_size * 0.8, pool.min_size)
                logger.info(f"Decreasing pool size for {database}: {current_size} -> {new_size}")
            
            # Применяем изменения
            if new_size != current_size:
                pool.resize(int(new_size))

class DatabaseLoadBalancer:
    """
    Intelligent load balancing между read replicas
    """
    def __init__(self):
        self.replicas = {}  # region -> [replica_configs]
        self.weights = {}   # replica_id -> weight
        self.health_scores = {}  # replica_id -> health_score
        
    def choose_database(self, query_type, consistency, current_load):
        """
        Выбор оптимальной базы данных для запроса
        """
        if query_type == 'write' or consistency == 'strong':
            return self._get_master()
        
        # Для read запросов выбираем лучшую read replica
        available_replicas = self._get_healthy_read_replicas()
        
        if not available_replicas:
            # Fallback to master
            return self._get_master()
        
        # Weighted random selection на основе health и load
        return self._weighted_selection(available_replicas, current_load)
    
    def _weighted_selection(self, replicas, current_load):
        """
        Weighted random selection учитывающий health и текущую нагрузку
        """
        weights = []
        
        for replica in replicas:
            health_score = self.health_scores.get(replica.id, 1.0)
            load_factor = 1.0 / (1.0 + current_load.get(replica.id, 0))
            latency_factor = 1.0 / (1.0 + replica.average_latency / 100)
            
            weight = health_score * load_factor * latency_factor
            weights.append(weight)
        
        # Weighted random choice
        total_weight = sum(weights)
        if total_weight == 0:
            return random.choice(replicas)
        
        r = random.uniform(0, total_weight)
        cumulative_weight = 0
        
        for i, weight in enumerate(weights):
            cumulative_weight += weight
            if r <= cumulative_weight:
                return replicas[i]
        
        return replicas[-1]  # Fallback

class ResultSetCache:
    """
    Кэширование результатов запросов с intelligent invalidation
    """
    def __init__(self, cache_backend):
        self.cache = cache_backend
        self.query_parser = SQLQueryParser()
        self.dependency_tracker = QueryDependencyTracker()
        
    def get_cached_result(self, query, params):
        """
        Получение кэшированного результата запроса
        """
        # Парсим запрос для понимания зависимостей
        parsed_query = self.query_parser.parse(query)
        cache_key = self._generate_cache_key(parsed_query, params)
        
        # Проверяем актуальность кэша
        if self._is_cache_valid(parsed_query, cache_key):
            result = self.cache.get(cache_key)
            if result:
                return result
        
        return None
    
    def cache_result(self, query, params, result, ttl=3600):
        """
        Кэширование результата запроса
        """
        parsed_query = self.query_parser.parse(query)
        cache_key = self._generate_cache_key(parsed_query, params)
        
        # Определяем зависимости запроса (таблицы, индексы)
        dependencies = self._extract_dependencies(parsed_query)
        
        # Сохраняем результат с метаданными
        cache_entry = {
            'result': result,
            'cached_at': time.time(),
            'dependencies': dependencies,
            'query_hash': self._hash_query(parsed_query)
        }
        
        self.cache.set(cache_key, cache_entry, ttl)
        
        # Регистрируем зависимости для invalidation
        self.dependency_tracker.register_cache_entry(cache_key, dependencies)
    
    def invalidate_by_table(self, table_name):
        """
        Инвалидация всех кэшированных запросов для таблицы
        """
        affected_keys = self.dependency_tracker.get_dependent_cache_keys(table_name)
        
        for cache_key in affected_keys:
            self.cache.delete(cache_key)
        
        self.dependency_tracker.remove_dependencies(table_name)
    
    def _generate_cache_key(self, parsed_query, params):
        """
        Генерация стабильного ключа кэша
        """
        # Нормализуем запрос (убираем whitespace, приводим к lowercase)
        normalized_query = self._normalize_query(parsed_query)
        
        # Сортируем параметры для стабильности
        sorted_params = sorted(params.items()) if isinstance(params, dict) else params
        
        key_data = {
            'query': normalized_query,
            'params': sorted_params
        }
        
        return hashlib.sha256(json.dumps(key_data, sort_keys=True).encode()).hexdigest()
    
    def _is_cache_valid(self, parsed_query, cache_key):
        """
        Проверка актуальности кэша на основе dependency tracking
        """
        cache_entry = self.cache.get(cache_key)
        if not cache_entry:
            return False
        
        # Проверяем зависимости
        for table_name in cache_entry['dependencies']:
            last_modified = self.dependency_tracker.get_table_last_modified(table_name)
            if last_modified > cache_entry['cached_at']:
                return False
        
        return True

class SQLQueryParser:
    """
    Парсинг SQL запросов для извлечения зависимостей
    """
    def __init__(self):
        # Паттерны для извлечения таблиц из различных типов запросов
        self.select_pattern = re.compile(r'FROM\s+([a-zA-Z_][a-zA-Z0-9_]*)', re.IGNORECASE)
        self.join_pattern = re.compile(r'JOIN\s+([a-zA-Z_][a-zA-Z0-9_]*)', re.IGNORECASE)
        self.update_pattern = re.compile(r'UPDATE\s+([a-zA-Z_][a-zA-Z0-9_]*)', re.IGNORECASE)
        self.insert_pattern = re.compile(r'INSERT\s+INTO\s+([a-zA-Z_][a-zA-Z0-9_]*)', re.IGNORECASE)
        self.delete_pattern = re.compile(r'DELETE\s+FROM\s+([a-zA-Z_][a-zA-Z0-9_]*)', re.IGNORECASE)
    
    def parse(self, query):
        """
        Парсинг SQL запроса
        """
        return {
            'original': query,
            'normalized': self._normalize(query),
            'type': self._detect_query_type(query),
            'tables': self._extract_tables(query),
            'columns': self._extract_columns(query),
            'conditions': self._extract_conditions(query)
        }
    
    def _extract_tables(self, query):
        """
        Извлечение всех таблиц из запроса
        """
        tables = set()
        
        # SELECT queries
        tables.update(self.select_pattern.findall(query))
        tables.update(self.join_pattern.findall(query))
        
        # DML queries
        tables.update(self.update_pattern.findall(query))
        tables.update(self.insert_pattern.findall(query))
        tables.update(self.delete_pattern.findall(query))
        
        return list(tables)
    
    def _detect_query_type(self, query):
        """
        Определение типа запроса
        """
        query_upper = query.strip().upper()
        
        if query_upper.startswith('SELECT'):
            return 'SELECT'
        elif query_upper.startswith('INSERT'):
            return 'INSERT'
        elif query_upper.startswith('UPDATE'):
            return 'UPDATE'
        elif query_upper.startswith('DELETE'):
            return 'DELETE'
        else:
            return 'UNKNOWN'
```

### Buffer Pool Optimization

```sql
-- InnoDB Buffer Pool Analysis
SELECT 
    ROUND(
        (SELECT VARIABLE_VALUE FROM performance_schema.global_status 
         WHERE VARIABLE_NAME = 'Innodb_buffer_pool_pages_data') /
        (SELECT VARIABLE_VALUE FROM performance_schema.global_status 
         WHERE VARIABLE_NAME = 'Innodb_buffer_pool_pages_total') * 100, 2
    ) AS buffer_pool_utilization,
    
    ROUND(
        (SELECT VARIABLE_VALUE FROM performance_schema.global_status 
         WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests') /
        ((SELECT VARIABLE_VALUE FROM performance_schema.global_status 
          WHERE VARIABLE_NAME = 'Innodb_buffer_pool_read_requests') +
         (SELECT VARIABLE_VALUE FROM performance_schema.global_status 
          WHERE VARIABLE_NAME = 'Innodb_buffer_pool_reads')) * 100, 2
    ) AS buffer_pool_hit_ratio;

-- Buffer Pool Pages by Type
SELECT 
    page_type,
    COUNT(*) as page_count,
    ROUND(COUNT(*) / (SELECT COUNT(*) FROM information_schema.innodb_buffer_page) * 100, 2) as percentage
FROM information_schema.innodb_buffer_page 
GROUP BY page_type 
ORDER BY page_count DESC;

-- Most Cached Tables
SELECT 
    TABLE_SCHEMA,
    TABLE_NAME,
    COUNT(*) as cached_pages,
    ROUND(COUNT(*) * 16 / 1024, 2) as cached_mb
FROM information_schema.innodb_buffer_page ibp
JOIN information_schema.innodb_buffer_page_lru ibl ON ibp.SPACE = ibl.SPACE
GROUP BY TABLE_SCHEMA, TABLE_NAME
ORDER BY cached_pages DESC
LIMIT 20;

-- PostgreSQL Buffer Cache Analysis  
SELECT 
    c.relname,
    pg_size_pretty(count(*) * 8192) as buffered,
    round(100.0 * count(*) / (
        SELECT setting FROM pg_settings WHERE name='shared_buffers'
    )::integer,1) AS buffers_percent,
    round(100.0 * count(*) * 8192 / pg_relation_size(c.oid),1) AS percent_of_relation
FROM pg_class c
INNER JOIN pg_buffercache b ON b.relfilenode = c.relfilenode
INNER JOIN pg_database d ON (b.reldatabase = d.oid AND d.datname = current_database())
GROUP BY c.oid, c.relname
ORDER BY 3 DESC
LIMIT 20;
```

---

## Модуль 8: HTTP кэширование и CDN стратегии

### Advanced HTTP Caching Headers

```python
class SmartHTTPCacheManager:
    """
    Интеллектуальное управление HTTP кэшированием
    """
    def __init__(self):
        self.cache_policies = {
            'static_assets': {
                'max_age': 31536000,  # 1 year
                'immutable': True,
                'public': True
            },
            'api_data': {
                'max_age': 300,  # 5 minutes
                'stale_while_revalidate': 60,
                'stale_if_error': 3600
            },
            'user_content': {
                'max_age': 0,
                'must_revalidate': True,
                'private': True
            },
            'dynamic_content': {
                'max_age': 60,
                'stale_while_revalidate': 30,
                'vary': ['Accept-Encoding', 'User-Agent']
            }
        }
    
    def generate_cache_headers(self, content_type, user_context=None):
        """
        Генерация оптимальных cache headers для контента
        """
        policy = self._determine_cache_policy(content_type, user_context)
        headers = {}
        
        # Cache-Control header
        cache_control_parts = []
        
        if policy.get('public'):
            cache_control_parts.append('public')
        elif policy.get('private'):
            cache_control_parts.append('private')
        
        if policy.get('max_age'):
            cache_control_parts.append(f"max-age={policy['max_age']}")
        
        if policy.get('must_revalidate'):
            cache_control_parts.append('must-revalidate')
        
        if policy.get('no_cache'):
            cache_control_parts.append('no-cache')
        
        if policy.get('no_store'):
            cache_control_parts.append('no-store')
        
        if policy.get('immutable'):
            cache_control_parts.append('immutable')
        
        if policy.get('stale_while_revalidate'):
            cache_control_parts.append(f"stale-while-revalidate={policy['stale_while_revalidate']}")
        
        if policy.get('stale_if_error'):
            cache_control_parts.append(f"stale-if-error={policy['stale_if_error']}")
        
        headers['Cache-Control'] = ', '.join(cache_control_parts)
        
        # ETag generation
        if policy.get('use_etag', True):
            headers['ETag'] = self._generate_etag(content_type, user_context)
        
        # Vary header
        if policy.get('vary'):
            headers['Vary'] = ', '.join(policy['vary'])
        
        # Last-Modified
        if policy.get('use_last_modified', True):
            headers['Last-Modified'] = self._get_last_modified(content_type)
        
        return headers
    
    def _generate_etag(self, content_type, user_context):
        """
        Генерация ETag на основе контента и контекста пользователя
        """
        etag_components = [
            content_type,
            str(user_context.get('user_id', 'anonymous') if user_context else 'anonymous'),
            str(user_context.get('preferences_hash', '') if user_context else ''),
            str(time.time() // 300)  # 5-minute buckets для semi-dynamic content
        ]
        
        etag_string = '|'.join(etag_components)
        etag_hash = hashlib.md5(etag_string.encode()).hexdigest()
        
        return f'"{etag_hash}"'
    
    def handle_conditional_request(self, request_headers, content_etag, content_last_modified):
        """
        Обработка conditional requests (304 Not Modified)
        """
        # If-None-Match (ETag validation)
        if_none_match = request_headers.get('If-None-Match')
        if if_none_match:
            # Поддержка multiple ETags
            request_etags = [etag.strip() for etag in if_none_match.split(',')]
            if content_etag in request_etags or '*' in request_etags:
                return True  # 304 Not Modified
        
        # If-Modified-Since (Last-Modified validation)
        if_modified_since = request_headers.get('If-Modified-Since')
        if if_modified_since and content_last_modified:
            try:
                request_time = email.utils.parsedate_to_datetime(if_modified_since)
                content_time = email.utils.parsedate_to_datetime(content_last_modified)
                
                if content_time <= request_time:
                    return True  # 304 Not Modified
            except (ValueError, TypeError):
                pass  # Ignore invalid dates
        
        return False  # Content should be sent

class CDNOptimizationManager:
    """
    Оптимизация контента для CDN
    """
    def __init__(self, cdn_config):
        self.cdn_config = cdn_config
        self.edge_locations = cdn_config.get('edge_locations', [])
        self.origin_shield = cdn_config.get('origin_shield', False)
        
    def optimize_for_cdn(self, content, content_type):
        """
        Оптимизация контента для CDN кэширования
        """
        optimizations = []
        
        # Compression optimization
        if self._should_compress(content_type):
            compressed_content = self._compress_content(content)
            optimizations.append({
                'type': 'compression',
                'original_size': len(content),
                'compressed_size': len(compressed_content),
                'compression_ratio': len(compressed_content) / len(content)
            })
            content = compressed_content
        
        # Image optimization
        if content_type.startswith('image/'):
            optimized_content = self._optimize_image(content, content_type)
            optimizations.append({
                'type': 'image_optimization',
                'original_size': len(content),
                'optimized_size': len(optimized_content)
            })
            content = optimized_content
        
        # Minification for text assets
        if content_type in ['text/css', 'application/javascript', 'text/html']:
            minified_content = self._minify_content(content, content_type)
            optimizations.append({
                'type': 'minification',
                'original_size': len(content),
                'minified_size': len(minified_content)
            })
            content = minified_content
        
        return content, optimizations
    
    def generate_cdn_cache_keys(self, url, user_context=None):
        """
        Генерация ключей кэша для CDN с учетом персонализации
        """
        base_key = self._normalize_url(url)
        
        variations = [base_key]  # Base version
        
        # Device-specific variations
        if user_context and user_context.get('device_type'):
            device_key = f"{base_key}?device={user_context['device_type']}"
            variations.append(device_key)
        
        # Geographic variations
        if user_context and user_context.get('country'):
            geo_key = f"{base_key}?geo={user_context['country']}"
            variations.append(geo_key)
        
        # Language variations
        if user_context and user_context.get('language'):
            lang_key = f"{base_key}?lang={user_context['language']}"
            variations.append(lang_key)
        
        return variations
    
    def cache_warmup_strategy(self, popular_urls):
        """
        Стратегия предварительного прогрева CDN кэша
        """
        warmup_plan = []
        
        for url in popular_urls:
            # Приоритезируем edge locations по географии пользователей
            priority_edges = self._get_priority_edges_for_url(url)
            
            for edge in priority_edges[:5]:  # Top 5 edges
                warmup_plan.append({
                    'url': url,
                    'edge_location': edge,
                    'priority': self._calculate_warmup_priority(url, edge),
                    'estimated_traffic': self._estimate_traffic(url, edge)
                })
        
        # Сортируем по приоритету
        warmup_plan.sort(key=lambda x: x['priority'], reverse=True)
        
        return warmup_plan
    
    def intelligent_purging(self, changed_content):
        """
        Интеллектуальная очистка CDN кэша
        """
        purge_operations = []
        
        for content_item in changed_content:
            # Определяем зависимые URLs
            dependent_urls = self._find_dependent_urls(content_item)
            
            # Группируем по типу purge операции
            if content_item['urgency'] == 'high':
                # Immediate purge
                purge_operations.append({
                    'type': 'immediate',
                    'urls': dependent_urls,
                    'estimated_cost': len(dependent_urls) * 0.005  # $0.005 per purge
                })
            else:
                # Batch purge (cheaper but slower)
                purge_operations.append({
                    'type': 'batch',
                    'urls': dependent_urls,
                    'delay': 300,  # 5 minutes
                    'estimated_cost': len(dependent_urls) * 0.001
                })
        
        return purge_operations

class EdgeSideIncludes:
    """
    Реализация Edge Side Includes для dynamic content caching
    """
    def __init__(self):
        self.esi_processor = ESIProcessor()
        
    def process_template(self, template, user_context):
        """
        Обработка ESI директив в template
        """
        # Парсим ESI tags
        esi_tags = self._parse_esi_tags(template)
        
        processed_content = template
        
        for tag in esi_tags:
            if tag['type'] == 'include':
                # Обрабатываем <esi:include src="..." />
                included_content = self._process_include(tag, user_context)
                processed_content = processed_content.replace(
                    tag['original'], 
                    included_content
                )
            
            elif tag['type'] == 'choose':
                # Обрабатываем <esi:choose> conditions
                chosen_content = self._process_choose(tag, user_context)
                processed_content = processed_content.replace(
                    tag['original'],
                    chosen_content
                )
        
        return processed_content
    
    def _process_include(self, include_tag, user_context):
        """
        Обработка <esi:include> директивы
        """
        src_url = include_tag['src']
        
        # Параметры для персонализации
        cache_key_params = []
        
        if include_tag.get('cache_key'):
            cache_key_params.append(include_tag['cache_key'])
        
        if user_context:
            if user_context.get('user_segment'):
                cache_key_params.append(f"segment={user_context['user_segment']}")
            
            if user_context.get('ab_test_variant'):
                cache_key_params.append(f"variant={user_context['ab_test_variant']}")
        
        # Формируем URL с параметрами кэширования
        cache_key = '&'.join(cache_key_params)
        if cache_key:
            separator = '&' if '?' in src_url else '?'
            full_url = f"{src_url}{separator}{cache_key}"
        else:
            full_url = src_url
        
        # Запрашиваем контент (это будет кэшироваться отдельно)
        try:
            return self._fetch_include_content(full_url, include_tag.get('ttl', 300))
        except Exception as e:
            # Fallback content при ошибке
            return include_tag.get('alt', '<!-- ESI Include Error -->')
    
    def _process_choose(self, choose_tag, user_context):
        """
        Обработка <esi:choose> условной логики
        """
        conditions = choose_tag['conditions']
        
        for condition in conditions:
            if self._evaluate_condition(condition['when'], user_context):
                return condition['content']
        
        # Default case
        return choose_tag.get('otherwise', '')
    
    def _evaluate_condition(self, condition_expr, user_context):
        """
        Вычисление ESI условий
        """
        # Простая реализация ESI условий
        # В production следует использовать более безопасный parser
        
        # Заменяем переменные из user_context
        evaluated_expr = condition_expr
        
        if user_context:
            for key, value in user_context.items():
                placeholder = f"$(HTTP_{key.upper()})"
                evaluated_expr = evaluated_expr.replace(placeholder, str(value))
        
        # Безопасное вычисление простых условий
        try:
            # Только простые сравнения для безопасности
            if '==' in evaluated_expr:
                left, right = evaluated_expr.split('==', 1)
                return left.strip().strip('"\'') == right.strip().strip('"\'')
            elif '!=' in evaluated_expr:
                left, right = evaluated_expr.split('!=', 1)
                return left.strip().strip('"\'') != right.strip().strip('"\'')
        except:
            pass
        
        return False

# Nginx ESI Configuration Example
nginx_esi_config = """
location / {
    # Enable ESI processing
    ssi on;
    ssi_types text/html;
    
    # Cache static parts
    location ~* \.(css|js|png|jpg|jpeg|gif|ico|svg)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
    
    # Dynamic ESI includes
    location /esi/ {
        proxy_pass http://backend;
        proxy_cache esi_cache;
        proxy_cache_key "$request_uri$cookie_user_segment";
        proxy_cache_valid 200 5m;
        proxy_cache_bypass $http_cache_control;
        
        # ESI-specific headers
        proxy_set_header X-ESI-Enabled "true";
        proxy_set_header X-User-Segment $cookie_user_segment;
    }
}

# ESI Cache zone
proxy_cache_path /var/cache/nginx/esi levels=1:2 keys_zone=esi_cache:10m max_size=100m inactive=60m;
"""
```

---

## Модуль 9: Архитектурные паттерны высокого уровня

### Multi-Level Cache Hierarchy

```python
class HierarchicalCacheSystem:
    """
    Многоуровневая система кэширования с intelligent routing
    """
    def __init__(self):
        self.levels = [
            # L1: CPU Cache симуляция (в реальности managed автоматически)
            ProcessorCache(size_kb=32, latency_ns=1),
            
            # L2: Application Memory Cache
            LocalMemoryCache(size_mb=100, latency_ms=0.1),
            
            # L3: Local SSD Cache  
            LocalSSDCache(size_gb=10, latency_ms=1),
            
            # L4: Distributed Memory Cache (Redis/Memcached)
            DistributedCache(size_gb=100, latency_ms=5),
            
            # L5: Distributed SSD Cache
            DistributedSSDCache(size_gb=1000, latency_ms=20),
            
            # L6: Database Query Cache
            DatabaseCache(latency_ms=50),
            
            # L7: Cold Storage (S3, etc.)
            ColdStorage(latency_ms=200)
        ]
        
        self.cache_manager = CacheHierarchyManager(self.levels)
        self.access_predictor = AccessPatternPredictor()
        self.cost_optimizer = CostOptimizer()
    
    def get(self, key, access_context=None):
        """
        Интеллектуальное получение данных с multi-level promotion
        """
        access_start = time.time()
        
        # Предсказываем вероятность будущих обращений
        access_probability = self.access_predictor.predict_future_access(
            key, access_context
        )
        
        # Проходим по уровням кэша
        for level_idx, cache_level in enumerate(self.levels):
            try:
                value = cache_level.get(key)
                if value is not None:
                    access_time = time.time() - access_start
                    
                    # Записываем метрики
                    self.cache_manager.record_hit(level_idx, key, access_time)
                    
                    # Продвижение на более быстрые уровни (cache promotion)
                    self._promote_to_faster_levels(key, value, level_idx, access_probability)
                    
                    return value
                    
            except CacheUnavailableException:
                # Уровень недоступен, пробуем следующий
                continue
        
        # Cache miss на всех уровнях
        self.cache_manager.record_miss(key)
        return None
    
    def put(self, key, value, access_context=None):
        """
        Интеллектуальное размещение данных по уровням кэша
        """
        value_size = self._estimate_size(value)
        access_probability = self.access_predictor.predict_future_access(key, access_context)
        
        # Определяем оптимальные уровни для размещения
        target_levels = self.cost_optimizer.optimize_placement(
            key, value_size, access_probability
        )
        
        # Размещаем на выбранных уровнях
        for level_idx in target_levels:
            try:
                self.levels[level_idx].put(key, value)
                self.cache_manager.record_placement(level_idx, key, value_size)
            except CacheFull:
                # Вытесняем менее важные данные
                self._evict_least_valuable(level_idx, value_size)
                self.levels[level_idx].put(key, value)
    
    def _promote_to_faster_levels(self, key, value, current_level, access_probability):
        """
        Продвижение популярных данных на более быстрые уровни
        """
        # Продвигаем только если высокая вероятность повторного доступа
        if access_probability < 0.7:
            return
        
        # Продвигаем на 1-2 уровня выше максимум
        promotion_levels = min(2, current_level)
        
        for level_idx in range(current_level - promotion_levels, current_level):
            if level_idx >= 0:
                try:
                    self.levels[level_idx].put(key, value)
                except CacheFull:
                    # Продвижение не критично, пропускаем
                    pass

class AccessPatternPredictor:
    """
    ML-based предсказание паттернов доступа к данным
    """
    def __init__(self):
        self.access_history = defaultdict(list)  # key -> [access_times]
        self.feature_extractor = AccessFeatureExtractor()
        self.model = AccessPredictionModel()
        
        # Периодическое обучение модели
        self.training_scheduler = TrainingScheduler()
        self.training_scheduler.start()
    
    def record_access(self, key, access_context=None):
        """
        Запись обращения к данным
        """
        access_record = {
            'timestamp': time.time(),
            'key': key,
            'context': access_context or {}
        }
        
        self.access_history[key].append(access_record)
        
        # Ограничиваем историю (скользящее окно)
        cutoff_time = time.time() - 86400 * 7  # 7 дней
        self.access_history[key] = [
            record for record in self.access_history[key]
            if record['timestamp'] > cutoff_time
        ]
    
    def predict_future_access(self, key, access_context=None):
        """
        Предсказание вероятности обращения в ближайшее время
        """
        if key not in self.access_history:
            # Новый ключ - используем контекстные подсказки
            return self._predict_for_new_key(key, access_context)
        
        # Извлекаем признаки из истории доступа
        features = self.feature_extractor.extract_features(
            self.access_history[key], access_context
        )
        
        # Предсказываем вероятность
        probability = self.model.predict_access_probability(features)
        
        return probability
    
    def _predict_for_new_key(self, key, access_context):
        """
        Предсказание для новых ключей на основе контекста
        """
        if not access_context:
            return 0.5  # Default probability
        
        # Анализируем похожие ключи
        similar_keys = self._find_similar_keys(key, access_context)
        
        if similar_keys:
            # Усредняем вероятности для похожих ключей
            probabilities = [
                self.predict_future_access(similar_key, access_context)
                for similar_key in similar_keys
            ]
            return sum(probabilities) / len(probabilities)
        
        return 0.5

class CostOptimizer:
    """
    Оптимизация размещения данных по уровням кэша с учетом стоимости
    """
    def __init__(self):
        # Стоимость хранения на разных уровнях ($ per GB per hour)
        self.storage_costs = {
            0: 10.0,    # L1: Очень дорого (CPU cache simulation)
            1: 5.0,     # L2: Application memory
            2: 1.0,     # L3: Local SSD
            3: 0.5,     # L4: Distributed memory
            4: 0.1,     # L5: Distributed SSD
            5: 0.05,    # L6: Database cache
            6: 0.01     # L7: Cold storage
        }
        
        # Латентность доступа (ms)
        self.access_latencies = {
            0: 0.001, 1: 0.1, 2: 1.0, 3: 5.0, 4: 20.0, 5: 50.0, 6: 200.0
        }
        
        # Ёмкость уровней (GB)
        self.capacities = {
            0: 0.001, 1: 0.1, 2: 10, 3: 100, 4: 1000, 5: 10000, 6: float('inf')
        }
    
    def optimize_placement(self, key, value_size_bytes, access_probability):
        """
        Определение оптимальных уровней размещения
        """
        value_size_gb = value_size_bytes / (1024**3)
        
        # Вычисляем value функцию для каждого уровня
        level_values = {}
        
        for level_idx in range(len(self.storage_costs)):
            # Выгода от размещения на уровне
            access_time_saved = self._calculate_access_time_saved(level_idx)
            expected_accesses = access_probability * 100  # Expected accesses per hour
            time_benefit = access_time_saved * expected_accesses
            
            # Стоимость размещения
            storage_cost = self.storage_costs[level_idx] * value_size_gb
            
            # Проверяем, помещается ли в capacity
            if value_size_gb > self.capacities[level_idx]:
                level_values[level_idx] = -float('inf')  # Не помещается
            else:
                # Value = Benefit - Cost
                level_values[level_idx] = time_benefit - storage_cost
        
        # Выбираем уровни с положительным value
        beneficial_levels = [
            level for level, value in level_values.items() 
            if value > 0
        ]
        
        # Сортируем по убыванию value
        beneficial_levels.sort(key=lambda x: level_values[x], reverse=True)
        
        # Возвращаем топ-3 уровня или все beneficial
        return beneficial_levels[:3]
    
    def _calculate_access_time_saved(self, level_idx):
        """
        Расчет экономии времени при размещении на данном уровне
        """
        current_latency = self.access_latencies[level_idx]
        
        # Сравниваем с worst-case latency (cold storage)
        worst_case_latency = self.access_latencies[6]
        
        return worst_case_latency - current_latency

class CacheCoherencyManager:
    """
    Управление консистентностью между уровнями кэша
    """
    def __init__(self, cache_levels):
        self.cache_levels = cache_levels
        self.invalidation_queue = queue.Queue()
        self.coherency_protocol = CoherencyProtocol()
        
        # Background invalidation worker
        self.invalidation_worker = threading.Thread(
            target=self._process_invalidations,
            daemon=True
        )
        self.invalidation_worker.start()
    
    def invalidate_key(self, key, invalidation_reason='data_changed'):
        """
        Инвалидация ключа на всех уровнях кэша
        """
        invalidation_task = {
            'key': key,
            'reason': invalidation_reason,
            'timestamp': time.time(),
            'levels': list(range(len(self.cache_levels)))
        }
        
        self.invalidation_queue.put(invalidation_task)
    
    def invalidate_pattern(self, key_pattern, invalidation_reason='pattern_invalidation'):
        """
        Инвалидация по паттерну ключей
        """
        for level_idx, cache_level in enumerate(self.cache_levels):
            matching_keys = cache_level.find_keys_by_pattern(key_pattern)
            
            for key in matching_keys:
                invalidation_task = {
                    'key': key,
                    'reason': invalidation_reason,
                    'timestamp': time.time(),
                    'levels': [level_idx]
                }
                self.invalidation_queue.put(invalidation_task)
    
    def _process_invalidations(self):
        """
        Background обработка invalidation задач
        """
        while True:
            try:
                task = self.invalidation_queue.get(timeout=1)
                
                for level_idx in task['levels']:
                    try:
                        self.cache_levels[level_idx].delete(task['key'])
                        
                        # Логируем инвалидацию
                        logger.info(f"Invalidated {task['key']} on level {level_idx}, reason: {task['reason']}")
                        
                    except Exception as e:
                        logger.error(f"Failed to invalidate {task['key']} on level {level_idx}: {e}")
                
                self.invalidation_queue.task_done()
                
            except queue.Empty:
                continue  # Continue processing
            except Exception as e:
                logger.error(f"Invalidation worker error: {e}")
```

### Cache-Aside с Circuit Breaker

```python
class RobustCacheAside:
    """
    Cache-Aside паттерн с circuit breaker и graceful degradation
    """
    def __init__(self, cache, database, circuit_breaker_config=None):
        self.cache = cache
        self.database = database
        
        # Circuit breakers для различных компонентов
        self.cache_circuit_breaker = CircuitBreaker(
            failure_threshold=5,
            recovery_timeout=30,
            expected_exception=CacheException
        )
        
        self.db_circuit_breaker = CircuitBreaker(
            failure_threshold=3,
            recovery_timeout=60,
            expected_exception=DatabaseException
        )
        
        # Fallback mechanisms
        self.fallback_cache = LocalMemoryCache(max_size=1000)
        self.request_coalescing = RequestCoalescer()
        
        # Metrics and monitoring
        self.metrics = CacheMetrics()
        self.health_checker = HealthChecker([cache, database])
    
    def get(self, key, fallback_factory=None):
        """
        Robust получение данных с multiple fallbacks
        """
        # Попытка получить из основного кэша
        try:
            with self.cache_circuit_breaker:
                value = self.cache.get(key)
                if value is not None:
                    self.metrics.record_cache_hit('primary')
                    return value
        except CircuitBreakerOpenException:
            self.metrics.record_cache_miss('primary_circuit_open')
        except CacheException as e:
            self.metrics.record_cache_error('primary', e)
        
        # Fallback 1: Local memory cache
        try:
            value = self.fallback_cache.get(key)
            if value is not None:
                self.metrics.record_cache_hit('fallback')
                return value
        except Exception as e:
            self.metrics.record_cache_error('fallback', e)
        
        # Fallback 2: Request coalescing для популярных ключей
        if self._is_popular_key(key):
            return self.request_coalescing.get_or_load(
                key, 
                lambda: self._load_from_database(key)
            )
        
        # Fallback 3: Direct database access
        return self._load_from_database(key, fallback_factory)
    
    def set(self, key, value):
        """
        Robust установка данных с write-through на доступные системы
        """
        errors = []
        success_count = 0
        
        # Database write (most important)
        try:
            with self.db_circuit_breaker:
                self.database.set(key, value)
                success_count += 1
        except CircuitBreakerOpenException:
            errors.append("Database circuit breaker open")
        except DatabaseException as e:
            errors.append(f"Database error: {e}")
        
        # Cache write (best effort)
        try:
            with self.cache_circuit_breaker:
                self.cache.set(key, value)
                success_count += 1
        except CircuitBreakerOpenException:
            # Cache недоступен, но это не критично для write операций
            pass
        except CacheException as e:
            errors.append(f"Cache error: {e}")
        
        # Fallback cache write (always available)
        try:
            self.fallback_cache.set(key, value)
            success_count += 1
        except Exception as e:
            errors.append(f"Fallback cache error: {e}")
        
        # Проверяем успешность операции
        if success_count == 0:
            raise Exception(f"All write operations failed: {'; '.join(errors)}")
        
        self.metrics.record_write_operation(success_count, len(errors))
        
        if errors:
            logger.warning(f"Partial write failure for key {key}: {'; '.join(errors)}")
    
    def _load_from_database(self, key, fallback_factory=None):
        """
        Загрузка из базы данных с fallback механизмами
        """
        try:
            with self.db_circuit_breaker:
                value = self.database.get(key)
                
                if value is not None:
                    # Асинхронно обновляем кэши
                    self._async_cache_update(key, value)
                    self.metrics.record_database_hit()
                    return value
                
        except CircuitBreakerOpenException:
            self.metrics.record_database_circuit_open()
        except DatabaseException as e:
            self.metrics.record_database_error(e)
        
        # Последний fallback
        if fallback_factory:
            try:
                fallback_value = fallback_factory(key)
                if fallback_value is not None:
                    # Кэшируем fallback значение на короткое время
                    self.fallback_cache.set(key, fallback_value, ttl=60)
                    self.metrics.record_fallback_used()
                    return fallback_value
            except Exception as e:
                logger.error(f"Fallback factory failed for key {key}: {e}")
        
        self.metrics.record_total_miss()
        return None
    
    def _async_cache_update(self, key, value):
        """
        Асинхронное обновление кэшей после database load
        """
        def update_caches():
            # Обновляем основной кэш
            try:
                if not self.cache_circuit_breaker.is_open():
                    self.cache.set(key, value)
            except Exception:
                pass  # Не критично
            
            # Обновляем fallback кэш
            try:
                self.fallback_cache.set(key, value)
            except Exception:
                pass  # Не критично
        
        threading.Thread(target=update_caches, daemon=True).start()

class CircuitBreaker:
    """
    Circuit Breaker паттерн для graceful degradation
    """
    def __init__(self, failure_threshold=5, recovery_timeout=60, expected_exception=Exception):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
        
        self.failure_count = 0
        self.last_failure_time = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN
        
        self.lock = threading.Lock()
    
    def __enter__(self):
        with self.lock:
            if self.state == 'OPEN':
                if self._should_attempt_reset():
                    self.state = 'HALF_OPEN'
                else:
                    raise CircuitBreakerOpenException("Circuit breaker is OPEN")
        
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        with self.lock:
            if exc_type and issubclass(exc_type, self.expected_exception):
                self._record_failure()
            else:
                self._record_success()
        
        return False  # Don't suppress exceptions
    
    def _record_failure(self):
        """Регистрация неудачной операции"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = 'OPEN'
            logger.warning(f"Circuit breaker opened after {self.failure_count} failures")
    
    def _record_success(self):
        """Регистрация успешной операции"""
        if self.state == 'HALF_OPEN':
            self.state = 'CLOSED'
            logger.info("Circuit breaker closed after successful operation")
        
        self.failure_count = 0
        self.last_failure_time = None
    
    def _should_attempt_reset(self):
        """Проверка, можно ли попробовать reset circuit breaker"""
        if not self.last_failure_time:
            return True
        
        return time.time() - self.last_failure_time >= self.recovery_timeout
    
    @property
    def is_open(self):
        return self.state == 'OPEN'

class RequestCoalescer:
    """
    Request coalescing для предотвращения thundering herd
    """
    def __init__(self):
        self.in_flight_requests = {}  # key -> Future
        self.lock = threading.Lock()
    
    def get_or_load(self, key, loader_func):
        """
        Получение значения с coalescing одинаковых запросов
        """
        with self.lock:
            # Проверяем, есть ли уже запрос в процессе
            if key in self.in_flight_requests:
                future = self.in_flight_requests[key]
                # Освобождаем lock и ждем результата
                
        # Если есть in-flight request, ждем его завершения
        if key in self.in_flight_requests:
            try:
                return future.result(timeout=10)  # 10 second timeout
            except concurrent.futures.TimeoutError:
                # Timeout - делаем собственный запрос
                pass
            except Exception:
                # Ошибка в другом запросе - делаем собственный
                pass
        
        # Создаем новый запрос
        with self.lock:
            # Double-check после получения lock
            if key in self.in_flight_requests:
                future = self.in_flight_requests[key]
                try:
                    return future.result(timeout=10)
                except:
                    pass  # Fallback to creating new request
            
            # Создаем Future для этого запроса
            executor = concurrent.futures.ThreadPoolExecutor(max_workers=1)
            future = executor.submit(loader_func)
            self.in_flight_requests[key] = future
        
        try:
            result = future.result()
            return result
        finally:
            # Убираем completed request из in-flight
            with self.lock:
                self.in_flight_requests.pop(key, None)
```

---

## Модуль 10: Мониторинг и обеспечение качества

### Comprehensive Cache Monitoring

```python
class CacheMetricsCollector:
    """
    Комплексный сбор метрик кэширования для мониторинга
    """
    def __init__(self, cache_instance, prometheus_registry=None):
        self.cache = cache_instance
        self.start_time = time.time()
        
        # Prometheus metrics
        if prometheus_registry:
            self.hit_counter = prometheus_client.Counter(
                'cache_hits_total',
                'Total cache hits',
                ['cache_name', 'cache_level'],
                registry=prometheus_registry
            )
            
            self.miss_counter = prometheus_client.Counter(
                'cache_misses_total', 
                'Total cache misses',
                ['cache_name', 'cache_level'],
                registry=prometheus_registry
            )
            
            self.latency_histogram = prometheus_client.Histogram(
                'cache_operation_duration_seconds',
                'Cache operation latency',
                ['cache_name', 'operation'],
                buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0],
                registry=prometheus_registry
            )
            
            self.memory_gauge = prometheus_client.Gauge(
                'cache_memory_usage_bytes',
                'Cache memory usage in bytes',
                ['cache_name'],
                registry=prometheus_registry
            )
            
            self.eviction_counter = prometheus_client.Counter(
                'cache_evictions_total',
                'Total cache evictions',
                ['cache_name', 'eviction_reason'],
                registry=prometheus_registry
            )
        
        # Internal metrics storage
        self.metrics_buffer = collections.deque(maxlen=10000)
        self.aggregated_metrics = {}
        
        # Background metrics processor
        self.metrics_processor = threading.Thread(
            target=self._process_metrics_loop,
            daemon=True
        )
        self.metrics_processor.start()
    
    def record_operation(self, operation_type, key, latency_ms, hit=None):
        """
        Запись операции кэша с полными метриками
        """
        timestamp = time.time()
        
        metric_record = {
            'timestamp': timestamp,
            'operation': operation_type,
            'key': key,
            'latency_ms': latency_ms,
            'hit': hit,
            'cache_size': self._get_cache_size(),
            'memory_usage': self._get_memory_usage()
        }
        
        self.metrics_buffer.append(metric_record)
        
        # Update Prometheus metrics
        if hasattr(self, 'latency_histogram'):
            self.latency_histogram.labels(
                cache_name=self.cache.name,
                operation=operation_type
            ).observe(latency_ms / 1000.0)
            
            if hit is not None:
                if hit:
                    self.hit_counter.labels(
                        cache_name=self.cache.name,
                        cache_level=self.cache.level
                    ).inc()
                else:
                    self.miss_counter.labels(
                        cache_name=self.cache.name,
                        cache_level=self.cache.level
                    ).inc()
    
    def get_real_time_metrics(self):
        """
        Получение метрик в реальном времени
        """
        recent_records = [
            record for record in self.metrics_buffer
            if time.time() - record['timestamp'] <= 60  # Last minute
        ]
        
        if not recent_records:
            return self._empty_metrics()
        
        # Calculate hit ratio
        hits = sum(1 for r in recent_records if r.get('hit') is True)
        misses = sum(1 for r in recent_records if r.get('hit') is False)
        total_requests = hits + misses
        hit_ratio = hits / total_requests if total_requests > 0 else 0
        
        # Calculate average latency
        latencies = [r['latency_ms'] for r in recent_records]
        avg_latency = sum(latencies) / len(latencies) if latencies else 0
        
        # Calculate percentiles
        sorted_latencies = sorted(latencies)
        p50 = self._percentile(sorted_latencies, 50)
        p95 = self._percentile(sorted_latencies, 95)
        p99 = self._percentile(sorted_latencies, 99)
        
        # Operations per second
        ops_per_second = len(recent_records) / 60.0
        
        return {
            'timestamp': time.time(),
            'hit_ratio': hit_ratio,
            'miss_ratio': 1 - hit_ratio,
            'avg_latency_ms': avg_latency,
            'p50_latency_ms': p50,
            'p95_latency_ms': p95,
            'p99_latency_ms': p99,
            'ops_per_second': ops_per_second,
            'cache_size': recent_records[-1]['cache_size'] if recent_records else 0,
            'memory_usage_bytes': recent_records[-1]['memory_usage'] if recent_records else 0,
            'uptime_seconds': time.time() - self.start_time
        }
    
    def get_historical_trends(self, time_range_hours=24):
        """
        Анализ трендов за указанный период
        """
        cutoff_time = time.time() - (time_range_hours * 3600)
        historical_records = [
            record for record in self.metrics_buffer
            if record['timestamp'] >= cutoff_time
        ]
        
        if not historical_records:
            return {}
        
        # Group by hour for trend analysis
        hourly_metrics = defaultdict(list)
        
        for record in historical_records:
            hour_bucket = int(record['timestamp'] // 3600) * 3600
            hourly_metrics[hour_bucket].append(record)
        
        trends = {}
        for hour, records in hourly_metrics.items():
            hits = sum(1 for r in records if r.get('hit') is True)
            misses = sum(1 for r in records if r.get('hit') is False)
            total = hits + misses
            
            trends[hour] = {
                'hit_ratio': hits / total if total > 0 else 0,
                'avg_latency': sum(r['latency_ms'] for r in records) / len(records),
                'operations': len(records),
                'memory_usage': records[-1]['memory_usage'] if records else 0
            }
        
        return trends
    
    def detect_anomalies(self):
        """
        Детекция аномалий в работе кэша
        """
        recent_metrics = self.get_real_time_metrics()
        historical_trends = self.get_historical_trends(24)
        
        anomalies = []
        
        # Анализ hit ratio
        if historical_trends:
            avg_historical_hit_ratio = sum(
                trend['hit_ratio'] for trend in historical_trends.values()
            ) / len(historical_trends)
            
            if recent_metrics['hit_ratio'] < avg_historical_hit_ratio * 0.7:
                anomalies.append({
                    'type': 'low_hit_ratio',
                    'severity': 'warning',
                    'message': f"Hit ratio dropped to {recent_metrics['hit_ratio']:.2%} (historical avg: {avg_historical_hit_ratio:.2%})",
                    'current_value': recent_metrics['hit_ratio'],
                    'expected_value': avg_historical_hit_ratio
                })
        
        # Анализ latency
        if recent_metrics['p95_latency_ms'] > 100:  # 100ms threshold
            anomalies.append({
                'type': 'high_latency',
                'severity': 'warning' if recent_metrics['p95_latency_ms'] < 500 else 'critical',
                'message': f"P95 latency is {recent_metrics['p95_latency_ms']:.1f}ms",
                'current_value': recent_metrics['p95_latency_ms'],
                'threshold': 100
            })
        
        # Анализ memory usage
        if recent_metrics['memory_usage_bytes'] > self.cache.max_memory * 0.9:
            anomalies.append({
                'type': 'high_memory_usage',
                'severity': 'warning',
                'message': f"Memory usage is at {recent_metrics['memory_usage_bytes'] / self.cache.max_memory:.1%}",
                'current_value': recent_metrics['memory_usage_bytes'],
                'threshold': self.cache.max_memory * 0.9
            })
        
        return anomalies
    
    def _process_metrics_loop(self):
        """
        Background обработка и агрегация метрик
        """
        while True:
            try:
                # Обновляем агрегированные метрики каждые 30 секунд
                time.sleep(30)
                
                # Вычисляем скользящие средние
                self._update_moving_averages()
                
                # Обновляем Prometheus gauges
                if hasattr(self, 'memory_gauge'):
                    self.memory_gauge.labels(
                        cache_name=self.cache.name
                    ).set(self._get_memory_usage())
                
                # Детекция аномалий
                anomalies = self.detect_anomalies()
                if anomalies:
                    self._handle_anomalies(anomalies)
                    
            except Exception as e:
                logger.error(f"Metrics processing error: {e}")

class CacheHealthChecker:
    """
    Проверка health кэша и автоматическое восстановление
    """
    def __init__(self, cache_instances):
        self.caches = cache_instances
        self.health_status = {}
        self.recovery_strategies = {
            'connection_failure': self._recover_connection,
            'memory_pressure': self._recover_memory_pressure,
            'performance_degradation': self._recover_performance,
            'data_corruption': self._recover_data_corruption
        }
        
        # Health check scheduler
        self.scheduler = threading.Thread(target=self._health_check_loop, daemon=True)
        self.scheduler.start()
    
    def check_cache_health(self, cache):
        """
        Комплексная проверка health кэша
        """
        health_report = {
            'cache_name': cache.name,
            'timestamp': time.time(),
            'status': 'healthy',
            'issues': [],
            'metrics': {}
        }
        
        try:
            # Test 1: Basic connectivity
            start_time = time.time()
            test_key = f"health_check_{int(time.time())}"
            test_value = "health_check_value"
            
            cache.set(test_key, test_value, ttl=60)
            retrieved_value = cache.get(test_key)
            cache.delete(test_key)
            
            connectivity_latency = (time.time() - start_time) * 1000
            
            if retrieved_value != test_value:
                health_report['issues'].append({
                    'type': 'data_integrity',
                    'severity': 'critical',
                    'message': 'Retrieved value does not match stored value'
                })
                health_report['status'] = 'unhealthy'
            
            health_report['metrics']['connectivity_latency_ms'] = connectivity_latency
            
            # Test 2: Performance check
            if connectivity_latency > 50:  # 50ms threshold
                health_report['issues'].append({
                    'type': 'performance_degradation',
                    'severity': 'warning' if connectivity_latency < 200 else 'critical',
                    'message': f'High latency: {connectivity_latency:.1f}ms'
                })
                
                if connectivity_latency > 200:
                    health_report['status'] = 'unhealthy'
            
            # Test 3: Memory usage check
            try:
                memory_info = cache.get_memory_info()
                health_report['metrics']['memory_usage_percent'] = memory_info.get('usage_percent', 0)
                
                if memory_info.get('usage_percent', 0) > 90:
                    health_report['issues'].append({
                        'type': 'memory_pressure',
                        'severity': 'critical',
                        'message': f"Memory usage at {memory_info['usage_percent']:.1f}%"
                    })
                    health_report['status'] = 'unhealthy'
                    
            except Exception as e:
                health_report['issues'].append({
                    'type': 'monitoring_failure',
                    'severity': 'warning',
                    'message': f'Could not retrieve memory info: {e}'
                })
            
            # Test 4: Hit ratio check
            try:
                hit_ratio = cache.get_hit_ratio()
                health_report['metrics']['hit_ratio'] = hit_ratio
                
                if hit_ratio < 0.5:  # 50% threshold
                    health_report['issues'].append({
                        'type': 'low_hit_ratio',
                        'severity': 'warning',
                        'message': f'Low hit ratio: {hit_ratio:.1%}'
                    })
                    
            except Exception as e:
                health_report['issues'].append({
                    'type': 'metrics_failure',
                    'severity': 'warning',
                    'message': f'Could not retrieve hit ratio: {e}'
                })
            
        except ConnectionError:
            health_report['status'] = 'unhealthy'
            health_report['issues'].append({
                'type': 'connection_failure',
                'severity': 'critical',
                'message': 'Cannot connect to cache'
            })
            
        except Exception as e:
            health_report['status'] = 'unhealthy'
            health_report['issues'].append({
                'type': 'unknown_error',
                'severity': 'critical',
                'message': f'Health check failed: {e}'
            })
        
        return health_report
    
    def _health_check_loop(self):
        """
        Периодические health checks
        """
        while True:
            try:
                for cache in self.caches:
                    health_report = self.check_cache_health(cache)
                    self.health_status[cache.name] = health_report
                    
                    # Автоматическое восстановление при проблемах
                    if health_report['status'] == 'unhealthy':
                        self._attempt_recovery(cache, health_report)
                
                time.sleep(60)  # Health check каждую минуту
                
            except Exception as e:
                logger.error(f"Health check loop error: {e}")
                time.sleep(60)
    
    def _attempt_recovery(self, cache, health_report):
        """
        Попытка автоматического восстановления
        """
        for issue in health_report['issues']:
            issue_type = issue['type']
            
            if issue_type in self.recovery_strategies:
                try:
                    logger.info(f"Attempting recovery for {cache.name}: {issue_type}")
                    self.recovery_strategies[issue_type](cache, issue)
                    
                except Exception as e:
                    logger.error(f"Recovery failed for {cache.name}: {e}")
    
    def _recover_connection(self, cache, issue):
        """
        Восстановление соединения с кэшем
        """
        # Попытка переподключения
        cache.reconnect()
        
        # Проверка после переподключения
        time.sleep(5)
        health_report = self.check_cache_health(cache)
        
        if health_report['status'] == 'healthy':
            logger.info(f"Successfully recovered connection for {cache.name}")
        else:
            logger.error(f"Connection recovery failed for {cache.name}")
    
    def _recover_memory_pressure(self, cache, issue):
        """
        Восстановление при memory pressure
        """
        # Принудительная очистка expired entries
        cache.cleanup_expired()
        
        # Уменьшение TTL для новых entries
        cache.set_default_ttl(cache.get_default_ttl() // 2)
        
        # Принудительная eviction наименее используемых entries
        cache.evict_lru_entries(count=cache.size() // 4)
        
        logger.info(f"Applied memory pressure recovery for {cache.name}")
    
    def _recover_performance(self, cache, issue):
        """
        Восстановление производительности
        """
        # Проверка network connectivity
        if hasattr(cache, 'test_network_latency'):
            latency = cache.test_network_latency()
            if latency > 100:
                logger.warning(f"High network latency detected: {latency}ms")
        
        # Temporary performance optimizations
        cache.enable_batch_mode()
        cache.reduce_consistency_level()
        
        logger.info(f"Applied performance recovery for {cache.name}")

class CacheAlertManager:
    """
    Система алертов для критических проблем с кэшем
    """
    def __init__(self, notification_backends=None):
        self.notification_backends = notification_backends or []
        self.alert_history = defaultdict(list)
        self.alert_rules = {
            'critical_hit_ratio': {
                'condition': lambda metrics: metrics.get('hit_ratio', 1) < 0.3,
                'message': 'Critical: Cache hit ratio below 30%',
                'cooldown': 300  # 5 minutes
            },
            'high_latency': {
                'condition': lambda metrics: metrics.get('p95_latency_ms', 0) > 500,
                'message': 'Critical: P95 latency above 500ms',
                'cooldown': 300
            },
            'memory_exhaustion': {
                'condition': lambda metrics: metrics.get('memory_usage_percent', 0) > 95,
                'message': 'Critical: Memory usage above 95%',
                'cooldown': 180  # 3 minutes
            },
            'cache_unavailable': {
                'condition': lambda health: health.get('status') == 'unhealthy',
                'message': 'Critical: Cache instance unavailable',
                'cooldown': 60   # 1 minute
            }
        }
    
    def check_alerts(self, cache_name, metrics, health_status):
        """
        Проверка условий срабатывания алертов
        """
        current_time = time.time()
        triggered_alerts = []
        
        for alert_name, rule in self.alert_rules.items():
            try:
                # Проверяем условие срабатывания
                should_alert = False
                
                if 'hit_ratio' in alert_name or 'latency' in alert_name or 'memory' in alert_name:
                    should_alert = rule['condition'](metrics)
                elif 'unavailable' in alert_name:
                    should_alert = rule['condition'](health_status)
                
                if should_alert:
                    # Проверяем cooldown
                    last_alert_time = self._get_last_alert_time(cache_name, alert_name)
                    
                    if not last_alert_time or (current_time - last_alert_time) > rule['cooldown']:
                        alert = {
                            'cache_name': cache_name,
                            'alert_name': alert_name,
                            'message': rule['message'],
                            'timestamp': current_time,
                            'metrics': metrics.copy(),
                            'health_status': health_status.copy()
                        }
                        
                        triggered_alerts.append(alert)
                        self._record_alert(cache_name, alert_name, current_time)
                        
            except Exception as e:
                logger.error(f"Error checking alert {alert_name}: {e}")
        
        # Отправляем уведомления
        for alert in triggered_alerts:
            self._send_alert_notifications(alert)
        
        return triggered_alerts
    
    def _send_alert_notifications(self, alert):
        """
        Отправка уведомлений через различные каналы
        """
        for backend in self.notification_backends:
            try:
                backend.send_alert(alert)
            except Exception as e:
                logger.error(f"Failed to send alert via {backend.__class__.__name__}: {e}")
    
    def _get_last_alert_time(self, cache_name, alert_name):
        """
        Получение времени последнего алерта
        """
        alerts = self.alert_history.get(f"{cache_name}:{alert_name}", [])
        return alerts[-1] if alerts else None
    
    def _record_alert(self, cache_name, alert_name, timestamp):
        """
        Запись алерта в историю
        """
        key = f"{cache_name}:{alert_name}"
        self.alert_history[key].append(timestamp)
        
        # Ограничиваем историю
        if len(self.alert_history[key]) > 100:
            self.alert_history[key] = self.alert_history[key][-50:]

class SlackNotificationBackend:
    """
    Уведомления в Slack
    """
    def __init__(self, webhook_url, channel='#alerts'):
        self.webhook_url = webhook_url
        self.channel = channel
    
    def send_alert(self, alert):
        severity_emoji = {
            'critical': '🚨',
            'warning': '⚠️',
            'info': 'ℹ️'
        }
        
        severity = self._determine_severity(alert)
        emoji = severity_emoji.get(severity, '❓')
        
        message = {
            'channel': self.channel,
            'username': 'Cache Monitor',
            'icon_emoji': ':warning:',
            'attachments': [{
                'color': 'danger' if severity == 'critical' else 'warning',
                'title': f"{emoji} Cache Alert: {alert['alert_name']}",
                'text': alert['message'],
                'fields': [
                    {
                        'title': 'Cache',
                        'value': alert['cache_name'],
                        'short': True
                    },
                    {
                        'title': 'Time',
                        'value': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(alert['timestamp'])),
                        'short': True
                    }
                ],
                'footer': 'Cache Monitoring System',
                'ts': int(alert['timestamp'])
            }]
        }
        
        # Добавляем метрики в alert
        if alert.get('metrics'):
            metrics_text = '\n'.join([
                f"Hit Ratio: {alert['metrics'].get('hit_ratio', 0):.1%}",
                f"P95 Latency: {alert['metrics'].get('p95_latency_ms', 0):.1f}ms",
                f"Memory Usage: {alert['metrics'].get('memory_usage_percent', 0):.1f}%"
            ])
            
            message['attachments'][0]['fields'].append({
                'title': 'Metrics',
                'value': f"```{metrics_text}```",
                'short': False
            })
        
        # Отправляем в Slack
        response = requests.post(self.webhook_url, json=message)
        response.raise_for_status()
    
    def _determine_severity(self, alert):
        if 'critical' in alert['message'].lower():
            return 'critical'
        elif 'warning' in alert['message'].lower():
            return 'warning'
        else:
            return 'info'
```

### Performance Benchmarking и Load Testing

```python
class CacheBenchmarkSuite:
    """
    Комплексное тестирование производительности кэша
    """
    def __init__(self, cache_instance):
        self.cache = cache_instance
        self.results = {}
        
    def run_full_benchmark(self):
        """
        Запуск полного набора тестов производительности
        """
        print("Starting comprehensive cache benchmark...")
        
        # Test 1: Basic Operations
        self.results['basic_operations'] = self._benchmark_basic_operations()
        
        # Test 2: Concurrent Access
        self.results['concurrent_access'] = self._benchmark_concurrent_access()
        
        # Test 3: Memory Efficiency
        self.results['memory_efficiency'] = self._benchmark_memory_efficiency()
        
        # Test 4: Scalability
        self.results['scalability'] = self._benchmark_scalability()
        
        # Test 5: Network Performance (для distributed caches)
        if hasattr(self.cache, 'network_latency'):
            self.results['network_performance'] = self._benchmark_network_performance()
        
        # Test 6: Stress Testing
        self.results['stress_test'] = self._benchmark_stress_conditions()
        
        return self._generate_benchmark_report()
    
    def _benchmark_basic_operations(self):
        """
        Тестирование базовых операций: GET, SET, DELETE
        """
        operations = ['set', 'get', 'delete']
        sample_sizes = [1000, 10000, 100000]
        results = {}
        
        for sample_size in sample_sizes:
            results[sample_size] = {}
            
            # Подготовка тестовых данных
            test_data = {
                f"test_key_{i}": f"test_value_{i}" * 10  # ~100 bytes per value
                for i in range(sample_size)
            }
            
            for operation in operations:
                if operation == 'set':
                    latencies = self._time_set_operations(test_data)
                elif operation == 'get':
                    # Сначала заполняем кэш
                    self._bulk_set(test_data)
                    latencies = self._time_get_operations(list(test_data.keys()))
                elif operation == 'delete':
                    # Сначала заполняем кэш
                    self._bulk_set(test_data)
                    latencies = self._time_delete_operations(list(test_data.keys()))
                
                results[sample_size][operation] = {
                    'avg_latency_ms': sum(latencies) / len(latencies),
                    'p50_latency_ms': self._percentile(latencies, 50),
                    'p95_latency_ms': self._percentile(latencies, 95),
                    'p99_latency_ms': self._percentile(latencies, 99),
                    'operations_per_second': len(latencies) / (sum(latencies) / 1000),
                    'total_operations': len(latencies)
                }
        
        return results
    
    def _benchmark_concurrent_access(self):
        """
        Тестирование производительности при конкурентном доступе
        """
        thread_counts = [1, 5, 10, 20, 50, 100]
        operations_per_thread = 1000
        results = {}
        
        # Подготовка данных
        test_data = {
            f"concurrent_key_{i}": f"concurrent_value_{i}" * 10
            for i in range(10000)
        }
        self._bulk_set(test_data)
        
        for thread_count in thread_counts:
            print(f"Testing {thread_count} concurrent threads...")
            
            results[thread_count] = self._run_concurrent_test(
                thread_count, operations_per_thread, test_data
            )
        
        return results
    
    def _run_concurrent_test(self, thread_count, operations_per_thread, test_data):
        """
        Запуск теста с заданным количеством потоков
        """
        keys = list(test_data.keys())
        all_latencies = []
        barrier = threading.Barrier(thread_count)
        
        def worker_thread():
            # Синхронизируем старт всех потоков
            barrier.wait()
            
            thread_latencies = []
            for _ in range(operations_per_thread):
                key = random.choice(keys)
                
                start_time = time.time()
                self.cache.get(key)
                latency_ms = (time.time() - start_time) * 1000
                
                thread_latencies.append(latency_ms)
            
            return thread_latencies
        
        # Запускаем потоки
        with concurrent.futures.ThreadPoolExecutor(max_workers=thread_count) as executor:
            start_time = time.time()
            futures = [executor.submit(worker_thread) for _ in range(thread_count)]
            
            # Собираем результаты
            for future in concurrent.futures.as_completed(futures):
                all_latencies.extend(future.result())
        
        total_time = time.time() - start_time
        
        return {
            'total_operations': len(all_latencies),
            'total_time_seconds': total_time,
            'operations_per_second': len(all_latencies) / total_time,
            'avg_latency_ms': sum(all_latencies) / len(all_latencies),
            'p95_latency_ms': self._percentile(all_latencies, 95),
            'p99_latency_ms': self._percentile(all_latencies, 99),
            'thread_count': thread_count
        }
    
    def _benchmark_memory_efficiency(self):
        """
        Тестирование эффективности использования памяти
        """
        memory_results = {}
        
        # Test 1: Memory overhead per entry
        initial_memory = self._get_memory_usage()
        
        test_entries = {
            f"memory_test_{i}": "x" * 100  # 100 byte values
            for i in range(10000)
        }
        
        self._bulk_set(test_entries)
        final_memory = self._get_memory_usage()
        
        memory_per_entry = (final_memory - initial_memory) / len(test_entries)
        overhead_ratio = memory_per_entry / 100  # 100 bytes actual data
        
        memory_results['overhead_analysis'] = {
            'bytes_per_entry': memory_per_entry,
            'overhead_ratio': overhead_ratio,
            'efficiency_percent': (100 / memory_per_entry) * 100
        }
        
        # Test 2: Memory efficiency with different value sizes
        value_sizes = [10, 100, 1000, 10000]  # bytes
        memory_results['size_efficiency'] = {}
        
        for size in value_sizes:
            self.cache.clear()
            
            initial_mem = self._get_memory_usage()
            test_value = "x" * size
            
            # Store 1000 entries of this size
            for i in range(1000):
                self.cache.set(f"size_test_{size}_{i}", test_value)
            
            final_mem = self._get_memory_usage()
            
            memory_results['size_efficiency'][size] = {
                'actual_size_bytes': size,
                'memory_per_entry': (final_mem - initial_mem) / 1000,
                'overhead_bytes': ((final_mem - initial_mem) / 1000) - size,
                'overhead_percent': (((final_mem - initial_mem) / 1000) - size) / size * 100
            }
        
        return memory_results
    
    def _benchmark_scalability(self):
        """
        Тестирование масштабируемости кэша
        """
        cache_sizes = [1000, 10000, 100000, 1000000]
        scalability_results = {}
        
        for size in cache_sizes:
            print(f"Testing scalability with {size} entries...")
            
            self.cache.clear()
            
            # Заполняем кэш до указанного размера
            fill_start = time.time()
            for i in range(size):
                self.cache.set(f"scale_key_{i}", f"scale_value_{i}" * 10)
            
            fill_time = time.time() - fill_start
            
            # Тестируем производительность чтения
            test_keys = [f"scale_key_{random.randint(0, size-1)}" for _ in range(10000)]
            
            read_start = time.time()
            read_latencies = []
            
            for key in test_keys:
                start = time.time()
                self.cache.get(key)
                read_latencies.append((time.time() - start) * 1000)
            
            read_time = time.time() - read_start
            
            scalability_results[size] = {
                'fill_time_seconds': fill_time,
                'fill_rate_ops_per_second': size / fill_time,
                'read_avg_latency_ms': sum(read_latencies) / len(read_latencies),
                'read_p95_latency_ms': self._percentile(read_latencies, 95),
                'read_ops_per_second': len(test_keys) / read_time,
                'memory_usage_bytes': self._get_memory_usage()
            }
        
        return scalability_results
    
    def _benchmark_stress_conditions(self):
        """
        Стресс-тестирование кэша в экстремальных условиях
        """
        stress_results = {}
        
        # Test 1: Memory pressure
        print("Testing under memory pressure...")
        stress_results['memory_pressure'] = self._test_memory_pressure()
        
        # Test 2: High eviction rate
        print("Testing high eviction rate...")
        stress_results['high_eviction'] = self._test_high_eviction_rate()
        
        # Test 3: Large value handling
        print("Testing large values...")
        stress_results['large_values'] = self._test_large_values()
        
        # Test 4: Hotspot handling
        print("Testing hotspot access patterns...")
        stress_results['hotspot_handling'] = self._test_hotspot_patterns()
        
        return stress_results
    
    def _test_memory_pressure(self):
        """
        Тест поведения при нехватке памяти
        """
        self.cache.clear()
        
        # Заполняем кэш до максимума
        entry_count = 0
        start_time = time.time()
        
        try:
            while True:
                key = f"pressure_key_{entry_count}"
                value = "x" * 1000  # 1KB values
                
                self.cache.set(key, value)
                entry_count += 1
                
                # Проверяем каждые 1000 записей
                if entry_count % 1000 == 0:
                    memory_usage = self._get_memory_usage()
                    if hasattr(self.cache, 'max_memory') and memory_usage > self.cache.max_memory * 0.95:
                        break
                
                # Защита от бесконечного цикла
                if entry_count > 1000000:
                    break
                    
        except Exception as e:
            # Ожидаемо при достижении лимита памяти
            pass
        
        fill_time = time.time() - start_time
        
        # Тестируем производительность при полном кэше
        test_latencies = []
        for _ in range(1000):
            start = time.time()
            self.cache.get(f"pressure_key_{random.randint(0, entry_count-1)}")
            test_latencies.append((time.time() - start) * 1000)
        
        return {
            'max_entries_stored': entry_count,
            'fill_time_seconds': fill_time,
            'avg_latency_under_pressure_ms': sum(test_latencies) / len(test_latencies),
            'p95_latency_under_pressure_ms': self._percentile(test_latencies, 95),
            'memory_efficiency': entry_count * 1000 / self._get_memory_usage()
        }
    
    def _generate_benchmark_report(self):
        """
        Генерация итогового отчета по бенчмаркам
        """
        report = {
            'benchmark_timestamp': time.time(),
            'cache_type': self.cache.__class__.__name__,
            'test_results': self.results,
            'summary': self._generate_summary(),
            'recommendations': self._generate_recommendations()
        }
        
        return report
    
    def _generate_summary(self):
        """
        Генерация краткого резюме результатов
        """
        summary = {}
        
        # Basic operations summary
        if 'basic_operations' in self.results:
            basic_ops = self.results['basic_operations']
            largest_dataset = max(basic_ops.keys())
            
            summary['peak_performance'] = {
                'get_ops_per_second': basic_ops[largest_dataset]['get']['operations_per_second'],
                'set_ops_per_second': basic_ops[largest_dataset]['set']['operations_per_second'],
                'avg_get_latency_ms': basic_ops[largest_dataset]['get']['avg_latency_ms']
            }
        
        # Concurrency summary
        if 'concurrent_access' in self.results:
            concurrent = self.results['concurrent_access']
            best_throughput = max(concurrent.values(), key=lambda x: x['operations_per_second'])
            
            summary['concurrency'] = {
                'max_throughput_ops_per_second': best_throughput['operations_per_second'],
                'optimal_thread_count': best_throughput['thread_count']
            }
        
        # Memory efficiency summary
        if 'memory_efficiency' in self.results:
            memory = self.results['memory_efficiency']
            
            summary['memory_efficiency'] = {
                'overhead_ratio': memory['overhead_analysis']['overhead_ratio'],
                'efficiency_percent': memory['overhead_analysis']['efficiency_percent']
            }
        
        return summary
    
    def _generate_recommendations(self):
        """
        Генерация рекомендаций на основе результатов тестов
        """
        recommendations = []
        
        # Analyze basic performance
        if 'basic_operations' in self.results:
            basic_ops = self.results['basic_operations']
            largest_dataset = max(basic_ops.keys())
            
            get_latency = basic_ops[largest_dataset]['get']['avg_latency_ms']
            if get_latency > 10:
                recommendations.append({
                    'type': 'performance',
                    'priority': 'high',
                    'issue': f'High GET latency: {get_latency:.2f}ms',
                    'recommendation': 'Consider using faster storage or optimizing cache size'
                })
        
        # Analyze memory efficiency
        if 'memory_efficiency' in self.results:
            memory = self.results['memory_efficiency']
            overhead_ratio = memory['overhead_analysis']['overhead_ratio']
            
            if overhead_ratio > 3.0:  # More than 3x overhead
                recommendations.append({
                    'type': 'memory',
                    'priority': 'medium',
                    'issue': f'High memory overhead: {overhead_ratio:.1f}x',
                    'recommendation': 'Consider using more memory-efficient serialization or compression'
                })
        
        # Analyze concurrency
        if 'concurrent_access' in self.results:
            concurrent = self.results['concurrent_access']
            
            # Check if performance degrades significantly with more threads
            single_thread = concurrent.get(1, {}).get('operations_per_second', 0)
            multi_thread = concurrent.get(20, {}).get('operations_per_second', 0)
            
            if multi_thread < single_thread * 0.5:  # 50% degradation
                recommendations.append({
                    'type': 'concurrency',
                    'priority': 'high',
                    'issue': 'Poor concurrent performance scaling',
                    'recommendation': 'Consider using a thread-safe cache implementation or connection pooling'
                })
        
        return recommendations

# Пример использования
def run_cache_performance_analysis():
    """
    Пример комплексного анализа производительности кэша
    """
    # Инициализация кэша (пример с Redis)
    cache = Redis(host='localhost', port=6379, db=0)
    
    # Запуск бенчмарков
    benchmark_suite = CacheBenchmarkSuite(cache)
    results = benchmark_suite.run_full_benchmark()
    
    # Вывод результатов
    print("\n" + "="*50)
    print("CACHE PERFORMANCE BENCHMARK RESULTS")
    print("="*50)
    
    print(f"\nCache Type: {results['cache_type']}")
    print(f"Benchmark Date: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(results['benchmark_timestamp']))}")
    
    # Summary
    summary = results['summary']
    if 'peak_performance' in summary:
        print(f"\nPeak Performance:")
        print(f"  GET: {summary['peak_performance']['get_ops_per_second']:,.0f} ops/sec")
        print(f"  SET: {summary['peak_performance']['set_ops_per_second']:,.0f} ops/sec")
        print(f"  Avg GET Latency: {summary['peak_performance']['avg_get_latency_ms']:.2f}ms")
    
    if 'concurrency' in summary:
        print(f"\nConcurrency:")
        print(f"  Max Throughput: {summary['concurrency']['max_throughput_ops_per_second']:,.0f} ops/sec")
        print(f"  Optimal Threads: {summary['concurrency']['optimal_thread_count']}")
    
    if 'memory_efficiency' in summary:
        print(f"\nMemory Efficiency:")
        print(f"  Overhead Ratio: {summary['memory_efficiency']['overhead_ratio']:.1f}x")
        print(f"  Efficiency: {summary['memory_efficiency']['efficiency_percent']:.1f}%")
    
    # Recommendations
    recommendations = results['recommendations']
    if recommendations:
        print(f"\nRecommendations:")
        for i, rec in enumerate(recommendations, 1):
            print(f"  {i}. [{rec['priority'].upper()}] {rec['issue']}")
            print(f"     → {rec['recommendation']}")
    
    return results
```

---

## Модуль 11: Продвинутые темы и будущее кэширования

### Machine Learning в кэшировании

```python
class MLCachingSystem:
    """
    Система кэширования с использованием машинного обучения
    для предсказания access patterns и оптимизации
    """
    def __init__(self, cache_backend):
        self.cache = cache_backend
        self.feature_extractor = CacheFeatureExtractor()
        self.access_predictor = AccessPredictor()
        self.eviction_optimizer = MLEvictionOptimizer()
        self.prefetch_engine = PrefetchEngine()
        
        # Training data collection
        self.training_data_collector = TrainingDataCollector()
        self.model_trainer = ModelTrainer()
        
        # Online learning
        self.online_learner = OnlineLearner()
        
    def get(self, key, user_context=None):
        """
        Intelligent GET с ML-предсказаниями
        """
        # Извлекаем features для текущего запроса
        features = self.feature_extractor.extract_request_features(
            key, user_context, time.time()
        )
        
        # Записываем для обучения
        self.training_data_collector.record_access(key, features)
        
        # Пытаемся получить из кэша
        value = self.cache.get(key)
        
        if value is not None:
            # Cache hit - обновляем модели
            self.online_learner.update_hit_prediction(features, True)
            
            # Предсказываем следующие вероятные запросы
            next_keys = self.access_predictor.predict_next_accesses(
                key, user_context, features
            )
            
            # Запускаем prefetch для предсказанных ключей
            self.prefetch_engine.schedule_prefetch(next_keys)
            
            return value
        
        else:
            # Cache miss - учимся на ошибке
            self.online_learner.update_hit_prediction(features, False)
            
            # Анализируем, стоило ли кэшировать этот ключ
            should_cache = self.access_predictor.predict_future_value(key, features)
            
            return None, {'should_cache': should_cache, 'features': features}
    
    def set(self, key, value, user_context=None):
        """
        Intelligent SET с ML-оптимизацией размещения
        """
        features = self.feature_extractor.extract_request_features(
            key, user_context, time.time()
        )
        
        # Предсказываем optimal TTL
        optimal_ttl = self.access_predictor.predict_optimal_ttl(key, features)
        
        # Предсказываем вероятность будущих обращений
        access_probability = self.access_predictor.predict_access_probability(
            key, features
        )
        
        # Определяем приоритет кэширования
        cache_priority = self._calculate_cache_priority(
            access_probability, len(value), features
        )
        
        # Если кэш полный, используем ML для выбора жертвы eviction
        if self.cache.is_full():
            victim_key = self.eviction_optimizer.select_eviction_candidate(
                self.cache.get_all_keys(), features
            )
            if victim_key:
                self.cache.delete(victim_key)
        
        # Кэшируем с ML-оптимизированными параметрами
        self.cache.set(key, value, ttl=optimal_ttl, priority=cache_priority)
        
        # Обновляем модели
        self.training_data_collector.record_cache_set(key, features, optimal_ttl)

class CacheFeatureExtractor:
    """
    Извлечение признаков для ML моделей кэширования
    """
    def __init__(self):
        self.feature_history = defaultdict(list)
        self.global_stats = CacheGlobalStats()
        
    def extract_request_features(self, key, user_context, timestamp):
        """
        Извлечение features для запроса
        """
        features = {}
        
        # Временные признаки
        features.update(self._extract_temporal_features(timestamp))
        
        # Признаки ключа
        features.update(self._extract_key_features(key))
        
        # Признаки пользователя
        if user_context:
            features.update(self._extract_user_features(user_context))
        
        # Исторические признаки
        features.update(self._extract_historical_features(key))
        
        # Глобальные признаки системы
        features.update(self._extract_system_features())
        
        return features
    
    def _extract_temporal_features(self, timestamp):
        """
        Временные признаки: время дня, день недели, etc.
        """
        dt = datetime.fromtimestamp(timestamp)
        
        return {
            'hour_of_day': dt.hour,
            'day_of_week': dt.weekday(),
            'is_weekend': dt.weekday() >= 5,
            'is_business_hours': 9 <= dt.hour <= 17,
            'minute_of_hour': dt.minute,
            'second_of_minute': dt.second
        }
    
    def _extract_key_features(self, key):
        """
        Признаки самого ключа
        """
        return {
            'key_length': len(key),
            'key_complexity': len(set(key)),  # Unique characters
            'has_numbers': any(c.isdigit() for c in key),
            'has_special_chars': any(not c.isalnum() for c in key),
            'key_hash': hash(key) % 1000,  # Bucketed hash
            'key_prefix': self._extract_key_prefix(key),
            'key_suffix': self._extract_key_suffix(key)
        }
    
    def _extract_user_features(self, user_context):
        """
        Признаки пользователя и контекста
        """
        features = {}
        
        if 'user_id' in user_context:
            features['user_id_hash'] = hash(user_context['user_id']) % 10000
        
        if 'session_id' in user_context:
            features['session_id_hash'] = hash(user_context['session_id']) % 1000
        
        if 'device_type' in user_context:
            features['is_mobile'] = user_context['device_type'] == 'mobile'
            features['is_desktop'] = user_context['device_type'] == 'desktop'
        
        if 'user_segment' in user_context:
            features['user_segment'] = user_context['user_segment']
        
        if 'geographic_region' in user_context:
            features['region_hash'] = hash(user_context['geographic_region']) % 100
        
        return features
    
    def _extract_historical_features(self, key):
        """
        Исторические признаки доступа к ключу
        """
        history = self.feature_history.get(key, [])
        
        if not history:
            return {
                'access_count_1h': 0,
                'access_count_24h': 0,
                'last_access_age_minutes': float('inf'),
                'access_frequency_per_hour': 0,
                'access_pattern_regularity': 0
            }
        
        now = time.time()
        
        # Подсчет обращений за разные периоды
        accesses_1h = sum(1 for t in history if now - t <= 3600)
        accesses_24h = sum(1 for t in history if now - t <= 86400)
        
        # Последнее обращение
        last_access = max(history) if history else 0
        last_access_age = (now - last_access) / 60  # в минутах
        
        # Частота обращений
        if len(history) > 1:
            time_span = max(history) - min(history)
            frequency = len(history) / (time_span / 3600) if time_span > 0 else 0
        else:
            frequency = 0
        
        # Регулярность паттерна доступа
        regularity = self._calculate_access_regularity(history)
        
        return {
            'access_count_1h': accesses_1h,
            'access_count_24h': accesses_24h,
            'last_access_age_minutes': last_access_age,
            'access_frequency_per_hour': frequency,
            'access_pattern_regularity': regularity
        }
    
    def _extract_system_features(self):
        """
        Признаки состояния системы кэширования
        """
        return {
            'cache_hit_ratio_global': self.global_stats.get_hit_ratio(),
            'cache_memory_utilization': self.global_stats.get_memory_utilization(),
            'current_load_ops_per_second': self.global_stats.get_current_load(),
            'eviction_rate_per_minute': self.global_stats.get_eviction_rate(),
            'average_latency_ms': self.global_stats.get_average_latency()
        }

class AccessPredictor:
    """
    ML модель для предсказания паттернов доступа
    """
    def __init__(self):
        self.models = {
            'access_probability': AccessProbabilityModel(),
            'next_access_time': NextAccessTimeModel(),
            'optimal_ttl': OptimalTTLModel(),
            'access_sequence': AccessSequenceModel()
        }
        
        self.ensemble = EnsemblePredictor(self.models)
        
    def predict_access_probability(self, key, features):
        """
        Предсказание вероятности обращения к ключу в ближайшее время
        """
        return self.models['access_probability'].predict(features)
    
    def predict_next_accesses(self, current_key, user_context, features):
        """
        Предсказание следующих вероятных ключей доступа
        """
        # Используем sequence model для предсказания следующих ключей
        predicted_sequence = self.models['access_sequence'].predict_sequence(
            current_key, features, sequence_length=5
        )
        
        return predicted_sequence
    
    def predict_optimal_ttl(self, key, features):
        """
        Предсказание оптимального TTL для ключа
        """
        base_ttl = self.models['optimal_ttl'].predict(features)
        
        # Корректируем на основе access patterns
        access_frequency = features.get('access_frequency_per_hour', 0)
        
        if access_frequency > 10:  # Высокая частота
            return max(base_ttl, 3600)  # Минимум 1 час
        elif access_frequency < 0.1:  # Низкая частота
            return min(base_ttl, 300)   # Максимум 5 минут
        else:
            return base_ttl

class AccessProbabilityModel:
    """
    Модель предсказания вероятности доступа к данным
    """
    def __init__(self):
        self.model = self._create_model()
        self.feature_scaler = StandardScaler()
        self.is_trained = False
        
    def _create_model(self):
        """
        Создание модели машинного обучения
        """
        from sklearn.ensemble import GradientBoostingClassifier
        
        return GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=6,
            random_state=42
        )
    
    def train(self, training_data):
        """
        Обучение модели на исторических данных
        """
        features = []
        labels = []
        
        for record in training_data:
            feature_vector = self._features_to_vector(record['features'])
            features.append(feature_vector)
            labels.append(1 if record['was_accessed_again'] else 0)
        
        # Нормализация features
        features_scaled = self.feature_scaler.fit_transform(features)
        
        # Обучение модели
        self.model.fit(features_scaled, labels)
        self.is_trained = True
        
        # Валидация модели
        from sklearn.model_selection import cross_val_score
        cv_scores = cross_val_score(self.model, features_scaled, labels, cv=5)
        
        return {
            'cv_mean_accuracy': cv_scores.mean(),
            'cv_std_accuracy': cv_scores.std(),
            'feature_importance': self._get_feature_importance()
        }
    
    def predict(self, features):
        """
        Предсказание вероятности доступа
        """
        if not self.is_trained:
            return 0.5  # Default probability
        
        feature_vector = self._features_to_vector(features)
        feature_scaled = self.feature_scaler.transform([feature_vector])
        
        probability = self.model.predict_proba(feature_scaled)[0][1]
        return probability
    
    def _features_to_vector(self, features):
        """
        Конвертация features в вектор для ML модели
        """
        # Определяем порядок features для консистентности
        feature_names = [
            'hour_of_day', 'day_of_week', 'is_weekend', 'is_business_hours',
            'key_length', 'key_complexity', 'has_numbers', 'has_special_chars',
            'access_count_1h', 'access_count_24h', 'last_access_age_minutes',
            'access_frequency_per_hour', 'access_pattern_regularity',
            'cache_hit_ratio_global', 'cache_memory_utilization',
            'current_load_ops_per_second'
        ]
        
        vector = []
        for feature_name in feature_names:
            value = features.get(feature_name, 0)
            # Конвертируем boolean в numeric
            if isinstance(value, bool):
                value = 1 if value else 0
            # Обрабатываем inf значения
            if value == float('inf'):
                value = 9999
            vector.append(value)
        
        return vector

class PrefetchEngine:
    """
    Движок упреждающей загрузки данных на основе ML предсказаний
    """
    def __init__(self, cache, data_loader):
        self.cache = cache
        self.data_loader = data_loader
        self.prefetch_queue = queue.PriorityQueue()
        self.prefetch_stats = PrefetchStats()
        
        # Worker threads для prefetch
        self.workers = []
        for i in range(3):  # 3 worker threads
            worker = threading.Thread(
                target=self._prefetch_worker,
                daemon=True,
                name=f"PrefetchWorker-{i}"
            )
            worker.start()
            self.workers.append(worker)
    
    def schedule_prefetch(self, predicted_keys):
        """
        Планирование prefetch для предсказанных ключей
        """
        for key_info in predicted_keys:
            if isinstance(key_info, str):
                key = key_info
                probability = 0.5
                priority = 5
            else:
                key = key_info['key']
                probability = key_info.get('probability', 0.5)
                priority = key_info.get('priority', 5)
            
            # Проверяем, нужен ли prefetch
            if self._should_prefetch(key, probability):
                # Приоритет: чем выше вероятность, тем выше приоритет (меньше число)
                queue_priority = int((1 - probability) * 100)
                
                prefetch_task = {
                    'key': key,
                    'probability': probability,
                    'scheduled_at': time.time(),
                    'priority': priority
                }
                
                self.prefetch_queue.put((queue_priority, prefetch_task))
    
    def _should_prefetch(self, key, probability):
        """
        Определение необходимости prefetch
        """
        # Не prefetch если уже в кэше
        if self.cache.exists(key):
            return False
        
        # Не prefetch если низкая вероятность
        if probability < 0.7:
            return False
        
        # Не prefetch если система перегружена
        if self.prefetch_stats.get_queue_size() > 1000:
            return False
        
        # Не prefetch если недавно был неудачный prefetch для этого ключа
        if self.prefetch_stats.was_recent_miss(key):
            return False
        
        return True
    
    def _prefetch_worker(self):
        """
        Worker thread для выполнения prefetch операций
        """
        while True:
            try:
                # Получаем задачу из очереди
                priority, task = self.prefetch_queue.get(timeout=5)
                
                key = task['key']
                start_time = time.time()
                
                try:
                    # Загружаем данные
                    data = self.data_loader.load(key)
                    
                    if data is not None:
                        # Кэшируем загруженные данные
                        self.cache.set(key, data, ttl=3600)  # 1 hour default TTL
                        
                        # Записываем успешный prefetch
                        self.prefetch_stats.record_success(
                            key, time.time() - start_time, task['probability']
                        )
                    else:
                        # Данных нет - записываем miss
                        self.prefetch_stats.record_miss(key, task['probability'])
                
                except Exception as e:
                    # Ошибка загрузки - записываем error
                    self.prefetch_stats.record_error(key, str(e))
                
                finally:
                    self.prefetch_queue.task_done()
                    
            except queue.Empty:
                continue  # Таймаут - продолжаем работу
            except Exception as e:
                logger.error(f"Prefetch worker error: {e}")

class CacheOptimizationEngine:
    """
    Движок автоматической оптимизации кэша на основе ML анализа
    """
    def __init__(self, cache_system):
        self.cache_system = cache_system
        self.performance_analyzer = CachePerformanceAnalyzer()
        self.optimization_strategies = [
            TTLOptimizationStrategy(),
            EvictionPolicyOptimizer(),
            MemoryAllocationOptimizer(),
            ConcurrencyOptimizer()
        ]
        
        # Непрерывная оптимизация
        self.optimization_scheduler = threading.Thread(
            target=self._continuous_optimization_loop,
            daemon=True
        )
        self.optimization_scheduler.start()
    
    def _continuous_optimization_loop(self):
        """
        Непрерывный цикл анализа и оптимизации
        """
        while True:
            try:
                # Анализируем текущую производительность
                performance_metrics = self.performance_analyzer.analyze_current_performance()
                
                # Выявляем проблемы
                issues = self.performance_analyzer.identify_performance_issues(performance_metrics)
                
                # Применяем оптимизации
                for issue in issues:
                    self._apply_optimization(issue, performance_metrics)
                
                # Ждем перед следующим циклом
                time.sleep(300)  # 5 минут
                
            except Exception as e:
                logger.error(f"Optimization loop error: {e}")
                time.sleep(60)  # Короткая пауза при ошибке
    
    def _apply_optimization(self, issue, metrics):
        """
        Применение оптимизации для конкретной проблемы
        """
        for strategy in self.optimization_strategies:
            if strategy.can_handle(issue):
                try:
                    optimization_result = strategy.optimize(
                        self.cache_system, issue, metrics
                    )
                    
                    logger.info(f"Applied optimization: {optimization_result}")
                    
                    # Мониторинг эффекта оптимизации
                    self._monitor_optimization_effect(optimization_result)
                    
                except Exception as e:
                    logger.error(f"Optimization failed: {e}")

class TTLOptimizationStrategy:
    """
    Стратегия оптимизации TTL на основе access patterns
    """
    def can_handle(self, issue):
        return issue['type'] in ['high_miss_ratio', 'memory_pressure', 'stale_data']
    
    def optimize(self, cache_system, issue, metrics):
        """
        Оптимизация TTL политик
        """
        if issue['type'] == 'high_miss_ratio':
            # Увеличиваем TTL для популярных ключей
            self._increase_popular_key_ttl(cache_system, metrics)
            
        elif issue['type'] == 'memory_pressure':
            # Уменьшаем TTL для редко используемых ключей
            self._decrease_unpopular_key_ttl(cache_system, metrics)
            
        elif issue['type'] == 'stale_data':
            # Настраиваем TTL на основе частоты изменений данных
            self._adaptive_ttl_by_data_freshness(cache_system, metrics)
        
        return {
            'strategy': 'TTL Optimization',
            'issue_type': issue['type'],
            'action_taken': 'TTL policies updated',
            'expected_improvement': self._estimate_improvement(issue, metrics)
        }
    
    def _increase_popular_key_ttl(self, cache_system, metrics):
        """
        Увеличение TTL для популярных ключей
        """
        popular_keys = metrics.get('popular_keys', [])
        
        for key_info in popular_keys[:100]:  # Top 100 popular keys
            key = key_info['key']
            current_ttl = cache_system.get_ttl(key)
            access_frequency = key_info['access_frequency']
            
            # Увеличиваем TTL пропорционально популярности
            if access_frequency > 10:  # > 10 accesses per hour
                new_ttl = min(current_ttl * 2, 86400)  # Max 24 hours
                cache_system.update_ttl(key, new_ttl)
```

---

## Заключение: Мастерство кэширования

### Принципы мастера кэширования

После изучения всех модулей этого руководства, вы должны понимать, что кэширование — это не просто "добавить Redis и забыть". Это сложная инженерная дисциплина, требующая глубокого понимания:

#### 1. Системного мышления
```
Мастер кэширования думает системно:
┌─────────────────────────────────────────┐
│ Бизнес требования                        │
│ ├── Производительность (latency, QPS)   │
│ ├── Надежность (availability, durability)│
│ └── Стоимость (infrastructure, operations)│
│                                         │
│ Технические ограничения                  │
│ ├── Memory constraints                  │
│ ├── Network bandwidth                   │
│ ├── Consistency requirements            │
│ └── Scalability needs                   │
│                                         │
│ Операционные аспекты                    │
│ ├── Monitoring и alerting               │
│ ├── Disaster recovery                   │
│ ├── Security compliance                 │
│ └── Team expertise                      │
└─────────────────────────────────────────┘
```

#### 2. Понимания компромиссов
Каждое решение в кэшировании — это trade-off:

**Consistency vs Performance**
- Strong consistency = Higher latency
- Eventual consistency = Better performance, возможные stale reads

**Memory vs Latency**
- Больше memory = Больше cache hits = Lower latency
- Ограниченная memory = Cache optimization необходимость

**Complexity vs Reliability**
- Простые решения = Легче поддерживать
- Сложные решения = Больше features, больше точек отказа

#### 3. Эволюционного подхода

```python
# Эволюция кэширования в проекте:

# Phase 1: Простое локальное кэширование
cache = {}
def get_user(user_id):
    if user_id in cache:
        return cache[user_id]
    user = database.get_user(user_id)
    cache[user_id] = user
    return user

# Phase 2: TTL и размер ограничения
from cachetools import TTLCache
cache = TTLCache(maxsize=1000, ttl=300)

# Phase 3: Распределенное кэширование
import redis
cache = redis.Redis(host='redis-server')

# Phase 4: Multi-level кэширование
class MultiLevelCache:
    def __init__(self):
        self.l1 = TTLCache(maxsize=100, ttl=60)    # Local
        self.l2 = redis.Redis()                    # Distributed
        
# Phase 5: Intelligent кэширование с ML
class IntelligentCache:
    def __init__(self):
        self.cache_levels = [LocalCache(), RedisCache(), DatabaseCache()]
        self.ml_optimizer = MLCacheOptimizer()
        self.predictive_prefetch = PredictivePrefetch()

# Phase 6: Production-ready система
class ProductionCacheSystem:
    def __init__(self):
        self.cache_hierarchy = CacheHierarchy()
        self.monitoring = ComprehensiveMonitoring()
        self.auto_scaling = AutoScalingManager()
        self.disaster_recovery = DisasterRecoveryManager()
        self.security = CacheSecurityManager()
```

### Архитектурные паттерны мирового класса

#### Netflix Architecture Pattern
```python
class NetflixStyleCaching:
    """
    Паттерн кэширования в стиле Netflix
    - Географически распределенные кэши
    - Predictive caching на основе viewing patterns
    - Multi-tier cache hierarchy
    """
    def __init__(self):
        self.content_cache = {
            'edge': EdgeCache(),      # CDN edge locations
            'regional': RegionalCache(),  # Regional data centers
            'origin': OriginCache()   # Origin servers
        }
        
        self.metadata_cache = {
            'user_profiles': UserProfileCache(),
            'recommendations': RecommendationCache(),
            'content_metadata': ContentMetadataCache()
        }
        
        self.predictive_engine = PredictiveContentEngine()
    
    def get_content(self, content_id, user_location, user_profile):
        """
        Intelligent content delivery с predictive caching
        """
        # 1. Определяем ближайший edge
        edge_location = self._find_nearest_edge(user_location)
        
        # 2. Проверяем edge cache
        content = self.content_cache['edge'].get(content_id, edge_location)
        if content:
            # Предсказываем следующий контент
            self._predict_and_cache_next_content(user_profile, content_id)
            return content
        
        # 3. Fallback to regional cache
        content = self.content_cache['regional'].get(content_id, user_location)
        if content:
            # Асинхронно копируем на edge
            self._async_promote_to_edge(content_id, content, edge_location)
            return content
        
        # 4. Fallback to origin
        content = self.content_cache['origin'].get(content_id)
        if content:
            # Распространяем по уровням
            self._propagate_content(content_id, content, user_location)
        
        return content
```

#### Google-style Distributed Caching
```python
class GoogleStyleDistributedCache:
    """
    Паттерн в стиле Google: consistency через Paxos/Raft
    """
    def __init__(self, nodes):
        self.nodes = nodes
        self.consensus_manager = RaftConsensusManager(nodes)
        self.shard_manager = ConsistentHashSharding(nodes)
        self.replication_factor = 3
    
    def get(self, key):
        """
        Linearizable reads через consensus
        """
        # Определяем shards для ключа
        primary_shard, replica_shards = self.shard_manager.get_shards(key)
        
        # Читаем с кворума для consistency
        quorum_size = (self.replication_factor // 2) + 1
        
        read_responses = []
        for shard in [primary_shard] + replica_shards[:quorum_size-1]:
            try:
                response = shard.read_with_version(key)
                read_responses.append(response)
            except ShardUnavailableException:
                continue
        
        if len(read_responses) < quorum_size:
            raise InsufficientQuorumException()
        
        # Выбираем значение с highest version
        latest_response = max(read_responses, key=lambda r: r.version)
        
        # Read repair если есть inconsistency
        if not all(r.version == latest_response.version for r in read_responses):
            self._async_read_repair(key, latest_response, read_responses)
        
        return latest_response.value
    
    def set(self, key, value):
        """
        Consensus-based writes
        """
        # Получаем новую версию через consensus
        new_version = self.consensus_manager.get_next_version(key)
        
        versioned_value = VersionedValue(value, new_version, time.time())
        
        # Записываем на кворум shards
        primary_shard, replica_shards = self.shard_manager.get_shards(key)
        target_shards = [primary_shard] + replica_shards[:self.replication_factor-1]
        
        # Two-phase commit
        transaction_id = self._generate_transaction_id()
        
        # Phase 1: Prepare
        prepare_votes = []
        for shard in target_shards:
            vote = shard.prepare_write(transaction_id, key, versioned_value)
            prepare_votes.append((shard, vote))
        
        successful_prepares = [v for _, v in prepare_votes if v.success]
        
        if len(successful_prepares) >= (self.replication_factor // 2) + 1:
            # Phase 2: Commit
            for shard, vote in prepare_votes:
                if vote.success:
                    shard.commit_write(transaction_id)
        else:
            # Abort
            for shard, vote in prepare_votes:
                if vote.success:
                    shard.abort_write(transaction_id)
            raise WriteFailedException("Insufficient replicas for write")
```

#### Amazon DynamoDB-style Eventually Consistent Caching
```python
class DynamoDBStyleCache:
    """
    Eventually consistent distributed cache с vector clocks
    """
    def __init__(self, nodes, consistency_level='eventual'):
        self.nodes = nodes
        self.consistency_level = consistency_level
        self.vector_clock = VectorClock(nodes)
        self.merkle_trees = {}  # Для anti-entropy
        self.hinted_handoff = HintedHandoffManager()
    
    def get(self, key, consistency='eventual'):
        """
        Configurable consistency reads
        """
        if consistency == 'strong':
            return self._strong_consistent_read(key)
        else:
            return self._eventually_consistent_read(key)
    
    def _eventually_consistent_read(self, key):
        """
        Eventually consistent read - может вернуть stale data
        """
        target_nodes = self._get_preference_list(key, n=3)
        
        # Читаем с первого доступного узла
        for node in target_nodes:
            try:
                response = node.get(key)
                if response:
                    return response.value
            except NodeUnavailableException:
                continue
        
        return None
    
    def _strong_consistent_read(self, key):
        """
        Strong consistent read - читаем с кворума
        """
        target_nodes = self._get_preference_list(key, n=3)
        quorum_size = 2  # R=2 для strong consistency
        
        responses = []
        for node in target_nodes:
            try:
                response = node.get_with_metadata(key)
                if response:
                    responses.append(response)
                if len(responses) >= quorum_size:
                    break
            except NodeUnavailableException:
                continue
        
        if len(responses) < quorum_size:
            raise InsufficientQuorumException()
        
        # Выбираем значение с latest vector clock
        latest_response = max(responses, key=lambda r: r.vector_clock.timestamp)
        
        # Запускаем read repair асинхронно
        if len(set(r.vector_clock.timestamp for r in responses)) > 1:
            self._async_read_repair(key, responses)
        
        return latest_response.value
    
    def put(self, key, value, w=2):  # W=2 write quorum
        """
        Eventually consistent write с configurable durability
        """
        target_nodes = self._get_preference_list(key, n=3)
        
        # Increment vector clock
        self.vector_clock.increment(self._node_id())
        
        versioned_value = VersionedValue(
            value=value,
            vector_clock=self.vector_clock.copy(),
            timestamp=time.time()
        )
        
        successful_writes = 0
        failed_nodes = []
        
        for node in target_nodes:
            try:
                node.put(key, versioned_value)
                successful_writes += 1
                
                if successful_writes >= w:
                    break
                    
            except NodeUnavailableException:
                failed_nodes.append(node)
                continue
        
        if successful_writes < w:
            raise InsufficientWriteQuorumException()
        
        # Hinted handoff для failed nodes
        for node in failed_nodes:
            self.hinted_handoff.store_hint(node, key, versioned_value)
        
        return True
    
    def _anti_entropy_repair(self):
        """
        Background anti-entropy процесс с Merkle trees
        """
        for peer_node in self.nodes:
            if peer_node.id == self._node_id():
                continue
            
            # Сравниваем Merkle trees
            local_tree = self.merkle_trees.get(peer_node.id)
            remote_tree = peer_node.get_merkle_tree()
            
            if local_tree and remote_tree:
                differences = local_tree.compare(remote_tree)
                
                # Синхронизируем различающиеся ключи
                for key_range in differences:
                    self._sync_key_range(peer_node, key_range)
```

### Production Deployment Strategies

#### Blue-Green Cache Deployment
```python
class BlueGreenCacheDeployment:
    """
    Blue-Green deployment для cache infrastructure
    """
    def __init__(self):
        self.blue_env = CacheEnvironment('blue')
        self.green_env = CacheEnvironment('green')
        self.active_env = 'blue'
        self.traffic_splitter = TrafficSplitter()
        
    def deploy_new_version(self, new_cache_config):
        """
        Zero-downtime deployment нового cache конфигурации
        """
        inactive_env = 'green' if self.active_env == 'blue' else 'blue'
        target_env = self.green_env if inactive_env == 'green' else self.blue_env
        
        print(f"Deploying to {inactive_env} environment...")
        
        # 1. Deploy новая конфигурация в inactive environment
        target_env.deploy(new_cache_config)
        
        # 2. Warm up новый cache
        self._warm_up_cache(target_env)
        
        # 3. Health check новой среды
        if not self._health_check(target_env):
            raise DeploymentFailedException("Health check failed")
        
        # 4. Gradual traffic shifting
        self._gradual_traffic_shift(inactive_env)
        
        # 5. Monitor metrics во время переключения
        self._monitor_deployment_metrics(inactive_env)
        
        # 6. Complete switch
        self.active_env = inactive_env
        print(f"Deployment complete. Active environment: {self.active_env}")
    
    def _gradual_traffic_shift(self, target_env):
        """
        Постепенное переключение трафика
        """
        traffic_percentages = [5, 10, 25, 50, 75, 100]
        
        for percentage in traffic_percentages:
            print(f"Shifting {percentage}% traffic to {target_env}")
            
            self.traffic_splitter.set_traffic_split(target_env, percentage)
            
            # Ждем и мониторим
            time.sleep(300)  # 5 minutes
            
            metrics = self._get_environment_metrics(target_env)
            if not self._validate_metrics(metrics):
                # Rollback при проблемах
                print("Metrics validation failed, rolling back...")
                self.traffic_splitter.set_traffic_split(target_env, 0)
                raise DeploymentFailedException("Metrics validation failed")
    
    def _warm_up_cache(self, target_env):
        """
        Cache warming перед переключением трафика
        """
        # Получаем popular keys из активной среды
        active_env_instance = self.blue_env if self.active_env == 'blue' else self.green_env
        popular_keys = active_env_instance.get_popular_keys(limit=10000)
        
        # Переносим popular data в новую среду
        batch_size = 100
        for i in range(0, len(popular_keys), batch_size):
            batch = popular_keys[i:i + batch_size]
            
            # Bulk transfer
            data_batch = active_env_instance.bulk_get(batch)
            target_env.bulk_set(data_batch)
            
            print(f"Warmed up {min(i + batch_size, len(popular_keys))}/{len(popular_keys)} keys")
```

#### Canary Cache Deployments
```python
class CanaryCacheDeployment:
    """
    Canary deployment с automated rollback
    """
    def __init__(self):
        self.production_cache = ProductionCacheCluster()
        self.canary_cache = CanaryCacheCluster()
        self.metrics_analyzer = MetricsAnalyzer()
        self.auto_rollback = AutoRollbackManager()
        
    def deploy_canary(self, new_config, canary_percentage=5):
        """
        Canary deployment с автоматическим мониторингом
        """
        # Deploy canary version
        self.canary_cache.deploy(new_config)
        
        # Start traffic splitting
        self._start_canary_traffic(canary_percentage)
        
        # Monitor canary metrics
        monitoring_result = self._monitor_canary_deployment()
        
        if monitoring_result.success:
            # Promote canary to production
            return self._promote_canary_to_production()
        else:
            # Automatic rollback
            return self._rollback_canary(monitoring_result.reason)
    
    def _monitor_canary_deployment(self, duration_minutes=30):
        """
        Мониторинг canary deployment с automatic analysis
        """
        start_time = time.time()
        end_time = start_time + (duration_minutes * 60)
        
        baseline_metrics = self.production_cache.get_baseline_metrics()
        
        while time.time() < end_time:
            current_metrics = {
                'production': self.production_cache.get_current_metrics(),
                'canary': self.canary_cache.get_current_metrics()
            }
            
            # Analyze metrics для anomalies
            analysis = self.metrics_analyzer.compare_metrics(
                baseline_metrics, current_metrics
            )
            
            if analysis.has_critical_issues():
                return MonitoringResult(
                    success=False,
                    reason=f"Critical issues detected: {analysis.issues}"
                )
            
            if analysis.has_performance_regression():
                return MonitoringResult(
                    success=False,
                    reason=f"Performance regression: {analysis.regression_details}"
                )
            
            time.sleep(60)  # Check every minute
        
        # Финальная оценка
        final_analysis = self.metrics_analyzer.final_evaluation(
            baseline_metrics, current_metrics
        )
        
        return MonitoringResult(
            success=final_analysis.passed,
            reason=final_analysis.summary
        )
```

### Cache Security Best Practices

#### Comprehensive Security Framework
```python
class CacheSecurityFramework:
    """
    Комплексная система безопасности для кэша
    """
    def __init__(self):
        self.encryption_manager = CacheEncryptionManager()
        self.access_control = CacheAccessControl()
        self.audit_logger = CacheAuditLogger()
        self.threat_detector = CacheThreatDetector()
        
    def secure_cache_operation(self, operation, key, value=None, user_context=None):
        """
        Безопасное выполнение cache операции
        """
        # 1. Authentication & Authorization
        if not self.access_control.authorize_operation(operation, key, user_context):
            self.audit_logger.log_unauthorized_access(operation, key, user_context)
            raise UnauthorizedCacheAccessException()
        
        # 2. Input validation
        self._validate_cache_input(key, value)
        
        # 3. Threat detection
        threat_score = self.threat_detector.analyze_request(operation, key, user_context)
        if threat_score > 0.8:  # High threat score
            self.audit_logger.log_potential_threat(operation, key, user_context, threat_score)
            raise SuspiciousActivityException()
        
        # 4. Encryption для sensitive data
        if self._is_sensitive_data(key, value):
            if value is not None:
                value = self.encryption_manager.encrypt(value, key)
        
        # 5. Execute operation
        try:
            result = self._execute_cache_operation(operation, key, value)
            
            # 6. Decrypt результат если необходимо
            if result and self._is_sensitive_data(key):
                result = self.encryption_manager.decrypt(result, key)
            
            # 7. Audit logging
            self.audit_logger.log_successful_operation(operation, key, user_context)
            
            return result
            
        except Exception as e:
            self.audit_logger.log_operation_error(operation, key, user_context, str(e))
            raise
    
    def _validate_cache_input(self, key, value):
        """
        Валидация входных данных для предотвращения injection атак
        """
        # Key validation
        if not isinstance(key, str) or len(key) > 250:
            raise InvalidCacheKeyException("Invalid cache key format")
        
        # Проверка на injection patterns
        dangerous_patterns = ['<script', 'javascript:', 'data:', '../', '\\x']
        for pattern in dangerous_patterns:
            if pattern.lower() in key.lower():
                raise PotentialInjectionException(f"Dangerous pattern detected in key: {pattern}")
        
        # Value size validation
        if value is not None and len(str(value)) > 10 * 1024 * 1024:  # 10MB limit
            raise ValueTooLargeException("Cache value exceeds size limit")

class CacheEncryptionManager:
    """
    Управление шифрованием данных в кэше
    """
    def __init__(self):
        self.encryption_keys = self._load_encryption_keys()
        self.key_rotation_schedule = KeyRotationScheduler()
        
    def encrypt(self, data, cache_key):
        """
        Шифрование данных с key-specific encryption
        """
        # Определяем тип шифрования на основе ключа
        encryption_type = self._determine_encryption_type(cache_key)
        
        if encryption_type == 'aes_256':
            return self._aes_encrypt(data, self.encryption_keys['aes_256'])
        elif encryption_type == 'field_level':
            return self._field_level_encrypt(data, cache_key)
        else:
            return data  # No encryption needed
    
    def decrypt(self, encrypted_data, cache_key):
        """
        Расшифровка данных
        """
        encryption_type = self._determine_encryption_type(cache_key)
        
        if encryption_type == 'aes_256':
            return self._aes_decrypt(encrypted_data, self.encryption_keys['aes_256'])
        elif encryption_type == 'field_level':
            return self._field_level_decrypt(encrypted_data, cache_key)
        else:
            return encrypted_data
    
    def _determine_encryption_type(self, cache_key):
        """
        Определение необходимого уровня шифрования
        """
        # PII data patterns
        pii_patterns = ['user:', 'profile:', 'email:', 'phone:', 'ssn:', 'credit_card:']
        
        if any(pattern in cache_key.lower() for pattern in pii_patterns):
            return 'aes_256'
        
        # Financial data patterns
        financial_patterns = ['payment:', 'transaction:', 'balance:', 'account:']
        
        if any(pattern in cache_key.lower() for pattern in financial_patterns):
            return 'field_level'
        
        return 'none'

class CacheThreatDetector:
    """
    Детекция угроз и аномальной активности
    """
    def __init__(self):
        self.ml_model = AnomalyDetectionModel()
        self.rate_limiters = {}
        self.suspicious_patterns = SuspiciousPatternDatabase()
        
    def analyze_request(self, operation, key, user_context):
        """
        Анализ запроса на подозрительную активность
        """
        threat_score = 0.0
        
        # Rate limiting analysis
        user_id = user_context.get('user_id', 'anonymous')
        rate_limit_score = self._check_rate_limits(user_id, operation)
        threat_score += rate_limit_score * 0.3
        
        # Pattern analysis
        pattern_score = self._analyze_access_patterns(key, user_context)
        threat_score += pattern_score * 0.4
        
        # ML-based anomaly detection
        features = self._extract_threat_features(operation, key, user_context)
        ml_score = self.ml_model.predict_anomaly_score(features)
        threat_score += ml_score * 0.3
        
        return min(threat_score, 1.0)  # Cap at 1.0
    
    def _check_rate_limits(self, user_id, operation):
        """
        Проверка rate limits для пользователя
        """
        if user_id not in self.rate_limiters:
            self.rate_limiters[user_id] = RateLimiter(
                requests_per_minute=1000,
                requests_per_hour=10000
            )
        
        rate_limiter = self.rate_limiters[user_id]
        
        if not rate_limiter.allow_request():
            return 1.0  # Maximum threat score для rate limit violation
        
        # Градуальное увеличение threat score при приближении к лимиту
        current_rate = rate_limiter.get_current_rate()
        limit_utilization = current_rate / rate_limiter.requests_per_minute
        
        if limit_utilization > 0.8:  # 80% of rate limit
            return limit_utilization  # 0.8-1.0 threat score
        
        return 0.0
    
    def _analyze_access_patterns(self, key, user_context):
        """
        Анализ паттернов доступа на подозрительность
        """
        threat_score = 0.0
        
        # Sequential key enumeration detection
        if self._is_sequential_enumeration(key):
            threat_score += 0.6
        
        # Suspicious key patterns
        if self._matches_suspicious_pattern(key):
            threat_score += 0.8
        
        # Geographic anomalies
        if self._is_geographic_anomaly(user_context):
            threat_score += 0.4
        
        # Time-based anomalies
        if self._is_temporal_anomaly(user_context):
            threat_score += 0.3
        
        return min(threat_score, 1.0)
```

### Финальные рекомендации для Production

#### 1. Операционная готовность
```bash
# Cache Health Check Script
#!/bin/bash
cache_health_check() {
    echo "=== Cache Health Check ==="
    
    # Redis connectivity
    redis-cli ping || echo "ERROR: Redis not responding"
    
    # Memory usage
    MEMORY_USAGE=$(redis-cli info memory | grep used_memory_human | cut -d: -f2)
    echo "Memory usage: $MEMORY_USAGE"
    
    # Hit ratio
    HIT_RATIO=$(redis-cli info stats | grep keyspace_hits | cut -d: -f2)
    MISS_RATIO=$(redis-cli info stats | grep keyspace_misses | cut -d: -f2)
    echo "Hit ratio: $((HIT_RATIO * 100 / (HIT_RATIO + MISS_RATIO)))%"
    
    # Slow operations
    echo "Recent slow operations:"
    redis-cli slowlog get 5
    
    # Cluster status (if applicable)
    redis-cli cluster info || echo "Single instance mode"
}

# Automated Cache Maintenance
cache_maintenance() {
    echo "=== Cache Maintenance ==="
    
    # Cleanup expired keys
    redis-cli eval "return redis.call('eval', 'local keys = redis.call(\"keys\", \"*\") local expired = 0 for i=1,#keys do if redis.call(\"ttl\", keys[i]) == -1 then redis.call(\"del\", keys[i]) expired = expired + 1 end end return expired', 0)" 0
    
    # Memory defragmentation
    redis-cli memory purge
    
    # Backup critical data
    redis-cli bgsave
}
```

#### 2. Disaster Recovery Plan
```python
class CacheDisasterRecovery:
    """
    Комплексный план disaster recovery для cache systems
    """
    def __init__(self):
        self.backup_manager = CacheBackupManager()
        self.failover_manager = FailoverManager()
        self.data_integrity_checker = DataIntegrityChecker()
        
    def create_disaster_recovery_plan(self):
        """
        Создание плана восстановления после аварий
        """
        plan = {
            'backup_strategy': {
                'frequency': 'Every 6 hours',
                'retention': '30 days',
                'storage': 'Multi-region S3',
                'verification': 'Automated integrity checks'
            },
            
            'failover_strategy': {
                'rto': '< 5 minutes',  # Recovery Time Objective
                'rpo': '< 1 hour',     # Recovery Point Objective
                'automatic': True,
                'manual_override': True
            },
            
            'recovery_procedures': [
                'Assess damage scope',
                'Activate standby systems',
                'Restore from backup if needed',
                'Verify data integrity',
                'Resume normal operations',
                'Post-incident analysis'
            ]
        }
        
        return plan
    
    def execute_failover(self, failure_type):
        """
        Выполнение failover процедуры
        """
        if failure_type == 'primary_node_failure':
            return self._handle_primary_node_failure()
        elif failure_type == 'network_partition':
            return self._handle_network_partition()
        elif failure_type == 'data_corruption':
            return self._handle_data_corruption()
        else:
            return self._handle_unknown_failure(failure_type)
```

#### 3. Performance Optimization Checklist

```
Cache Performance Optimization Checklist:

□ Memory Management
  □ Configured appropriate memory limits
  □ Enabled memory optimization features
  □ Monitoring memory fragmentation
  □ Implemented proper eviction policies

□ Network Optimization  
  □ Connection pooling configured
  □ Pipeline operations where possible
  □ Compression enabled for large values
  □ Network latency minimized

□ Data Structure Optimization
  □ Using appropriate Redis data types
  □ Optimized serialization format
  □ Compression for large objects
  □ TTL optimization based on access patterns

□ Monitoring & Alerting
  □ Hit ratio monitoring (target: >80%)
  □ Latency monitoring (target: <10ms)
  □ Memory usage alerts
  □ Error rate tracking
  □ Capacity planning dashboards

□ Security
  □ Access control implemented
  □ Encryption for sensitive data
  □ Network security (VPC, firewalls)
  □ Audit logging enabled
  □ Threat detection active

□ Operational Excellence
  □ Automated backups
  □ Disaster recovery tested
  □ Runbooks documented
  □ Team training completed
  □ Incident response procedures
```

---

## Заключение: От новичка до эксперта

После изучения всех 12 модулей этого руководства, вы прошли путь от базового понимания кэширования до экспертного уровня владения всеми аспектами этой критически важной технологии.

### Что вы освоили:

**Фундаментальные знания:**
- Принципы локальности и теория кэширования
- Математика cache hit ratio и performance optimization
- CAP теорема применительно к кэшированию

**Практические навыки:**
- Реализация всех основных стратегий кэширования
- Настройка и оптимизация Redis, Memcached, и других систем
- Проектирование multi-level cache hierarchies
- Мониторинг и troubleshooting cache систем

**Архитектурное мастерство:**
- Проектирование distributed cache систем
- Обеспечение consistency и availability
- Security best practices
- Disaster recovery planning

**Продвинутые техники:**
- Machine Learning в кэшировании
- Predictive caching и intelligent prefetch
- Auto-scaling и self-optimization
- Performance engineering на enterprise уровне

### Дальнейшее развитие:

1. **Практический опыт**: Применяйте полученные знания в реальных проектах
2. **Community участие**: Вносите вклад в open-source cache проекты
3. **Непрерывное обучение**: Следите за новыми developments в области кэширования
4. **Ментoring**: Делитесь знаниями с другими разработчиками

### Финальная мудрость:

> **"Лучший кэш - тот, о котором не нужно думать"**

Это означает, что мастерство кэширования заключается в создании систем, которые:
- Работают прозрачно для пользователей
- Автоматически адаптируются к изменениям нагрузки  
- Самовосстанавливаются при сбоях
- Оптимизируются на основе real-world usage patterns

Вы теперь обладаете знаниями для создания именно таких систем. Удачи в применении полученных навыков! 🚀

---

**© 2025 Полное руководство по кэшированию для Backend разработчиков**
*Версия 2.0 - Complete Expert Edition* 