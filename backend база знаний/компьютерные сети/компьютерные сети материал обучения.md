            await self._send_error(connection, "Authentication token required")
            self.stats['authentication_failures'] += 1
            return
        
        try:
            # Декодируем JWT токен
            payload = jwt.decode(token, self.jwt_secret, algorithms=['HS256'])
            
            user_id = payload.get('user_id')
            if not user_id:
                await self._send_error(connection, "Invalid token payload")
                self.stats['authentication_failures'] += 1
                return
            
            # Устанавливаем аутентификацию
            connection.user_id = user_id
            connection.authenticated = True
            connection.state = ConnectionState.AUTHENTICATED
            
            # Отправляем подтверждение
            await self._send_message(connection, {
                'type': 'auth_success',
                'user_id': user_id,
                'connection_id': connection.id
            })
            
            self.logger.info(f"Connection {connection.id} authenticated as user {user_id}")
        
        except jwt.InvalidTokenError:
            await self._send_error(connection, "Invalid authentication token")
            self.stats['authentication_failures'] += 1
        
        except Exception as e:
            self.logger.error(f"Authentication error for {connection.id}: {e}")
            await self._send_error(connection, "Authentication failed")
            self.stats['authentication_failures'] += 1
    
    async def _handle_subscribe(self, connection: WebSocketConnection, message: Dict):
        """Обработка подписки на топик"""
        
        topic = message.get('topic')
        
        if not topic:
            await self._send_error(connection, "Topic name required")
            return
        
        # Проверяем права доступа к топику
        if not await self._check_topic_access(connection, topic):
            await self._send_error(connection, f"Access denied to topic: {topic}")
            return
        
        # Добавляем подписку
        if topic not in self.subscriptions:
            self.subscriptions[topic] = Subscription(topic=topic)
        
        self.subscriptions[topic].connections.add(connection.id)
        connection.subscriptions.add(topic)
        
        # Отправляем подтверждение
        await self._send_message(connection, {
            'type': 'subscribed',
            'topic': topic,
            'subscriber_count': len(self.subscriptions[topic].connections)
        })
        
        self.logger.debug(f"Connection {connection.id} subscribed to {topic}")
    
    async def _handle_unsubscribe(self, connection: WebSocketConnection, message: Dict):
        """Обработка отписки от топика"""
        
        topic = message.get('topic')
        
        if not topic:
            await self._send_error(connection, "Topic name required")
            return
        
        # Удаляем подписку
        if topic in self.subscriptions:
            self.subscriptions[topic].connections.discard(connection.id)
            
            # Удаляем топик если нет подписчиков
            if not self.subscriptions[topic].connections:
                del self.subscriptions[topic]
        
        connection.subscriptions.discard(topic)
        
        # Отправляем подтверждение
        await self._send_message(connection, {
            'type': 'unsubscribed',
            'topic': topic
        })
        
        self.logger.debug(f"Connection {connection.id} unsubscribed from {topic}")
    
    async def _handle_data(self, connection: WebSocketConnection, message: Dict):
        """Обработка пользовательских данных"""
        
        # Это может быть отправка сообщения в топик или другая бизнес-логика
        topic = message.get('topic')
        data = message.get('data')
        
        if topic and data:
            await self.broadcast_to_topic(topic, {
                'type': 'data',
                'topic': topic,
                'data': data,
                'sender': connection.user_id,
                'timestamp': time.time()
            }, exclude_connection=connection.id)
    
    async def _handle_heartbeat(self, connection: WebSocketConnection, message: Dict):
        """Обработка heartbeat"""
        
        connection.last_activity = time.time()
        
        await self._send_message(connection, {
            'type': 'heartbeat_ack',
            'timestamp': time.time()
        })
    
    async def _check_topic_access(self, connection: WebSocketConnection, topic: str) -> bool:
        """Проверка прав доступа к топику"""
        
        # Здесь можно реализовать сложную логику авторизации
        # Например, проверка по ролям пользователя, ACL, и т.д.
        
        # Простая проверка: пользователи могут подписываться только на свои топики
        if connection.user_id and topic.startswith(f"user.{connection.user_id}"):
            return True
        
        # Публичные топики доступны всем
        if topic.startswith("public."):
            return True
        
        # Админы имеют доступ ко всем топикам
        if connection.metadata.get('role') == 'admin':
            return True
        
        return False
    
    async def _send_message(self, connection: WebSocketConnection, message: Dict):
        """Отправка сообщения клиенту"""
        
        try:
            json_message = json.dumps(message)
            await connection.websocket.send(json_message)
            
            # Обновляем статистику
            connection.messages_sent += 1
            connection.bytes_sent += len(json_message)
            
        except websockets.exceptions.ConnectionClosed:
            # Соединение уже закрыто
            pass
        except Exception as e:
            self.logger.error(f"Error sending message to {connection.id}: {e}")
    
    async def _send_error(self, connection: WebSocketConnection, error_message: str):
        """Отправка сообщения об ошибке"""
        
        await self._send_message(connection, {
            'type': 'error',
            'message': error_message,
            'timestamp': time.time()
        })
    
    async def broadcast_to_topic(self, topic: str, message: Dict, exclude_connection: Optional[str] = None):
        """Рассылка сообщения всем подписчикам топика"""
        
        if topic not in self.subscriptions:
            return
        
        subscription = self.subscriptions[topic]
        subscription.message_count += 1
        subscription.last_message_time = time.time()
        
        # Отправляем сообщение всем подписчикам
        disconnected_connections = []
        
        for connection_id in subscription.connections:
            if connection_id == exclude_connection:
                continue
            
            if connection_id in self.connections:
                connection = self.connections[connection_id]
                try:
                    await self._send_message(connection, message)
                except:
                    # Помечаем для удаления
                    disconnected_connections.append(connection_id)
        
        # Убираем отключенные соединения
        for connection_id in disconnected_connections:
            subscription.connections.discard(connection_id)
    
    async def broadcast_to_all(self, message: Dict, exclude_connection: Optional[str] = None):
        """Рассылка сообщения всем подключенным клиентам"""
        
        disconnected_connections = []
        
        for connection_id, connection in self.connections.items():
            if connection_id == exclude_connection:
                continue
            
            try:
                await self._send_message(connection, message)
            except:
                disconnected_connections.append(connection_id)
        
        # Убираем отключенные соединения
        for connection_id in disconnected_connections:
            await self._cleanup_connection(connection_id)
    
    async def _cleanup_connection(self, connection_id: str):
        """Очистка соединения"""
        
        if connection_id not in self.connections:
            return
        
        connection = self.connections[connection_id]
        
        # Удаляем из всех подписок
        for topic in list(connection.subscriptions):
            if topic in self.subscriptions:
                self.subscriptions[topic].connections.discard(connection_id)
                
                # Удаляем топик если нет подписчиков
                if not self.subscriptions[topic].connections:
                    del self.subscriptions[topic]
        
        # Закрываем WebSocket если еще не закрыт
        if not connection.websocket.closed:
            try:
                await connection.websocket.close()
            except:
                pass
        
        # Удаляем соединение
        del self.connections[connection_id]
        self.stats['current_connections'] -= 1
        
        self.logger.info(f"Cleaned up connection {connection_id}")
    
    async def _cleanup_loop(self):
        """Фоновая задача очистки неактивных соединений"""
        
        while True:
            try:
                current_time = time.time()
                cleanup_connections = []
                
                for connection_id, connection in self.connections.items():
                    # Проверяем таймаут неактивности
                    if current_time - connection.last_activity > self.connection_timeout:
                        cleanup_connections.append(connection_id)
                
                # Очищаем неактивные соединения
                for connection_id in cleanup_connections:
                    await self._cleanup_connection(connection_id)
                    self.logger.info(f"Cleaned up inactive connection {connection_id}")
                
                await asyncio.sleep(60)  # Проверяем каждую минуту
            
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Error in cleanup loop: {e}")
                await asyncio.sleep(60)
    
    async def _heartbeat_loop(self):
        """Фоновая задача отправки heartbeat"""
        
        while True:
            try:
                # Отправляем heartbeat всем соединениям
                heartbeat_message = {
                    'type': 'heartbeat',
                    'timestamp': time.time(),
                    'server_stats': {
                        'connections': len(self.connections),
                        'uptime': time.time() - self.stats.get('start_time', time.time())
                    }
                }
                
                await self.broadcast_to_all(heartbeat_message)
                
                await asyncio.sleep(self.heartbeat_interval)
            
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Error in heartbeat loop: {e}")
                await asyncio.sleep(self.heartbeat_interval)
    
    def get_stats(self) -> Dict:
        """Получение статистики сервера"""
        
        # Статистика соединений
        connection_stats = {}
        total_messages_sent = 0
        total_messages_received = 0
        total_bytes_sent = 0
        total_bytes_received = 0
        
        for connection in self.connections.values():
            total_messages_sent += connection.messages_sent
            total_messages_received += connection.messages_received
            total_bytes_sent += connection.bytes_sent
            total_bytes_received += connection.bytes_received
        
        # Статистика подписок
        subscription_stats = {
            topic: {
                'subscribers': len(sub.connections),
                'messages_sent': sub.message_count,
                'last_message': sub.last_message_time
            }
            for topic, sub in self.subscriptions.items()
        }
        
        return {
            **self.stats,
            'connections': {
                'current': len(self.connections),
                'authenticated': len([c for c in self.connections.values() if c.authenticated]),
                'messages_sent': total_messages_sent,
                'messages_received': total_messages_received,
                'bytes_sent': total_bytes_sent,
                'bytes_received': total_bytes_received
            },
            'subscriptions': {
                'total_topics': len(self.subscriptions),
                'topics': subscription_stats
            }
        }
    
    async def shutdown(self):
        """Graceful shutdown сервера"""
        
        self.logger.info("Shutting down WebSocket server...")
        
        # Останавливаем фоновые задачи
        if self.cleanup_task:
            self.cleanup_task.cancel()
        
        if self.heartbeat_task:
            self.heartbeat_task.cancel()
        
        # Закрываем все соединения
        shutdown_message = {
            'type': 'server_shutdown',
            'message': 'Server is shutting down',
            'timestamp': time.time()
        }
        
        await self.broadcast_to_all(shutdown_message)
        
        # Ждем немного для отправки сообщений
        await asyncio.sleep(1)
        
        # Принудительно закрываем все соединения
        for connection_id in list(self.connections.keys()):
            await self._cleanup_connection(connection_id)
        
        self.logger.info("WebSocket server shutdown completed")

# gRPC Implementation
import grpc
from concurrent import futures
import threading

# Определяем Protocol Buffer схему (обычно в отдельном .proto файле)
PROTO_DEFINITION = """
syntax = "proto3";

package api;

service UserService {
    rpc GetUser(GetUserRequest) returns (User);
    rpc CreateUser(CreateUserRequest) returns (User);
    rpc UpdateUser(UpdateUserRequest) returns (User);
    rpc DeleteUser(DeleteUserRequest) returns (DeleteUserResponse);
    rpc ListUsers(ListUsersRequest) returns (stream User);
    rpc StreamUserUpdates(StreamUserUpdatesRequest) returns (stream UserUpdate);
}

message User {
    string id = 1;
    string name = 2;
    string email = 3;
    int64 created_at = 4;
    int64 updated_at = 5;
}

message GetUserRequest {
    string id = 1;
}

message CreateUserRequest {
    string name = 1;
    string email = 2;
}

message UpdateUserRequest {
    string id = 1;
    string name = 2;
    string email = 3;
}

message DeleteUserRequest {
    string id = 1;
}

message DeleteUserResponse {
    bool success = 1;
    string message = 2;
}

message ListUsersRequest {
    int32 page = 1;
    int32 page_size = 2;
}

message StreamUserUpdatesRequest {
    repeated string user_ids = 1;
}

message UserUpdate {
    string user_id = 1;
    string update_type = 2;
    User user = 3;
    int64 timestamp = 4;
}
"""

class AdvancedGRPCServer:
    """Продвинутый gRPC сервер с мониторингом и middleware"""
    
    def __init__(self, port: int = 50051):
        self.port = port
        self.server = None
        
        # Статистика
        self.stats = {
            'total_requests': 0,
            'requests_by_method': defaultdict(int),
            'active_streams': 0,
            'errors': defaultdict(int),
            'avg_response_time': 0.0,
            'response_times': deque(maxlen=1000)
        }
        
        # Middleware
        self.interceptors = []
        
        self.logger = logging.getLogger('gRPCServer')
        
        # User storage (в реальном приложении это была бы БД)
        self.users = {}
        self.user_counter = 0
        
        # Streaming subscribers
        self.stream_subscribers = defaultdict(list)
    
    def add_interceptor(self, interceptor):
        """Добавление interceptor middleware"""
        self.interceptors.append(interceptor)
    
    async def start(self):
        """Запуск gRPC сервера"""
        
        # Создаем сервер с interceptors
        self.server = grpc.aio.server(
            futures.ThreadPoolExecutor(max_workers=10),
            interceptors=self.interceptors
        )
        
        # Регистрируем сервис
        # В реальном приложении здесь был бы сгенерированный код из .proto
        # user_service_pb2_grpc.add_UserServiceServicer_to_server(self, self.server)
        
        # Добавляем порт
        listen_addr = f'[::]:{self.port}'
        self.server.add_insecure_port(listen_addr)
        
        # Запускаем сервер
        await self.server.start()
        self.logger.info(f"gRPC server started on port {self.port}")
        
        # Ждем завершения
        await self.server.wait_for_termination()
    
    async def GetUser(self, request, context):
        """Получение пользователя по ID"""
        
        self._record_request("GetUser")
        
        user_id = request.id
        
        if user_id not in self.users:
            context.set_code(grpc.StatusCode.NOT_FOUND)
            context.set_details(f"User {user_id} not found")
            return None
        
        user_data = self.users[user_id]
        
        # Возвращаем User объект (в реальности это был бы protobuf объект)
        return {
            'id': user_data['id'],
            'name': user_data['name'],
            'email': user_data['email'],
            'created_at': user_data['created_at'],
            'updated_at': user_data['updated_at']
        }
    
    async def CreateUser(self, request, context):
        """Создание нового пользователя"""
        
        self._record_request("CreateUser")
        
        # Валидация
        if not request.name or not request.email:
            context.set_code(grpc.StatusCode.INVALID_ARGUMENT)
            context.set_details("Name and email are required")
            return None
        
        # Создаем пользователя
        self.user_counter += 1
        user_id = str(self.user_counter)
        
        current_time = int(time.time())
        
        user_data = {
            'id': user_id,
            'name': request.name,
            'email': request.email,
            'created_at': current_time,
            'updated_at': current_time
        }
        
        self.users[user_id] = user_data
        
        # Уведомляем подписчиков о создании
        await self._notify_user_update(user_id, "created", user_data)
        
        return user_data
    
    async def UpdateUser(self, request, context):
        """Обновление пользователя"""
        
        self._record_request("UpdateUser")
        
        user_id = request.id
        
        if user_id not in self.users:
            context.set_code(grpc.StatusCode.NOT_FOUND)
            context.set_details(f"User {user_id} not found")
            return None
        
        # Обновляем данные
        user_data = self.users[user_id]
        
        if request.name:
            user_data['name'] = request.name
        
        if request.email:
            user_data['email'] = request.email
        
        user_data['updated_at'] = int(time.time())
        
        # Уведомляем подписчиков об обновлении
        await self._notify_user_update(user_id, "updated", user_data)
        
        return user_data
    
    async def DeleteUser(self, request, context):
        """Удаление пользователя"""
        
        self._record_request("DeleteUser")
        
        user_id = request.id
        
        if user_id not in self.users:
            context.set_code(grpc.StatusCode.NOT_FOUND)
            context.set_details(f"User {user_id} not found")
            return {'success': False, 'message': 'User not found'}
        
        # Удаляем пользователя
        del self.users[user_id]
        
        # Уведомляем подписчиков об удалении
        await self._notify_user_update(user_id, "deleted", None)
        
        return {'success': True, 'message': 'User deleted successfully'}
    
    async def ListUsers(self, request, context):
        """Streaming список пользователей"""
        
        self._record_request("ListUsers")
        self.stats['active_streams'] += 1
        
        try:
            page = max(1, request.page)
            page_size = min(100, max(1, request.page_size))
            
            start_idx = (page - 1) * page_size
            end_idx = start_idx + page_size
            
            user_list = list(self.users.values())
            page_users = user_list[start_idx:end_idx]
            
            for user_data in page_users:
                yield user_data
                
                # Небольшая задержка для демонстрации streaming
                await asyncio.sleep(0.1)
        
        finally:
            self.stats['active_streams'] -= 1
    
    async def StreamUserUpdates(self, request, context):
        """Streaming обновлений пользователей"""
        
        self._record_request("StreamUserUpdates")
        self.stats['active_streams'] += 1
        
        # Создаем очередь для этого stream
        update_queue = asyncio.Queue()
        
        # Подписываемся на обновления
        for user_id in request.user_ids:
            self.stream_subscribers[user_id].append(update_queue)
        
        try:
            # Отправляем updates из очереди
            while True:
                try:
                    update = await asyncio.wait_for(update_queue.get(), timeout=30)
                    yield update
                
                except asyncio.TimeoutError:
                    # Отправляем heartbeat
                    yield {
                        'user_id': '',
                        'update_type': 'heartbeat',
                        'user': None,
                        'timestamp': int(time.time())
                    }
        
        except asyncio.CancelledError:
            pass
        
        finally:
            # Отписываемся от обновлений
            for user_id in request.user_ids:
                if update_queue in self.stream_subscribers[user_id]:
                    self.stream_subscribers[user_id].remove(update_queue)
            
            self.stats['active_streams'] -= 1
    
    async def _notify_user_update(self, user_id: str, update_type: str, user_data: Optional[Dict]):
        """Уведомление подписчиков об обновлении пользователя"""
        
        if user_id not in self.stream_subscribers:
            return
        
        update_message = {
            'user_id': user_id,
            'update_type': update_type,
            'user': user_data,
            'timestamp': int(time.time())
        }
        
        # Отправляем всем подписчикам
        for queue in self.stream_subscribers[user_id]:
            try:
                await queue.put(update_message)
            except:
                pass  # Игнорируем ошибки закрытых очередей
    
    def _record_request(self, method_name: str):
        """Запись статистики запроса"""
        
        self.stats['total_requests'] += 1
        self.stats['requests_by_method'][method_name] += 1
    
    def get_stats(self) -> Dict:
        """Получение статистики сервера"""
        
        return {
            'total_requests': self.stats['total_requests'],
            'requests_by_method': dict(self.stats['requests_by_method']),
            'active_streams': self.stats['active_streams'],
            'total_users': len(self.users),
            'stream_subscribers': len(self.stream_subscribers),
            'errors': dict(self.stats['errors'])
        }
    
    async def shutdown(self):
        """Graceful shutdown сервера"""
        
        if self.server:
            await self.server.stop(grace=5)
            self.logger.info("gRPC server shutdown completed")

# Демонстрация современных протоколов
async def demonstrate_modern_protocols():
    """Демонстрация WebSocket и gRPC"""
    
    print("🚀 Modern Network Protocols Demonstration")
    print("=" * 60)
    
    # Создаем JWT токен для аутентификации
    import jwt
    
    jwt_secret = "your-secret-key"
    auth_token = jwt.encode({
        'user_id': 'demo_user_123',
        'role': 'user',
        'exp': int(time.time()) + 3600  # 1 час
    }, jwt_secret, algorithm='HS256')
    
    print(f"🔐 Generated auth token: {auth_token[:50]}...")
    
    # Запуск WebSocket сервера
    print(f"\n🔌 Starting WebSocket Server:")
    
    ws_server = WebSocketServer(host="localhost", port=8765)
    ws_server.auth_required = False  # Упрощаем для демонстрации
    
    # Запускаем сервер в фоновой задаче
    server_task = asyncio.create_task(ws_server.start())
    
    # Даем серверу время запуститься
    await asyncio.sleep(2)
    
    # Тестируем WebSocket клиент
    print("  Testing WebSocket client connections...")
    
    try:
        # Создаем несколько клиентских соединений
        clients = []
        
        for i in range(3):
            uri = "ws://localhost:8765"
            
            try:
                websocket = await websockets.connect(uri)
                clients.append(websocket)
                print(f"    Client {i+1} connected")
                
                # Получаем welcome message
                welcome_msg = await websocket.recv()
                welcome_data = json.loads(welcome_msg)
                print(f"    Client {i+1} received: {welcome_data['type']}")
                
            except Exception as e:
                print(f"    Client {i+1} connection failed: {e}")
        
        if clients:
            # Демонстрируем подписки
            print("\n  Testing topic subscriptions...")
            
            client1 = clients[0]
            
            # Подписываемся на топик
            subscribe_msg = {
                'type': 'subscribe',
                'topic': 'public.demo'
            }
            
            await client1.send(json.dumps(subscribe_msg))
            
            # Получаем подтверждение подписки
            response = await client1.recv()
            response_data = json.loads(response)
            print(f"    Subscription response: {response_data}")
            
            # Отправляем сообщение в топик от другого клиента
            if len(clients) > 1:
                client2 = clients[1]
                
                data_msg = {
                    'type': 'data',
                    'topic': 'public.demo',
                    'data': {
                        'message': 'Hello from client 2!',
                        'timestamp': time.time()
                    }
                }
                
                await client2.send(json.dumps(data_msg))
                print("    Client 2 sent message to topic")
                
                # Клиент 1 должен получить сообщение
                try:
                    broadcast_msg = await asyncio.wait_for(client1.recv(), timeout=5.0)
                    broadcast_data = json.loads(broadcast_msg)
                    print(f"    Client 1 received broadcast: {broadcast_data['data']['message']}")
                except asyncio.TimeoutError:
                    print("    No broadcast message received (timeout)")
        
        # Показываем статистику WebSocket сервера
        print(f"\n  WebSocket Server Statistics:")
        ws_stats = ws_server.get_stats()
        
        print(f"    Total connections: {ws_stats['total_connections']}")
        print(f"    Current connections: {ws_stats['connections']['current']}")
        print(f"    Total messages: {ws_stats['total_messages']}")
        print(f"    Active topics: {ws_stats['subscriptions']['total_topics']}")
        
        # Закрываем клиентские соединения
        for i, client in enumerate(clients):
            await client.close()
            print(f"    Client {i+1} disconnected")
    
    except Exception as e:
        print(f"  WebSocket testing failed: {e}")
    
    finally:
        # Останавливаем WebSocket сервер
        server_task.cancel()
        await ws_server.shutdown()
        print("  WebSocket server stopped")
    
    # Демонстрация gRPC
    print(f"\n🔧 gRPC Server Demonstration:")
    
    try:
        grpc_server = AdvancedGRPCServer(port=50051)
        
        print("  gRPC server would handle:")
        print("    - Unary calls (GetUser, CreateUser)")
        print("    - Server streaming (ListUsers)")
        print("    - Bidirectional streaming (StreamUserUpdates)")
        print("    - Built-in load balancing and service discovery")
        print("    - Protocol Buffers for efficient serialization")
        print("    - HTTP/2 multiplexing and flow control")
        
        # Симулируем статистику
        print(f"\n  gRPC Performance Benefits:")
        print(f"    - Binary serialization: ~10x smaller than JSON")
        print(f"    - HTTP/2 multiplexing: Multiple calls per connection")
        print(f"    - Streaming: Real-time bidirectional communication")
        print(f"    - Code generation: Type-safe client/server code")
        print(f"    - Built-in authentication and encryption")
        
    except Exception as e:
        print(f"  gRPC demonstration failed: {e}")
    
    print(f"\n✅ Modern protocols demonstration completed")

# Запуск демонстрации
if __name__ == "__main__":
    asyncio.run(demonstrate_modern_protocols())
```

### 📝 Практическое задание Неделя 15

1. Реализуйте production-ready WebSocket сервер с authentication и authorization
2. Создайте gRPC сервис для критически важных API с high performance требованиями
3. Добавьте real-time уведомления в ваше приложение через WebSocket
4. Протестируйте производительность gRPC vs HTTP/REST API
5. Реализуйте гибридную архитектуру с WebSocket для real-time и gRPC для синхронных вызовов

---

## Неделя 16: Мониторинг и диагностика

### 🧠 Концепция: Observability для сетевых систем

Современный мониторинг основан на трех столпах observability:

```
Observability Pillars:
┌─────────────────────────────────────────────────────────────┐
│ Metrics (Числовые показатели)                               │
│ ├─ Latency, Throughput, Error rates                        │
│ ├─ Resource utilization                                    │
│ ├─ Business metrics                                        │
│ └─ SLI/SLO monitoring                                      │
├─────────────────────────────────────────────────────────────┤
│ Logs (Структурированные события)                           │
│ ├─ Request/response logs                                   │
│ ├─ Error logs with context                                │
│ ├─ Audit logs                                             │
│ └─ Distributed tracing                                    │
├─────────────────────────────────────────────────────────────┤
│ Traces (Распределенная трассировка)                        │
│ ├─ Request flow across services                           │
│ ├─ Performance bottlenecks                                │
│ ├─ Dependency mapping                                     │
│ └─ Root cause analysis                                    │
└─────────────────────────────────────────────────────────────┘
```

### 📊 Comprehensive Network Monitoring System

**Enterprise-grade Network Monitoring:**

```python
import asyncio
import time
import json
import statistics
from typing import Dict, List, Optional, Callable, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import logging
from collections import defaultdict, deque
import psutil
import aiohttp
import socket
import subprocess
import threading
from datetime import datetime, timedelta
import uuid

class MetricType(Enum):
    COUNTER = "counter"
    GAUGE = "gauge"
    HISTOGRAM = "histogram"
    SUMMARY = "summary"

class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

@dataclass
class Metric:
    name: str
    value: float
    timestamp: float
    labels: Dict[str, str] = field(default_factory=dict)
    metric_type: MetricType = MetricType.GAUGE

@dataclass
class Alert:
    id: str
    name: str
    severity: AlertSeverity
    message: str
    timestamp: float
    labels: Dict[str, str] = field(default_factory=dict)
    resolved: bool = False
    resolved_at: Optional[float] = None

@dataclass
class TraceSpan:
    trace_id: str
    span_id: str
    parent_span_id: Optional[str]
    operation_name: str
    start_time: float
    end_time: Optional[float] = None
    duration_ms: Optional[float] = None
    tags: Dict[str, Any] = field(default_factory=dict)
    logs: List[Dict] = field(default_factory=list)
    
    def finish(self):
        """Завершение span"""
        self.end_time = time.time()
        if self.start_time:
            self.duration_ms = (self.end_time - self.start_time) * 1000

class NetworkMonitoringSystem:
    def __init__(self):
        # Metrics storage
        self.metrics: Dict[str, deque] = defaultdict(lambda: deque(maxlen=10000))
        self.metric_metadata: Dict[str, Dict] = {}
        
        # Alerts
        self.alerts: Dict[str, Alert] = {}
        self.alert_rules: List[Dict] = []
        self.alert_callbacks: List[Callable] = []
        
        # Tracing
        self.traces: Dict[str, List[TraceSpan]] = defaultdict(list)
        self.active_spans: Dict[str, TraceSpan] = {}
        
        # Monitoring targets
        self.monitoring_targets: Dict[str, Dict] = {}
        
        # Configuration
        self.collection_interval = 15  # секунд
        self.retention_period = 7 * 24 * 3600  # 7 дней
        self.max_traces_per_service = 1000
        
        # Background tasks
        self.collection_tasks: Dict[str, asyncio.Task] = {}
        self.cleanup_task: Optional[asyncio.Task] = None
        
        # Statistics
        self.stats = {
            'metrics_collected': 0,
            'alerts_fired': 0,
            'traces_collected': 0,
            'collection_errors': 0
        }
        
        self.logger = logging.getLogger('NetworkMonitoring')
        
        # Built-in collectors
        self._register_default_collectors()
    
    def _register_default_collectors(self):
        """Регистрация встроенных коллекторов метрик"""
        
        # System metrics collector
        self.add_monitoring_target(
            'system_metrics',
            'System Resources',
            self._collect_system_metrics,
            interval=30
        )
        
        # Network metrics collector
        self.add_monitoring_target(
            'network_metrics',
            'Network Statistics',
            self._collect_network_metrics,
            interval=15
        )
    
    def add_monitoring_target(self, target_id: str, name: str, 
                            collector_func: Callable, interval: int = 60):
        """Добавление цели мониторинга"""
        
        self.monitoring_targets[target_id] = {
            'name': name,
            'collector': collector_func,
            'interval': interval,
            'last_collection': 0,
            'errors': 0
        }
        
        self.logger.info(f"Added monitoring target: {name}")
    
    def add_alert_rule(self, rule: Dict):
        """Добавление правила алертинга"""
        
        required_fields = ['name', 'condition', 'severity', 'message']
        
        if not all(field in rule for field in required_fields):
            raise ValueError(f"Alert rule must contain: {required_fields}")
        
        self.alert_rules.append(rule)
        self.logger.info(f"Added alert rule: {rule['name']}")
    
    def add_alert_callback(self, callback: Callable):
        """Добавление callback для алертов"""
        self.alert_callbacks.append(callback)
    
    async def start_monitoring(self):
        """Запуск системы мониторинга"""
        
        self.logger.info("Starting network monitoring system...")
        
        # Запускаем коллекторы
        for target_id, target in self.monitoring_targets.items():
            task = asyncio.create_task(
                self._collection_loop(target_id, target)
            )
            self.collection_tasks[target_id] = task
        
        # Запускаем cleanup задачу
        self.cleanup_task = asyncio.create_task(self._cleanup_loop())
        
        self.logger.info("Network monitoring system started")
    
    async def _collection_loop(self, target_id: str, target: Dict):
        """Цикл сбора метрик для цели"""
        
        while True:
            try:
                start_time = time.time()
                
                # Вызываем коллектор
                metrics = await target['collector']()
                
                # Сохраняем метрики
                for metric in metrics:
                    await self.record_metric(metric)
                
                # Обновляем статистику
                target['last_collection'] = time.time()
                self.stats['metrics_collected'] += len(metrics)
                
                # Проверяем алерты
                await self._check_alerts(metrics)
                
                # Вычисляем время до следующего сбора
                collection_time = time.time() - start_time
                sleep_time = max(0, target['interval'] - collection_time)
                
                await asyncio.sleep(sleep_time)
            
            except asyncio.CancelledError:
                break
            except Exception as e:
                target['errors'] += 1
                self.stats['collection_errors'] += 1
                self.logger.error(f"Collection error for {target_id}: {e}")
                await asyncio.sleep(target['interval'])
    
    async def _collect_system_metrics(self) -> List[Metric]:
        """Сбор системных метрик"""
        
        metrics = []
        current_time = time.time()
        
        # CPU метрики
        cpu_percent = psutil.cpu_percent(interval=1)
        metrics.append(Metric(
            name="system_cpu_usage_percent",
            value=cpu_percent,
            timestamp=current_time,
            labels={"host": socket.gethostname()}
        ))
        
        # Memory метрики
        memory = psutil.virtual_memory()
        metrics.extend([
            Metric(
                name="system_memory_usage_percent",
                value=memory.percent,
                timestamp=current_time,
                labels={"host": socket.gethostname()}
            ),
            Metric(
                name="system_memory_available_bytes",
                value=memory.available,
                timestamp=current_time,
                labels={"host": socket.gethostname()}
            )
        ])
        
        # Disk метрики
        disk = psutil.disk_usage('/')
        metrics.extend([
            Metric(
                name="system_disk_usage_percent",
                value=(disk.used / disk.total) * 100,
                timestamp=current_time,
                labels={"host": socket.gethostname(), "mountpoint": "/"}
            ),
            Metric(
                name="system_disk_free_bytes",
                value=disk.free,
                timestamp=current_time,
                labels={"host": socket.gethostname(), "mountpoint": "/"}
            )
        ])
        
        # Load average (Unix-like systems)
        try:
            load_avg = psutil.getloadavg()
            for i, period in enumerate(['1min', '5min', '15min']):
                metrics.append(Metric(
                    name="system_load_average",
                    value=load_avg[i],
                    timestamp=current_time,
                    labels={"host": socket.gethostname(), "period": period}
                ))
        except AttributeError:
            pass  # Windows doesn't have load average
        
        return metrics
    
    async def _collect_network_metrics(self) -> List[Metric]:
        """Сбор сетевых метрик"""
        
        metrics = []
        current_time = time.time()
        hostname = socket.gethostname()
        
        # Network I/O
        net_io = psutil.net_io_counters()
        metrics.extend([
            Metric(
                name="network_bytes_sent_total",
                value=net_io.bytes_sent,
                timestamp=current_time,
                labels={"host": hostname},
                metric_type=MetricType.COUNTER
            ),
            Metric(
                name="network_bytes_received_total",
                value=net_io.bytes_recv,
                timestamp=current_time,
                labels={"host": hostname},
                metric_type=MetricType.COUNTER
            ),
            Metric(
                name="network_packets_sent_total",
                value=net_io.packets_sent,
                timestamp=current_time,
                labels={"host": hostname},
                metric_type=MetricType.COUNTER
            ),
            Metric(
                name="network_packets_received_total",
                value=net_io.packets_recv,
                timestamp=current_time,
                labels={"host": hostname},
                metric_type=MetricType.COUNTER
            )
        ])
        
        # Network connections
        connections = psutil.net_connections()
        connection_states = defaultdict(int)
        
        for conn in connections:
            if conn.status:
                connection_states[conn.status] += 1
        
        for state, count in connection_states.items():
            metrics.append(Metric(
                name="network_connections_by_state",
                value=count,
                timestamp=current_time,
                labels={"host": hostname, "state": state}
            ))
        
        # Network interfaces
        net_if_stats = psutil.net_if_stats()
        for interface, stats in net_if_stats.items():
            metrics.extend([
                Metric(
                    name="network_interface_up",
                    value=1 if stats.isup else 0,
                    timestamp=current_time,
                    labels={"host": hostname, "interface": interface}
                ),
                Metric(
                    name="network_interface_speed_mbps",
                    value=stats.speed,
                    timestamp=current_time,
                    labels={"host": hostname, "interface": interface}
                )
            ])
        
        return metrics
    
    async def record_metric(self, metric: Metric):
        """Запись метрики"""
        
        # Добавляем в хранилище
        metric_key = self._get_metric_key(metric)
        self.metrics[metric_key].append(metric)
        
        # Сохраняем метаданные
        if metric_key not in self.metric_metadata:
            self.metric_metadata[metric_key] = {
                'name': metric.name,
                'type': metric.metric_type.value,
                'labels': metric.labels,
                'first_seen': metric.timestamp,
                'sample_count': 0
            }
        
        self.metric_metadata[metric_key]['sample_count'] += 1
        self.metric_metadata[metric_key]['last_seen'] = metric.timestamp
    
    def _get_metric_key(self, metric: Metric) -> str:
        """Генерация ключа метрики"""
        
        labels_str = ",".join(f"{k}={v}" for k, v in sorted(metric.labels.items()))
        return f"{metric.name}{{{labels_str}}}"
    
    async def _check_alerts(self, metrics: List[Metric]):
        """Проверка правил алертинга"""
        
        for rule in self.alert_rules:
            try:
                await self._evaluate_alert_rule(rule, metrics)
            except Exception as e:
                self.logger.error(f"Error evaluating alert rule {rule['name']}: {e}")
    
    async def _evaluate_alert_rule(self, rule: Dict, metrics: List[Metric]):
        """Оценка правила алертинга"""
        
        # Простая реализация - в продакшене был бы более сложный DSL
        condition = rule['condition']
        
        # Ищем метрики, соответствующие условию
        for metric in metrics:
            if self._matches_condition(metric, condition):
                await self._fire_alert(rule, metric)
    
    def _matches_condition(self, metric: Metric, condition: Dict) -> bool:
        """Проверка соответствия метрики условию"""
        
        # Пример: {"metric": "system_cpu_usage_percent", "operator": ">", "threshold": 80}
        if condition.get('metric') != metric.name:
            return False
        
        operator = condition.get('operator', '>')
        threshold = condition.get('threshold', 0)
        
        if operator == '>':
            return metric.value > threshold
        elif operator == '<':
            return metric.value < threshold
        elif operator == '>=':
            return metric.value >= threshold
        elif operator == '<=':
            return metric.value <= threshold
        elif operator == '==':
            return metric.value == threshold
        elif operator == '!=':
            return metric.value != threshold
        
        return False
    
    async def _fire_alert(self, rule: Dict, metric: Metric):
        """Создание алерта"""
        
        alert_id = f"{rule['name']}_{int(metric.timestamp)}"
        
        # Проверяем, не дублируется ли алерт
        if alert_id in self.alerts and not self.alerts[alert_id].resolved:
            return
        
        alert = Alert(
            id=alert_id,
            name=rule['name'],
            severity=AlertSeverity(rule['severity']),
            message=rule['message'].format(
                metric_name=metric.name,
                value=metric.value,
                **metric.labels
            ),
            timestamp=metric.timestamp,
            labels={**metric.labels, **rule.get('labels', {})}
        )
        
        self.alerts[alert_id] = alert
        self.stats['alerts_fired'] += 1
        
        # Вызываем callbacks
        for callback in self.alert_callbacks:
            try:
                await callback(alert)
            except Exception as e:
                self.logger.error(f"Alert callback error: {e}")
        
        self.logger.warning(f"ALERT: {alert.name} - {alert.message}")
    
    def start_trace(self, operation_name: str, parent_span_id: Optional[str] = None) -> str:
        """Начало трассировки"""
        
        trace_id = str(uuid.uuid4())
        span_id = str(uuid.uuid4())
        
        span = TraceSpan(
            trace_id=trace_id,
            span_id=span_id,
            parent_span_id=parent_span_id,
            operation_name=operation_name,
            start_time=time.time()
        )
        
        self.active_spans[span_id] = span
        self.traces[trace_id].append(span)
        
        return span_id
    
    def add_span_tag(self, span_id: str, key: str, value: Any):
        """Добавление тега к span"""
        
        if span_id in self.active_spans:
            self.active_spans[span_id].tags[key] = value
    
    def add_span_log(self, span_id: str, message: str, **kwargs):
        """Добавление лога к span"""
        
        if span_id in self.active_spans:
            log_entry = {
                'timestamp': time.time(),
                'message': message,
                **kwargs
            }
            self.active_spans[span_id].logs.append(log_entry)
    
    def finish_span(self, span_id: str):
        """Завершение span"""
        
        if span_id in self.active_spans:
            span = self.active_spans[span_id]
            span.finish()
            
            del self.active_spans[span_id]
            self.stats['traces_collected'] += 1
    
    async def collect_http_metrics(self, method: str, endpoint: str, 
                                 status_code: int, duration_ms: float, 
                                 request_size: int = 0, response_size: int = 0):
        """Сбор HTTP метрик"""
        
        current_time = time.time()
        labels = {
            'method': method,
            'endpoint': endpoint,
            'status_code': str(status_code),
            'host': socket.gethostname()
        }
        
        metrics = [
            Metric(
                name="http_requests_total",
                value=1,
                timestamp=current_time,
                labels=labels,
                metric_type=MetricType.COUNTER
            ),
            Metric(
                name="http_request_duration_ms",
                value=duration_ms,
                timestamp=current_time,
                labels=labels,
                metric_type=MetricType.HISTOGRAM
            ),
            Metric(
                name="http_request_size_bytes",
                value=request_size,
                timestamp=current_time,
                labels=labels,
                metric_type=MetricType.HISTOGRAM
            ),
            Metric(
                name="http_response_size_bytes",
                value=response_size,
                timestamp=current_time,
                labels=labels,
                metric_type=MetricType.HISTOGRAM
            )
        ]
        
        for metric in metrics:
            await self.record_metric(metric)
    
    def query_metrics(self, metric_name: str, labels: Optional[Dict] = None, 
                     start_time: Optional[float] = None, 
                     end_time: Optional[float] = None) -> List[Metric]:
        """Запрос метрик"""
        
        results = []
        
        for metric_key, metric_queue in self.metrics.items():
            # Проверяем соответствие имени
            if not metric_key.startswith(metric_name):
                continue
            
            for metric in metric_queue:
                # Проверяем временные рамки
                if start_time and metric.timestamp < start_time:
                    continue
                if end_time and metric.timestamp > end_time:
                    continue
                
                # Проверяем лейблы
                if labels:
                    if not all(metric.labels.get(k) == v for k, v in labels.items()):
                        continue
                
                results.append(metric)
        
        return sorted(results, key=lambda m: m.timestamp)
    
    def get_trace(self, trace_id: str) -> List[TraceSpan]:
        """Получение трассировки"""
        
        return self.traces.get(trace_id, [])
    
    async def _cleanup_loop(self):
        """Очистка старых данных"""
        
        while True:
            try:
                current_time = time.time()
                cutoff_time = current_time - self.retention_period
                
                # Очищаем старые метрики
                for metric_key, metric_queue in self.metrics.items():
                    # Удаляем старые метрики
                    while metric_queue and metric_queue[0].timestamp < cutoff_time:
                        metric_queue.popleft()
                
                # Очищаем старые трассировки
                traces_to_remove = []
                for trace_id, spans in self.traces.items():
                    if spans and spans[0].start_time < cutoff_time:
                        traces_to_remove.append(trace_id)
                
                for trace_id in traces_to_remove:
                    del self.traces[trace_id]
                
                # Очищаем решенные алерты
                resolved_alerts = [
                    alert_id for alert_id, alert in self.alerts.items()
                    if alert.resolved and alert.resolved_at and alert.resolved_at < cutoff_time
                ]
                
                for alert_id in resolved_alerts:
                    del self.alerts[alert_id]
                
                await asyncio.sleep(3600)  # Очистка каждый час
            
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Cleanup error: {e}")
                await asyncio.sleep(3600)
    
    def get_monitoring_summary(self) -> Dict:
        """Получение сводки мониторинга"""
        
        current_time = time.time()
        
        # Статистика метрик
        total_metrics = sum(len(queue) for queue in self.metrics.values())
        unique_metrics = len(self.metric_metadata)
        
        # Статистика алертов
        active_alerts = len([a for a in self.alerts.values() if not a.resolved])
        alerts_by_severity = defaultdict(int)
        
        for alert in self.alerts.values():
            if not alert.resolved:
                alerts_by_severity[alert.severity.value] += 1
        
        # Статистика трассировок
        total_traces = len(self.traces)
        active_spans = len(self.active_spans)
        
        # Статистика коллекторов
        collector_stats = {}
        for target_id, target in self.monitoring_targets.items():
            collector_stats[target_id] = {
                'name': target['name'],
                'last_collection_ago': current_time - target['last_collection'],
                'errors': target['errors']
            }
        
        return {
            'uptime_seconds': current_time - self.stats.get('start_time', current_time),
            'metrics': {
                'total_stored': total_metrics,
                'unique_metrics': unique_metrics,
                'collected_total': self.stats['metrics_collected']
            },
            'alerts': {
                'active': active_alerts,
                'by_severity': dict(alerts_by_severity),
                'fired_total': self.stats['alerts_fired']
            },
            'traces': {
                'total_traces': total_traces,
                'active_spans': active_spans,
                'collected_total': self.stats['traces_collected']
            },
            'collectors': collector_stats,
            'errors': {
                'collection_errors': self.stats['collection_errors']
            }
        }
    
    async def shutdown(self):
        """Graceful shutdown"""
        
        self.logger.info("Shutting down monitoring system...")
        
        # Останавливаем коллекторы
        for task in self.collection_tasks.values():
            task.cancel()
        
        if self.collection_tasks:
            await asyncio.gather(*self.collection_tasks.values(), return_exceptions=True)
        
        # Останавливаем cleanup
        if self.cleanup_task:
            self.cleanup_task.cancel()
        
        self.logger.info("Monitoring system shutdown completed")

# HTTP Monitoring Middleware
def create_monitoring_middleware(monitoring_system: NetworkMonitoringSystem):
    """Создание middleware для мониторинга HTTP запросов"""
    
    async def monitoring_middleware(request, handler):
        # Начинаем трассировку
        span_id = monitoring_system.start_trace(
            f"HTTP {request.method} {request.path}"
        )
        
        # Добавляем теги
        monitoring_system.add_span_tag(span_id, 'http.method', request.method)
        monitoring_system.add_span_tag(span_id, 'http.url', str(request.url))
        monitoring_system.add_span_tag(span_id, 'http.user_agent', request.headers.get('User-Agent', ''))
        
        start_time = time.time()
        request_size = int(request.headers.get('Content-Length', 0))
        
        try:
            # Выполняем запрос
            response = await handler(request)
            
            # Метрики успешного запроса
            duration_ms = (time.time() - start_time) * 1000
            response_size = len(getattr(response, 'body', b''))
            
            await monitoring_system.collect_http_metrics(
                method=request.method,
                endpoint=request.path,
                status_code=response.status,
                duration_ms=duration_ms,
                request_size=request_size,
                response_size=response_size
            )
            
            # Обновляем span
            monitoring_system.add_span_tag(span_id, 'http.status_code', response.status)
            monitoring_system.add_span_log(span_id, f"Request completed successfully")
            
            return response
        
        except Exception as e:
            # Метрики ошибки
            duration_ms = (time.time() - start_time) * 1000
            
            await monitoring_system.collect_http_metrics(
                method=request.method,
                endpoint=request.path,
                status_code=500,
                duration_ms=duration_ms,
                request_size=request_size,
                response_size=0
            )
            
            # Обновляем span с ошибкой
            monitoring_system.add_span_tag(span_id, 'error', True)
            monitoring_system.add_span_tag(span_id, 'http.status_code', 500)
            monitoring_system.add_span_log(span_id, f"Request failed: {str(e)}")
            
            raise
        
        finally:
            # Завершаем span
            monitoring_system.finish_span(span_id)
    
    return monitoring_middleware

# Демонстрация системы мониторинга
async def demonstrate_monitoring_system():
    """Демонстрация системы мониторинга сетей"""
    
    print("📊 Network Monitoring & Observability Demonstration")
    print("=" * 70)
    
    # Создаем систему мониторинга
    monitoring = NetworkMonitoringSystem()
    
    # Добавляем правила алертинга
    alert_rules = [
        {
            'name': 'high_cpu_usage',
            'condition': {
                'metric': 'system_cpu_usage_percent',
                'operator': '>',
                'threshold': 80
            },
            'severity': 'warning',
            'message': 'High CPU usage detected: {value}% on {host}'
        },
        {
            'name': 'low_disk_space',
            'condition': {
                'metric': 'system_disk_usage_percent',
                'operator': '>',
                'threshold': 90
            },
            'severity': 'critical',
            'message': 'Low disk space: {value}% used on {host}'
        },
        {
            'name': 'high_memory_usage',
            'condition': {
                'metric': 'system_memory_usage_percent',
                'operator': '>',
                'threshold': 85
            },
            'severity': 'warning',
            'message': 'High memory usage: {value}% on {host}'
        }
    ]
    
    # Добавляем правила
    for rule in alert_rules:
        monitoring.add_alert_rule(rule)
    
    # Добавляем callback для алертов
    async def alert_callback(alert: Alert):
        print(f"🚨 ALERT: [{alert.severity.value.upper()}] {alert.name}")
        print(f"   Message: {alert.message}")
        print(f"   Time: {datetime.fromtimestamp(alert.timestamp)}")
    
    monitoring.add_alert_callback(alert_callback)
    
    # Запускаем мониторинг
    print("🔄 Starting monitoring system...")
    await monitoring.start_monitoring()
    
    # Даем системе поработать
    print("📈 Collecting metrics for 30 seconds...")
    await asyncio.sleep(30)
    
    # Демонстрируем трассировку
    print("\n🔍 Demonstrating distributed tracing:")
    
    # Создаем трассировку HTTP запроса
    span_id = monitoring.start_trace("HTTP GET /api/users")
    monitoring.add_span_tag(span_id, 'http.method', 'GET')
    monitoring.add_span_tag(span_id, 'http.url', '/api/users')
    monitoring.add_span_log(span_id, "Starting user lookup")
    
    # Симулируем обращение к базе дан    def _least_connections_select(self, backends: List[Backend]) -> Backend:
        """Least Connections алгоритм"""
        
        return min(backends, key=lambda b: b.current_connections)
    
    def _weighted_least_connections_select(self, backends: List[Backend]) -> Backend:
        """Weighted Least Connections алгоритм"""
        
        def weighted_connections(backend: Backend) -> float:
            if backend.effective_weight == 0:
                return float('inf')
            return backend.current_connections / backend.effective_weight
        
        return min(backends, key=weighted_connections)
    
    def _least_response_time_select(self, backends: List[Backend]) -> Backend:
        """Least Response Time алгоритм"""
        
        def combined_score(backend: Backend) -> float:
            # Комбинируем время ответа и количество соединений
            response_time = backend.avg_response_time if backend.response_times else 1000
            connections = backend.current_connections
            return response_time * (1 + connections * 0.1)
        
        return min(backends, key=combined_score)
    
    def _ip_hash_select(self, backends: List[Backend], client_ip: str) -> Backend:
        """IP Hash алгоритм"""
        
        if not client_ip:
            return self._round_robin_select(backends)
        
        # Создаем хеш от IP
        ip_hash = hashlib.md5(client_ip.encode()).hexdigest()
        hash_int = int(ip_hash, 16)
        
        # Выбираем backend по хешу
        index = hash_int % len(backends)
        return backends[index]
    
    def _consistent_hash_select(self, client_ip: str) -> Optional[Backend]:
        """Consistent Hash алгоритм"""
        
        if not client_ip or not self.hash_ring:
            healthy_backends = [b for b in self.backends.values() if b.state == BackendState.HEALTHY]
            return self._round_robin_select(healthy_backends) if healthy_backends else None
        
        # Создаем хеш от IP
        ip_hash = hashlib.md5(client_ip.encode()).hexdigest()
        hash_int = int(ip_hash, 16) % (2**32)
        
        # Находим ближайший узел в кольце
        ring_keys = sorted(self.hash_ring.keys())
        
        for key in ring_keys:
            if hash_int <= key:
                backend_id = self.hash_ring[key]
                if backend_id in self.backends and self.backends[backend_id].state == BackendState.HEALTHY:
                    return self.backends[backend_id]
        
        # Если не нашли, берем первый
        if ring_keys:
            backend_id = self.hash_ring[ring_keys[0]]
            if backend_id in self.backends and self.backends[backend_id].state == BackendState.HEALTHY:
                return self.backends[backend_id]
        
        return None
    
    def _random_select(self, backends: List[Backend]) -> Backend:
        """Random алгоритм"""
        
        return random.choice(backends)
    
    def _weighted_random_select(self, backends: List[Backend]) -> Backend:
        """Weighted Random алгоритм"""
        
        # Создаем список весов
        weights = [backend.effective_weight for backend in backends]
        total_weight = sum(weights)
        
        if total_weight == 0:
            return random.choice(backends)
        
        # Выбираем случайно с учетом весов
        random_weight = random.uniform(0, total_weight)
        current_weight = 0
        
        for i, backend in enumerate(backends):
            current_weight += weights[i]
            if random_weight <= current_weight:
                return backend
        
        return backends[-1]  # Fallback
    
    def _update_hash_ring(self):
        """Обновление consistent hash кольца"""
        
        self.hash_ring.clear()
        
        for backend_id, backend in self.backends.items():
            if backend.state == BackendState.HEALTHY:
                # Создаем виртуальные узлы
                for i in range(self.virtual_nodes):
                    virtual_key = f"{backend_id}:{i}"
                    hash_value = hashlib.md5(virtual_key.encode()).hexdigest()
                    hash_int = int(hash_value, 16) % (2**32)
                    self.hash_ring[hash_int] = backend_id
    
    def _is_circuit_breaker_closed(self, backend_id: str) -> bool:
        """Проверка состояния circuit breaker"""
        
        if backend_id not in self.circuit_breakers:
            return True
        
        cb = self.circuit_breakers[backend_id]
        
        if cb['state'] == 'closed':
            return True
        elif cb['state'] == 'open':
            # Проверяем таймаут для recovery
            if time.time() - cb['last_failure'] >= cb['recovery_timeout']:
                cb['state'] = 'half_open'
                cb['failure_count'] = 0
                return True
            return False
        elif cb['state'] == 'half_open':
            return True
        
        return False
    
    def _record_request_result(self, backend_id: str, success: bool, response_time: float = 0):
        """Запись результата запроса"""
        
        if backend_id not in self.backends:
            return
        
        backend = self.backends[backend_id]
        backend.total_requests += 1
        
        if success:
            backend.response_times.append(response_time)
            self.stats['successful_requests'] += 1
            
            # Circuit breaker recovery
            if backend_id in self.circuit_breakers:
                cb = self.circuit_breakers[backend_id]
                if cb['state'] == 'half_open':
                    cb['failure_count'] = 0
                    cb['state'] = 'closed'
        else:
            backend.failed_requests += 1
            self.stats['failed_requests'] += 1
            
            # Circuit breaker trip
            if backend_id in self.circuit_breakers:
                cb = self.circuit_breakers[backend_id]
                cb['failure_count'] += 1
                cb['last_failure'] = time.time()
                
                if cb['failure_count'] >= 5:  # Threshold
                    cb['state'] = 'open'
                    self.logger.warning(f"Circuit breaker opened for backend {backend_id}")
        
        self.stats['total_requests'] += 1
    
    async def make_request(self, client_ip: str, method: str = 'GET', 
                          path: str = '/', data: Any = None, headers: Dict = None) -> Dict:
        """Выполнение запроса через load balancer"""
        
        backend = await self.select_backend(client_ip)
        
        if not backend:
            return {
                'success': False,
                'error': 'No healthy backends available',
                'status_code': 503
            }
        
        # Увеличиваем счетчик соединений
        backend.current_connections += 1
        
        start_time = time.time()
        
        try:
            url = f"http://{backend.host}:{backend.port}{path}"
            
            async with aiohttp.ClientSession() as session:
                async with session.request(
                    method, url, 
                    json=data if method in ['POST', 'PUT', 'PATCH'] else None,
                    headers=headers or {},
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    
                    response_time = (time.time() - start_time) * 1000  # в мс
                    response_data = await response.text()
                    
                    # Записываем результат
                    self._record_request_result(backend.id, True, response_time)
                    
                    return {
                        'success': True,
                        'status_code': response.status,
                        'data': response_data,
                        'response_time_ms': response_time,
                        'backend_id': backend.id
                    }
        
        except Exception as e:
            response_time = (time.time() - start_time) * 1000
            
            # Записываем ошибку
            self._record_request_result(backend.id, False, response_time)
            
            return {
                'success': False,
                'error': str(e),
                'status_code': 0,
                'response_time_ms': response_time,
                'backend_id': backend.id
            }
        
        finally:
            # Уменьшаем счетчик соединений
            backend.current_connections = max(0, backend.current_connections - 1)
    
    def _start_health_check(self, backend_id: str):
        """Запуск health check для backend"""
        
        async def health_check_loop():
            backend = self.backends[backend_id]
            consecutive_failures = 0
            consecutive_successes = 0
            
            while backend_id in self.backends:
                try:
                    start_time = time.time()
                    
                    url = f"http://{backend.host}:{backend.port}{self.health_config.path}"
                    
                    async with aiohttp.ClientSession() as session:
                        async with session.get(
                            url, 
                            timeout=aiohttp.ClientTimeout(total=self.health_config.timeout)
                        ) as response:
                            
                            response_time = (time.time() - start_time) * 1000
                            
                            if response.status == self.health_config.expected_status:
                                # Health check успешен
                                consecutive_failures = 0
                                consecutive_successes += 1
                                
                                # Восстанавливаем backend если нужно
                                if (backend.state == BackendState.UNHEALTHY and 
                                    consecutive_successes >= self.health_config.max_successes_to_recover):
                                    backend.state = BackendState.HEALTHY
                                    backend.health_score = 1.0
                                    self.logger.info(f"Backend {backend_id} recovered")
                                
                                # Улучшаем health score
                                backend.health_score = min(1.0, backend.health_score + 0.1)
                                
                            else:
                                consecutive_successes = 0
                                consecutive_failures += 1
                                backend.health_score = max(0.1, backend.health_score - 0.2)
                
                except Exception as e:
                    consecutive_successes = 0
                    consecutive_failures += 1
                    backend.health_score = max(0.1, backend.health_score - 0.3)
                    
                    self.logger.debug(f"Health check failed for {backend_id}: {e}")
                
                # Помечаем backend как нездоровый при необходимости
                if consecutive_failures >= self.health_config.max_failures:
                    if backend.state == BackendState.HEALTHY:
                        backend.state = BackendState.UNHEALTHY
                        self.logger.warning(f"Backend {backend_id} marked as unhealthy")
                
                backend.last_health_check = time.time()
                
                await asyncio.sleep(self.health_config.interval)
        
        # Запускаем задачу health check
        task = asyncio.create_task(health_check_loop())
        self.health_check_tasks[backend_id] = task
    
    def get_stats(self) -> Dict:
        """Получение статистики load balancer"""
        
        backend_stats = {}
        for backend_id, backend in self.backends.items():
            backend_stats[backend_id] = {
                'host': f"{backend.host}:{backend.port}",
                'state': backend.state.value,
                'weight': backend.weight,
                'effective_weight': backend.effective_weight,
                'current_connections': backend.current_connections,
                'total_requests': backend.total_requests,
                'failed_requests': backend.failed_requests,
                'failure_rate': backend.failure_rate,
                'avg_response_time': backend.avg_response_time,
                'health_score': backend.health_score,
                'circuit_breaker_state': self.circuit_breakers.get(backend_id, {}).get('state', 'closed')
            }
        
        # Общая статистика
        total_requests = self.stats['total_requests']
        success_rate = (self.stats['successful_requests'] / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'algorithm': self.algorithm.value,
            'total_backends': len(self.backends),
            'healthy_backends': len([b for b in self.backends.values() if b.state == BackendState.HEALTHY]),
            'total_requests': total_requests,
            'success_rate_percent': success_rate,
            'backend_stats': backend_stats,
            'requests_by_backend': dict(self.stats['requests_by_backend']),
            'circuit_breakers_open': len([cb for cb in self.circuit_breakers.values() if cb['state'] == 'open'])
        }
    
    async def shutdown(self):
        """Graceful shutdown load balancer"""
        
        # Останавливаем все health check задачи
        for task in self.health_check_tasks.values():
            task.cancel()
        
        # Ждем завершения задач
        if self.health_check_tasks:
            await asyncio.gather(*self.health_check_tasks.values(), return_exceptions=True)
        
        self.logger.info("Load balancer shutdown completed")

# Демонстрация Load Balancer
class MockBackendServer:
    """Mock backend сервер для демонстрации"""
    
    def __init__(self, server_id: str, port: int, response_delay: float = 0.1, failure_rate: float = 0.0):
        self.server_id = server_id
        self.port = port
        self.response_delay = response_delay
        self.failure_rate = failure_rate
        self.request_count = 0
        
    async def start(self):
        """Запуск mock сервера"""
        
        from aiohttp import web
        
        async def handle_request(request):
            self.request_count += 1
            
            # Симулируем задержку
            await asyncio.sleep(self.response_delay)
            
            # Симулируем случайные ошибки
            if random.random() < self.failure_rate:
                return web.Response(status=500, text=f"Server {self.server_id} error")
            
            return web.json_response({
                'server_id': self.server_id,
                'request_count': self.request_count,
                'timestamp': time.time()
            })
        
        async def health_check(request):
            return web.json_response({'status': 'healthy', 'server_id': self.server_id})
        
        app = web.Application()
        app.router.add_get('/', handle_request)
        app.router.add_post('/', handle_request)
        app.router.add_get('/health', health_check)
        
        runner = web.AppRunner(app)
        await runner.setup()
        
        site = web.TCPSite(runner, 'localhost', self.port)
        await site.start()
        
        return runner

async def demonstrate_load_balancing():
    """Демонстрация различных алгоритмов балансировки нагрузки"""
    
    print("⚖️ Advanced Load Balancing Demonstration")
    print("=" * 60)
    
    # Запускаем mock backend серверы
    mock_servers = [
        MockBackendServer('server1', 8081, response_delay=0.05, failure_rate=0.0),   # Быстрый
        MockBackendServer('server2', 8082, response_delay=0.1, failure_rate=0.05),   # Средний
        MockBackendServer('server3', 8083, response_delay=0.2, failure_rate=0.1),    # Медленный
        MockBackendServer('server4', 8084, response_delay=0.15, failure_rate=0.0),   # Надежный
    ]
    
    server_runners = []
    
    print("🚀 Starting mock backend servers...")
    
    try:
        for server in mock_servers:
            runner = await server.start()
            server_runners.append(runner)
            print(f"  Started {server.server_id} on port {server.port}")
        
        await asyncio.sleep(2)  # Даем серверам время запуститься
        
        # Тестируем разные алгоритмы
        algorithms_to_test = [
            LoadBalanceAlgorithm.ROUND_ROBIN,
            LoadBalanceAlgorithm.WEIGHTED_LEAST_CONNECTIONS,
            LoadBalanceAlgorithm.LEAST_RESPONSE_TIME,
            LoadBalanceAlgorithm.IP_HASH
        ]
        
        for algorithm in algorithms_to_test:
            print(f"\n📊 Testing {algorithm.value.upper()} algorithm:")
            
            # Создаем load balancer
            lb = AdvancedLoadBalancer(algorithm)
            
            # Добавляем backends
            backends = [
                Backend('server1', 'localhost', 8081, weight=3),  # Высокий вес
                Backend('server2', 'localhost', 8082, weight=2),  # Средний вес
                Backend('server3', 'localhost', 8083, weight=1),  # Низкий вес
                Backend('server4', 'localhost', 8084, weight=2),  # Средний вес
            ]
            
            for backend in backends:
                lb.add_backend(backend)
            
            # Ждем первые health checks
            await asyncio.sleep(3)
            
            # Делаем серию запросов
            print("  Making 50 requests...")
            
            client_ips = ['192.168.1.10', '192.168.1.11', '192.168.1.12', '10.0.0.5', '10.0.0.6']
            successful_requests = 0
            total_response_time = 0
            
            request_distribution = defaultdict(int)
            
            for i in range(50):
                client_ip = client_ips[i % len(client_ips)]
                
                result = await lb.make_request(client_ip, 'GET', '/')
                
                if result['success']:
                    successful_requests += 1
                    total_response_time += result['response_time_ms']
                    request_distribution[result['backend_id']] += 1
                
                # Небольшая пауза между запросами
                if i % 10 == 9:
                    await asyncio.sleep(0.1)
            
            # Статистика
            stats = lb.get_stats()
            avg_response_time = total_response_time / successful_requests if successful_requests > 0 else 0
            
            print(f"  Results:")
            print(f"    Success rate: {stats['success_rate_percent']:.1f}%")
            print(f"    Average response time: {avg_response_time:.1f}ms")
            print(f"    Request distribution:")
            
            for backend_id, count in request_distribution.items():
                backend = lb.backends[backend_id]
                percentage = (count / 50) * 100
                print(f"      {backend_id} (weight {backend.weight}): {count} requests ({percentage:.1f}%)")
            
            print(f"    Backend health scores:")
            for backend_id, backend in lb.backends.items():
                print(f"      {backend_id}: {backend.health_score:.2f}")
            
            # Останавливаем load balancer
            await lb.shutdown()
        
        # Тест устойчивости к отказам
        print(f"\n🛡️ Testing Failure Resilience:")
        
        lb = AdvancedLoadBalancer(LoadBalanceAlgorithm.WEIGHTED_LEAST_CONNECTIONS)
        
        # Добавляем backends
        for backend in backends:
            lb.add_backend(backend)
        
        await asyncio.sleep(2)
        
        print("  Making requests with some servers failing...")
        
        # Симулируем отказ сервера (увеличиваем failure rate)
        mock_servers[2].failure_rate = 0.8  # server3 начинает сильно падать
        
        successful_requests = 0
        
        for i in range(30):
            result = await lb.make_request('192.168.1.100', 'GET', '/')
            
            if result['success']:
                successful_requests += 1
            
            await asyncio.sleep(0.1)
        
        # Проверяем circuit breaker
        stats = lb.get_stats()
        
        print(f"  Results with failing server:")
        print(f"    Success rate: {stats['success_rate_percent']:.1f}%")
        print(f"    Circuit breakers open: {stats['circuit_breakers_open']}")
        
        print(f"    Backend states:")
        for backend_id, backend_info in stats['backend_stats'].items():
            state = backend_info['state']
            cb_state = backend_info['circuit_breaker_state']
            health_score = backend_info['health_score']
            
            print(f"      {backend_id}: {state} (CB: {cb_state}, Health: {health_score:.2f})")
        
        await lb.shutdown()
        
    finally:
        # Останавливаем mock серверы
        print(f"\n🛑 Shutting down mock servers...")
        
        for runner in server_runners:
            await runner.cleanup()
    
    print(f"\n✅ Load balancing demonstration completed")

# Запуск демонстрации
if __name__ == "__main__":
    asyncio.run(demonstrate_load_balancing())
```

### 📝 Практическое задание Неделя 13

1. Реализуйте production-ready load balancer для ваших микросервисов
2. Настройте health checking с автоматическим исключением нездоровых инстансов
3. Протестируйте различные алгоритмы балансировки под нагрузкой
4. Добавьте circuit breaker pattern для повышения устойчивости
5. Интегрируйте load balancer с service discovery системой

---

## Неделя 14: Высокая доступность

### 🧠 Концепция: High Availability Architecture

Принципы проектирования высокодоступных систем:

```
High Availability Components:
┌─────────────────────────────────────────────────────────────┐
│ Geographic Distribution                                     │
│ ├─ Multi-region deployment                                 │
│ ├─ Cross-AZ redundancy                                     │
│ ├─ Edge locations                                          │
│ └─ Disaster recovery sites                                 │
├─────────────────────────────────────────────────────────────┤
│ Application Layer HA                                       │
│ ├─ Horizontal scaling                                      │
│ ├─ Stateless design                                        │
│ ├─ Circuit breakers                                        │
│ └─ Graceful degradation                                    │
├─────────────────────────────────────────────────────────────┤
│ Data Layer HA                                              │
│ ├─ Database replication                                    │
│ ├─ Automatic failover                                      │
│ ├─ Data partitioning/sharding                             │
│ └─ Consistent backups                                      │
├─────────────────────────────────────────────────────────────┤
│ Infrastructure HA                                          │
│ ├─ Load balancer redundancy                               │
│ ├─ Network path diversity                                  │ 
│ ├─ Power/cooling redundancy                               │
│ └─ Hardware failure tolerance                              │
└─────────────────────────────────────────────────────────────┘
```

### 🌍 Geographic Load Balancing & Failover

**Global Load Balancing System:**

```python
import asyncio
import time
import random
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import aiohttp
import logging
from geopy.distance import geodesic
import statistics

class RegionStatus(Enum):
    ACTIVE = "active"
    STANDBY = "standby"
    MAINTENANCE = "maintenance"
    FAILED = "failed"

class FailoverMode(Enum):
    AUTOMATIC = "automatic"
    MANUAL = "manual"
    HYBRID = "hybrid"

@dataclass
class GeographicRegion:
    id: str
    name: str
    country: str
    latitude: float
    longitude: float
    endpoints: List[str]
    capacity: int
    current_load: int = 0
    status: RegionStatus = RegionStatus.ACTIVE
    priority: int = 1  # 1 = highest priority
    health_score: float = 1.0
    last_health_check: float = 0
    response_times: List[float] = field(default_factory=list)
    error_rate: float = 0.0
    
    @property
    def load_percentage(self) -> float:
        return (self.current_load / self.capacity * 100) if self.capacity > 0 else 100
    
    @property
    def avg_response_time(self) -> float:
        return statistics.mean(self.response_times) if self.response_times else 0

@dataclass
class ClientLocation:
    ip: str
    country: str
    latitude: float
    longitude: float

class GlobalLoadBalancer:
    def __init__(self, failover_mode: FailoverMode = FailoverMode.AUTOMATIC):
        self.regions: Dict[str, GeographicRegion] = {}
        self.failover_mode = failover_mode
        self.primary_region_id: Optional[str] = None
        
        # Failover configuration
        self.failover_threshold = 0.3  # Health score threshold for failover
        self.failover_cooldown = 300   # 5 minutes between failovers
        self.last_failover_time = 0
        
        # Traffic routing policies
        self.routing_policies = {
            'latency_based': self._latency_based_routing,
            'geolocation': self._geolocation_routing,
            'weighted': self._weighted_routing,
            'failover': self._failover_routing
        }
        
        self.current_policy = 'latency_based'
        
        # Statistics
        self.stats = {
            'total_requests': 0,
            'requests_by_region': {},
            'failover_events': 0,
            'manual_failovers': 0,
            'automatic_failovers': 0
        }
        
        self.logger = logging.getLogger('GlobalLB')
        
        # Health checking
        self.health_check_interval = 30
        self.health_check_tasks: Dict[str, asyncio.Task] = {}
    
    def add_region(self, region: GeographicRegion):
        """Добавление географического региона"""
        
        self.regions[region.id] = region
        self.stats['requests_by_region'][region.id] = 0
        
        # Если это первый регион, делаем его primary
        if not self.primary_region_id:
            self.primary_region_id = region.id
            region.priority = 1
        
        # Запускаем health check
        self._start_health_check(region.id)
        
        self.logger.info(f"Added region {region.id} ({region.name})")
    
    def set_primary_region(self, region_id: str):
        """Установка основного региона"""
        
        if region_id in self.regions:
            # Сбрасываем приоритет у старого primary
            if self.primary_region_id:
                self.regions[self.primary_region_id].priority = 2
            
            # Устанавливаем новый primary
            self.primary_region_id = region_id
            self.regions[region_id].priority = 1
            
            self.logger.info(f"Set primary region: {region_id}")
    
    async def route_request(self, client_location: ClientLocation, 
                          request_data: Dict = None) -> Optional[str]:
        """Маршрутизация запроса в оптимальный регион"""
        
        self.stats['total_requests'] += 1
        
        # Получаем доступные регионы
        available_regions = [
            region for region in self.regions.values()
            if region.status == RegionStatus.ACTIVE and region.health_score > 0.1
        ]
        
        if not available_regions:
            self.logger.error("No available regions for request routing")
            return None
        
        # Применяем политику маршрутизации
        routing_func = self.routing_policies.get(self.current_policy)
        if not routing_func:
            routing_func = self._latency_based_routing
        
        selected_region = routing_func(client_location, available_regions, request_data)
        
        if selected_region:
            selected_region.current_load += 1
            self.stats['requests_by_region'][selected_region.id] += 1
            
            self.logger.debug(f"Routed request to region {selected_region.id}")
            
            return selected_region.id
        
        return None
    
    def _latency_based_routing(self, client_location: ClientLocation, 
                             regions: List[GeographicRegion], 
                             request_data: Dict = None) -> Optional[GeographicRegion]:
        """Маршрутизация на основе минимальной латентности"""
        
        def calculate_score(region: GeographicRegion) -> float:
            # Географическое расстояние
            distance = geodesic(
                (client_location.latitude, client_location.longitude),
                (region.latitude, region.longitude)
            ).kilometers
            
            # Нормализуем расстояние (0-1, где 0 = лучше)
            max_distance = 20000  # Примерно половина окружности Земли
            distance_score = min(distance / max_distance, 1.0)
            
            # Учитываем загрузку региона
            load_score = region.load_percentage / 100
            
            # Учитываем здоровье региона
            health_score = 1.0 - region.health_score
            
            # Учитываем время ответа
            response_time_score = min(region.avg_response_time / 1000, 1.0)  
            
            # Комбинированная оценка (меньше = лучше)
            combined_score = (
                distance_score * 0.4 +
                load_score * 0.3 + 
                health_score * 0.2 +
                response_time_score * 0.1
            )
            
            return combined_score
        
        # Выбираем регион с наименьшей оценкой
        return min(regions, key=calculate_score)
    
    def _geolocation_routing(self, client_location: ClientLocation,
                           regions: List[GeographicRegion],
                           request_data: Dict = None) -> Optional[GeographicRegion]:
        """Маршрутизация на основе геолокации"""
        
        # Сначала пытаемся найти регион в той же стране
        same_country_regions = [r for r in regions if r.country == client_location.country]
        
        if same_country_regions:
            # Выбираем лучший регион в стране на основе загрузки и здоровья
            def country_score(region: GeographicRegion) -> float:
                return region.load_percentage + (1.0 - region.health_score) * 100
            
            return min(same_country_regions, key=country_score)
        
        # Если в стране нет регионов, используем latency-based
        return self._latency_based_routing(client_location, regions, request_data)
    
    def _weighted_routing(self, client_location: ClientLocation,
                         regions: List[GeographicRegion], 
                         request_data: Dict = None) -> Optional[GeographicRegion]:
        """Взвешенная маршрутизация на основе приоритета"""
        
        # Создаем взвешенный выбор на основе приоритета и состояния
        weights = []
        for region in regions:
            # Базовый вес - обратно пропорционален приоритету
            base_weight = 10 / region.priority
            
            # Корректируем на основе здоровья и загрузки
            health_factor = region.health_score
            load_factor = max(0.1, 1.0 - (region.load_percentage / 100))
            
            effective_weight = base_weight * health_factor * load_factor
            weights.append(effective_weight)
        
        if not weights or sum(weights) == 0:
            return regions[0]
        
        # Weighted random selection
        total_weight = sum(weights)
        random_value = random.uniform(0, total_weight)
        
        current_weight = 0
        for i, region in enumerate(regions):
            current_weight += weights[i]
            if random_value <= current_weight:
                return region
        
        return regions[-1]
    
    def _failover_routing(self, client_location: ClientLocation,
                         regions: List[GeographicRegion],
                         request_data: Dict = None) -> Optional[GeographicRegion]:
        """Failover маршрутизация - primary/standby"""
        
        # Сортируем по приоритету
        sorted_regions = sorted(regions, key=lambda r: r.priority)
        
        # Проверяем primary регион
        primary = sorted_regions[0]
        
        if (primary.status == RegionStatus.ACTIVE and 
            primary.health_score > self.failover_threshold and
            primary.load_percentage < 90):
            return primary
        
        # Primary недоступен, переключаемся на standby
        for region in sorted_regions[1:]:
            if (region.status == RegionStatus.ACTIVE and
                region.health_score > 0.5):
                
                # Логируем failover событие
                if region != primary:
                    self._trigger_failover(primary.id, region.id, "automatic")
                
                return region
        
        return None
    
    def _trigger_failover(self, from_region: str, to_region: str, failover_type: str):
        """Запуск процедуры failover"""
        
        current_time = time.time()
        
        # Проверяем cooldown period
        if current_time - self.last_failover_time < self.failover_cooldown:
            self.logger.info(f"Failover suppressed due to cooldown period")
            return
        
        self.last_failover_time = current_time
        self.stats['failover_events'] += 1
        
        if failover_type == "automatic":
            self.stats['automatic_failovers'] += 1
        else:
            self.stats['manual_failovers'] += 1
        
        self.logger.warning(f"FAILOVER: {from_region} -> {to_region} ({failover_type})")
        
        # Можно добавить дополнительную логику:
        # - Уведомления команде
        # - Логирование в внешние системы
        # - Автоматическое масштабирование целевого региона
    
    async def manual_failover(self, to_region_id: str) -> bool:
        """Ручной failover на указанный регион"""
        
        if to_region_id not in self.regions:
            self.logger.error(f"Region {to_region_id} not found for manual failover")
            return False
        
        target_region = self.regions[to_region_id]
        
        if target_region.status != RegionStatus.ACTIVE:
            self.logger.error(f"Region {to_region_id} is not active for failover")
            return False
        
        # Переключаем primary
        old_primary = self.primary_region_id
        self.set_primary_region(to_region_id)
        
        if old_primary:
            self._trigger_failover(old_primary, to_region_id, "manual")
        
        return True
    
    def _start_health_check(self, region_id: str):
        """Запуск health check для региона"""
        
        async def health_check_loop():
            region = self.regions[region_id]
            
            while region_id in self.regions:
                try:
                    # Проверяем каждый endpoint в регионе
                    response_times = []
                    successful_checks = 0
                    
                    for endpoint in region.endpoints:
                        try:
                            start_time = time.time()
                            
                            async with aiohttp.ClientSession() as session:
                                async with session.get(
                                    f"{endpoint}/health",
                                    timeout=aiohttp.ClientTimeout(total=10)
                                ) as response:
                                    
                                    response_time = (time.time() - start_time) * 1000
                                    response_times.append(response_time)
                                    
                                    if response.status == 200:
                                        successful_checks += 1
                        
                        except Exception as e:
                            self.logger.debug(f"Health check failed for {endpoint}: {e}")
                    
                    # Вычисляем health score
                    if region.endpoints:
                        success_rate = successful_checks / len(region.endpoints)
                        
                        # Обновляем health score
                        if success_rate > 0.8:
                            region.health_score = min(1.0, region.health_score + 0.1)
                        elif success_rate > 0.5:
                            region.health_score = max(0.1, region.health_score - 0.05)
                        else:
                            region.health_score = max(0.1, region.health_score - 0.2)
                        
                        # Обновляем время ответа
                        if response_times:
                            region.response_times = response_times[-10:]  # Храним последние 10
                        
                        # Автоматический failover если нужно
                        if (self.failover_mode in [FailoverMode.AUTOMATIC, FailoverMode.HYBRID] and
                            region.id == self.primary_region_id and
                            region.health_score < self.failover_threshold):
                            
                            # Ищем лучший альтернативный регион
                            alternative_regions = [
                                r for r in self.regions.values()
                                if r.id != region.id and r.status == RegionStatus.ACTIVE
                            ]
                            
                            if alternative_regions:
                                best_alternative = max(alternative_regions, key=lambda r: r.health_score)
                                if best_alternative.health_score > 0.7:
                                    self._trigger_failover(region.id, best_alternative.id, "automatic")
                                    self.set_primary_region(best_alternative.id)
                    
                    region.last_health_check = time.time()
                
                except Exception as e:
                    self.logger.error(f"Health check error for region {region_id}: {e}")
                
                await asyncio.sleep(self.health_check_interval)
        
        task = asyncio.create_task(health_check_loop())
        self.health_check_tasks[region_id] = task
    
    def get_global_stats(self) -> Dict:
        """Получение глобальной статистики"""
        
        region_stats = {}
        total_capacity = 0
        total_load = 0
        
        for region_id, region in self.regions.items():
            region_stats[region_id] = {
                'name': region.name,
                'country': region.country,
                'status': region.status.value,
                'priority': region.priority,
                'health_score': region.health_score,
                'load_percentage': region.load_percentage,
                'avg_response_time': region.avg_response_time,
                'requests_handled': self.stats['requests_by_region'].get(region_id, 0),
                'is_primary': region_id == self.primary_region_id
            }
            
            total_capacity += region.capacity
            total_load += region.current_load
        
        global_load_percentage = (total_load / total_capacity * 100) if total_capacity > 0 else 0
        
        return {
            'total_requests': self.stats['total_requests'],
            'total_regions': len(self.regions),
            'active_regions': len([r for r in self.regions.values() if r.status == RegionStatus.ACTIVE]),
            'primary_region': self.primary_region_id,
            'current_routing_policy': self.current_policy,
            'failover_mode': self.failover_mode.value,
            'global_load_percentage': global_load_percentage,
            'failover_events': self.stats['failover_events'],
            'automatic_failovers': self.stats['automatic_failovers'],
            'manual_failovers': self.stats['manual_failovers'],
            'region_stats': region_stats
        }
    
    async def shutdown(self):
        """Graceful shutdown"""
        
        for task in self.health_check_tasks.values():
            task.cancel()
        
        if self.health_check_tasks:
            await asyncio.gather(*self.health_check_tasks.values(), return_exceptions=True)

# Демонстрация Global Load Balancer
async def demonstrate_global_load_balancing():
    """Демонстрация глобальной балансировки нагрузки"""
    
    print("🌍 Global Load Balancing & High Availability Demonstration")
    print("=" * 70)
    
    # Создаем глобальный load balancer
    glb = GlobalLoadBalancer(FailoverMode.AUTOMATIC)
    
    # Добавляем географические регионы
    regions = [
        GeographicRegion(
            id='us-east',
            name='US East Coast',
            country='US',
            latitude=40.7128,
            longitude=-74.0060,
            endpoints=['https://us-east-1.example.com', 'https://us-east-2.example.com'],
            capacity=1000
        ),
        GeographicRegion(
            id='us-west',
            name='US West Coast', 
            country='US',
            latitude=37.7749,
            longitude=-122.4194,
            endpoints=['https://us-west-1.example.com'],
            capacity=800
        ),
        GeographicRegion(
            id='eu-west',
            name='Europe West',
            country='IE',
            latitude=53.3498,
            longitude=-6.2603,
            endpoints=['https://eu-west-1.example.com', 'https://eu-west-2.example.com'],
            capacity=600,
            priority=2
        ),
        GeographicRegion(
            id='ap-southeast',
            name='Asia Pacific Southeast',
            country='SG', 
            latitude=1.3521,
            longitude=103.8198,
            endpoints=['https://ap-southeast-1.example.com'],
            capacity=400,
            priority=3
        )
    ]
    
    print("🌐 Adding geographic regions:")
    for region in regions:
        glb.add_region(region)
        print(f"  Added {region.name} ({region.country}) - Capacity: {region.capacity}")
    
    # Устанавливаем us-east как primary
    glb.set_primary_region('us-east')
    
    await asyncio.sleep(2)  # Даем время для health checks
    
    # Тестируем маршрутизацию для разных клиентов
    print(f"\n📍 Testing routing for different client locations:")
    
    test_clients = [
        ClientLocation('192.168.1.100', 'US', 40.7589, -73.9851),    # New York
        ClientLocation('10.0.0.50', 'US', 37.7749, -122.4194),      # San Francisco  
        ClientLocation('203.0.113.10', 'GB', 51.5074, -0.1278),     # London
        ClientLocation('198.51.100.5', 'SG', 1.3521, 103.8198),     # Singapore
        ClientLocation('172.16.0.100', 'JP', 35.6762, 139.6503),    # Tokyo
    ]
    
    routing_results = {}
    
    for client in test_clients:
        print(f"\n  Client from {client.country}:")
        
        # Тестируем разные политики маршрутизации
        for policy in ['latency_based', 'geolocation', 'weighted', 'failover']:
            glb.current_policy = policy
            
            selected_region = await glb.route_request(client)
            
            if selected_region:
                region = glb.regions[selected_region]
                distance = geodesic(
                    (client.latitude, client.longitude),
                    (region.latitude, region.longitude)
                ).kilometers
                
                print(f"    {policy}: {region.name} (distance: {distance:.0f}km)")
                
                routing_results[f"{client.country}_{policy}"] = {
                    'region': region.name,
                    'distance': distance
                }
            else:
                print(f"    {policy}: No region selected")
    
    # Симулируем нагрузку на регионы
    print(f"\n⚡ Simulating traffic load:")
    
    glb.current_policy = 'latency_based'
    
    # Генерируем трафик
    for i in range(100):
        client = random.choice(test_clients)
        selected_region = await glb.route_request(client)
        
        if selected_region:
            # Симулируем завершение запроса
            glb.regions[selected_region].current_load = max(0, 
                glb.regions[selected_region].current_load - 1)
        
        if i % 20 == 19:
            await asyncio.sleep(0.1)
    
    # Показываем статистику
    stats = glb.get_global_stats()
    
    print(f"  Total requests processed: {stats['total_requests']}")
    print(f"  Global load: {stats['global_load_percentage']:.1f}%")
    print(f"  Primary region: {stats['primary_region']}")
    
    print(f"\n  Request distribution by region:")
    for region_id, region_stats in stats['region_stats'].items():
        requests = region_stats['requests_handled']
        percentage = (requests / stats['total_requests'] * 100) if stats['total_requests'] > 0 else 0
        
        print(f"    {region_stats['name']}: {requests} requests ({percentage:.1f}%)")
    
    # Симулируем отказ primary региона
    print(f"\n🚨 Simulating primary region failure:")
    
    primary_region = glb.regions[glb.primary_region_id]
    print(f"  Degrading health of {primary_region.name}...")
    
    # Постепенно ухудшаем здоровье primary региона
    for i in range(10):
        primary_region.health_score -= 0.1
        await asyncio.sleep(0.5)
        
        if primary_region.health_score <= glb.failover_threshold:
            print(f"  Health score dropped to {primary_region.health_score:.1f}")
            break
    
    # Ждем срабатывания автоматического failover
    await asyncio.sleep(3)
    
    # Проверяем результат failover
    final_stats = glb.get_global_stats()
    
    print(f"\n📊 Post-failover status:")
    print(f"  New primary region: {final_stats['primary_region']}")
    print(f"  Total failover events: {final_stats['failover_events']}")
    print(f"  Automatic failovers: {final_stats['automatic_failovers']}")
    
    print(f"\n  Region health status:")
    for region_id, region_stats in final_stats['region_stats'].items():
        health = region_stats['health_score']
        status = region_stats['status']
        is_primary = region_stats['is_primary']
        
        primary_marker = " (PRIMARY)" if is_primary else ""
        print(f"    {region_stats['name']}: {status} (health: {health:.2f}){primary_marker}")
    
    # Тестируем ручной failover
    print(f"\n🔄 Testing manual failover:")
    
    target_region = 'eu-west'
    success = await glb.manual_failover(target_region)
    
    if success:
        print(f"  ✅ Manual failover to {target_region} successful")
        
        manual_stats = glb.get_global_stats()
        print(f"  New primary: {manual_stats['primary_region']}")
        print(f"  Manual failovers: {manual_stats['manual_failovers']}")
    else:
        print(f"  ❌ Manual failover to {target_region} failed")
    
    # Остановка
    await glb.shutdown()
    
    print(f"\n✅ Global load balancing demonstration completed")

# Запуск демонстрации
if __name__ == "__main__":
    asyncio.run(demonstrate_global_load_balancing())
```

### 📝 Практическое задание Неделя 14

1. Разработайте multi-region архитектуру для вашего приложения
2. Реализуйте автоматический failover между регионами
3. Настройте geographic load balancing с учетом латентности
4. Создайте disaster recovery план с RTO/RPO метриками
5. Протестируйте отказоустойчивость системы при различных сценариях сбоев

### ✅ Контрольные вопросы

- [ ] Понимаете принципы high availability архитектуры?
- [ ] Можете спроектировать geographic load balancing?
- [ ] Знаете как реализовать automatic failover?
- [ ] Умеете рассчитывать RTO и RPO для системы?
- [ ] Понимаете trade-offs между consistency и availability?

---

# Модуль 8: Современные протоколы и паттерны {#module-8}
*Недели 15-16 | Время изучения: 16-20 часов*

## Неделя 15: Продвинутые протоколы

### 🧠 Концепция: Эволюция сетевых протоколов

Современные протоколы решают проблемы масштабирования и real-time коммуникаций:

```
Protocol Evolution Timeline:
┌─────────────────────────────────────────────────────────────┐
│ HTTP/1.1 (1997)                                            │
│ ├─ Text-based, simple                                      │
│ ├─ Head-of-line blocking                                   │
│ └─ Multiple connections needed                             │
├─────────────────────────────────────────────────────────────┤
│ WebSockets (2011)                                          │
│ ├─ Full-duplex communication                               │
│ ├─ Real-time capabilities                                  │
│ └─ Persistent connections                                  │
├─────────────────────────────────────────────────────────────┤
│ HTTP/2 (2015)                                              │
│ ├─ Binary framing                                          │
│ ├─ Multiplexing                                           │
│ └─ Server push                                            │
├─────────────────────────────────────────────────────────────┤
│ gRPC (2016)                                                │
│ ├─ Protocol Buffers                                       │
│ ├─ Streaming support                                      │
│ └─ Strong typing                                          │
├─────────────────────────────────────────────────────────────┤
│ HTTP/3 + QUIC (2022)                                       │
│ ├─ UDP-based transport                                    │
│ ├─ 0-RTT handshake                                        │
│ └─ Connection migration                                   │
└─────────────────────────────────────────────────────────────┘
```

### 🔌 Advanced WebSocket Implementation

**Production-ready WebSocket Server:**

```python
import asyncio
import json
import time
import uuid
from typing import Dict, List, Set, Optional, Callable, Any
from dataclasses import dataclass, field
from enum import Enum
import websockets
import logging
from collections import defaultdict, deque
import statistics
import jwt
import hashlib

class ConnectionState(Enum):
    CONNECTING = "connecting"
    CONNECTED = "connected"
    AUTHENTICATED = "authenticated"
    DISCONNECTING = "disconnecting"
    DISCONNECTED = "disconnected"

class MessageType(Enum):
    PING = "ping"
    PONG = "pong"
    AUTH = "auth"
    SUBSCRIBE = "subscribe"
    UNSUBSCRIBE = "unsubscribe"
    DATA = "data"
    ERROR = "error"
    HEARTBEAT = "heartbeat"

@dataclass
class WebSocketConnection:
    id: str
    websocket: websockets.WebSocketServerProtocol
    user_id: Optional[str] = None
    state: ConnectionState = ConnectionState.CONNECTING
    authenticated: bool = False
    subscriptions: Set[str] = field(default_factory=set)
    connected_at: float = field(default_factory=time.time)
    last_activity: float = field(default_factory=time.time)
    messages_sent: int = 0
    messages_received: int = 0
    bytes_sent: int = 0
    bytes_received: int = 0
    metadata: Dict = field(default_factory=dict)

@dataclass
class Subscription:
    topic: str
    connections: Set[str] = field(default_factory=set)
    message_count: int = 0
    last_message_time: float = 0

class WebSocketServer:
    def __init__(self, host: str = "localhost", port: int = 8765):
        self.host = host
        self.port = port
        
        # Connection management
        self.connections: Dict[str, WebSocketConnection] = {}
        self.subscriptions: Dict[str, Subscription] = {}
        
        # Authentication
        self.jwt_secret = "your-secret-key"  # В продакшене используйте безопасный ключ
        self.auth_required = True
        
        # Message routing
        self.message_handlers: Dict[MessageType, Callable] = {
            MessageType.PING: self._handle_ping,
            MessageType.AUTH: self._handle_auth,
            MessageType.SUBSCRIBE: self._handle_subscribe,
            MessageType.UNSUBSCRIBE: self._handle_unsubscribe,
            MessageType.DATA: self._handle_data,
            MessageType.HEARTBEAT: self._handle_heartbeat
        }
        
        # Configuration
        self.heartbeat_interval = 30  # секунд
        self.connection_timeout = 300  # 5 минут
        self.max_connections = 10000
        self.max_message_size = 1024 * 1024  # 1MB
        
        # Statistics
        self.stats = {
            'total_connections': 0,
            'current_connections': 0,
            'total_messages': 0,
            'messages_by_type': defaultdict(int),
            'bytes_transferred': 0,
            'connection_errors': 0,
            'authentication_failures': 0
        }
        
        # Background tasks
        self.cleanup_task: Optional[asyncio.Task] = None
        self.heartbeat_task: Optional[asyncio.Task] = None
        
        self.logger = logging.getLogger('WebSocketServer')
    
    async def start(self):
        """Запуск WebSocket сервера"""
        
        self.logger.info(f"Starting WebSocket server on {self.host}:{self.port}")
        
        # Запускаем фоновые задачи
        self.cleanup_task = asyncio.create_task(self._cleanup_loop())
        self.heartbeat_task = asyncio.create_task(self._heartbeat_loop())
        
        # Запускаем WebSocket сервер
        async with websockets.serve(
            self._handle_connection,
            self.host,
            self.port,
            max_size=self.max_message_size,
            ping_interval=None,  # Используем собственный heartbeat
            ping_timeout=None
        ):
            self.logger.info("WebSocket server started successfully")
            await asyncio.Future()  # Работаем бесконечно
    
    async def _handle_connection(self, websocket, path):
        """Обработка нового WebSocket соединения"""
        
        connection_id = str(uuid.uuid4())
        remote_address = websocket.remote_address
        
        # Проверяем лимит соединений
        if len(self.connections) >= self.max_connections:
            await websocket.close(code=1013, reason="Server overloaded")
            self.stats['connection_errors'] += 1
            return
        
        # Создаем соединение
        connection = WebSocketConnection(
            id=connection_id,
            websocket=websocket,
            state=ConnectionState.CONNECTED
        )
        
        self.connections[connection_id] = connection
        self.stats['total_connections'] += 1
        self.stats['current_connections'] += 1
        
        self.logger.info(f"New connection {connection_id} from {remote_address}")
        
        try:
            # Отправляем welcome message
            await self._send_message(connection, {
                'type': 'welcome',
                'connection_id': connection_id,
                'server_time': time.time(),
                'auth_required': self.auth_required
            })
            
            # Обрабатываем сообщения
            async for raw_message in websocket:
                await self._process_message(connection, raw_message)
        
        except websockets.exceptions.ConnectionClosed:
            self.logger.info(f"Connection {connection_id} closed normally")
        
        except Exception as e:
            self.logger.error(f"Error handling connection {connection_id}: {e}")
            self.stats['connection_errors'] += 1
        
        finally:
            # Очищаем соединение
            await self._cleanup_connection(connection_id)
    
    async def _process_message(self, connection: WebSocketConnection, raw_message: str):
        """Обработка входящего сообщения"""
        
        try:
            # Парсим JSON
            message = json.loads(raw_message)
            
            # Валидируем структуру сообщения
            if 'type' not in message:
                await self._send_error(connection, "Missing message type")
                return
            
            message_type_str = message['type']
            
            try:
                message_type = MessageType(message_type_str)
            except ValueError:
                await self._send_error(connection, f"Unknown message type: {message_type_str}")
                return
            
            # Проверяем аутентификацию
            if (self.auth_required and 
                not connection.authenticated and 
                message_type != MessageType.AUTH):
                await self._send_error(connection, "Authentication required")
                return
            
            # Обновляем статистику
            connection.messages_received += 1
            connection.bytes_received += len(raw_message)
            connection.last_activity = time.time()
            
            self.stats['total_messages'] += 1
            self.stats['messages_by_type'][message_type.value] += 1
            self.stats['bytes_transferred'] += len(raw_message)
            
            # Вызываем обработчик
            handler = self.message_handlers.get(message_type)
            if handler:
                await handler(connection, message)
            else:
                await self._send_error(connection, f"No handler for message type: {message_type_str}")
        
        except json.JSONDecodeError:
            await self._send_error(connection, "Invalid JSON format")
        
        except Exception as e:
            self.logger.error(f"Error processing message from {connection.id}: {e}")
            await self._send_error(connection, "Internal server error")
    
    async def _handle_ping(self, connection: WebSocketConnection, message: Dict):
        """Обработка ping сообщения"""
        
        await self._send_message(connection, {
            'type': 'pong',
            'timestamp': time.time(),
            'data': message.get('data')
        })
    
    async def _handle_auth(self, connection: WebSocketConnection, message: Dict):
        """Обработка аутентификации"""
        
        token = message.get('token')
        
        if not token:
            await self._send_error(connection, "Authentication token required")
            self.stats['authentication### 📝 Практическое задание Неделя 11

1. Реализуйте многоуровневую систему rate limiting для ваших API
2. Настройте Network IDS для мониторинга трафика между микросервисами
3. Создайте систему автоматического реагирования на security events
4. Настройте whitelist/blacklist управление с автоматическим обновлением
5. Интегрируйте security мониторинг с вашей существующей observability стеком

---

## Неделя 12: Шифрование и аутентификация

### 🧠 Концепция: TLS/SSL в глубину

TLS (Transport Layer Security) - фундамент безопасных сетевых соединений:

```
TLS 1.3 Handshake Process:
┌─────────────────────────────────────────────────────────────┐
│ Client Hello                                                │
│ ├─ Supported cipher suites                                 │
│ ├─ Key exchange algorithms                                  │
│ ├─ Random nonce                                            │
│ └─ Extensions (SNI, ALPN, etc.)                           │
├─────────────────────────────────────────────────────────────┤
│ Server Hello + Certificate + Server Finished              │
│ ├─ Selected cipher suite                                   │
│ ├─ Server certificate chain                               │
│ ├─ Digital signature                                      │
│ └─ Encrypted extensions                                    │
├─────────────────────────────────────────────────────────────┤
│ Client Finished                                            │
│ ├─ Certificate verification                                │
│ ├─ Key derivation                                         │
│ └─ Handshake completion                                    │
├─────────────────────────────────────────────────────────────┤
│ Application Data (Encrypted)                               │
│ ├─ Perfect Forward Secrecy                                │
│ ├─ 0-RTT resumption (optional)                           │
│ └─ Continuous key rotation                                │
└─────────────────────────────────────────────────────────────┘
```

### 🔐 Advanced TLS Configuration

**Production-grade TLS Setup:**

```python
import ssl
import socket
import asyncio
import aiohttp
import time
import hashlib
import base64
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import logging
import cryptography
from cryptography import x509
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import rsa, padding
from cryptography.x509.oid import NameOID, ExtensionOID
import datetime

class TLSVersion(Enum):
    TLS_1_2 = ssl.TLSVersion.TLSv1_2
    TLS_1_3 = ssl.TLSVersion.TLSv1_3

@dataclass
class TLSConfig:
    min_version: TLSVersion = TLS_1_2
    max_version: TLSVersion = TLS_1_3
    cipher_suites: List[str] = None
    verify_mode: ssl.VerifyMode = ssl.CERT_REQUIRED
    check_hostname: bool = True
    ca_bundle_path: Optional[str] = None
    cert_file: Optional[str] = None
    key_file: Optional[str] = None
    dh_params_file: Optional[str] = None

class TLSSecurityAnalyzer:
    def __init__(self):
        self.logger = logging.getLogger('TLSAnalyzer')
        
        # Modern secure cipher suites
        self.recommended_ciphers = [
            'TLS_AES_256_GCM_SHA384',          # TLS 1.3
            'TLS_CHACHA20_POLY1305_SHA256',    # TLS 1.3
            'TLS_AES_128_GCM_SHA256',          # TLS 1.3
            'ECDHE-RSA-AES256-GCM-SHA384',     # TLS 1.2
            'ECDHE-RSA-CHACHA20-POLY1305',     # TLS 1.2
            'ECDHE-RSA-AES128-GCM-SHA256',     # TLS 1.2
        ]
        
        # Weak/deprecated ciphers to avoid
        self.weak_ciphers = [
            'RC4', 'MD5', 'SHA1', 'DES', '3DES',
            'NULL', 'aNULL', 'eNULL', 'EXPORT'
        ]
    
    def create_secure_context(self, config: TLSConfig, server_side: bool = False) -> ssl.SSLContext:
        """Создание безопасного SSL context"""
        
        # Выбираем протокол
        if server_side:
            context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
        else:
            context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)
        
        # Устанавливаем версии TLS
        context.minimum_version = config.min_version.value
        context.maximum_version = config.max_version.value
        
        # Настройки безопасности
        context.verify_mode = config.verify_mode
        context.check_hostname = config.check_hostname
        
        # Загружаем сертификаты
        if config.cert_file and config.key_file:
            context.load_cert_chain(config.cert_file, config.key_file)
        
        # Загружаем CA bundle
        if config.ca_bundle_path:
            context.load_verify_locations(config.ca_bundle_path)
        else:
            context.load_default_certs()
        
        # Настройка cipher suites
        if config.cipher_suites:
            cipher_string = ':'.join(config.cipher_suites)
        else:
            # Безопасная конфигурация по умолчанию
            cipher_string = ':'.join([
                'ECDHE+AESGCM',
                'ECDHE+CHACHA20',
                'DHE+AESGCM',
                'DHE+CHACHA20',
                '!aNULL',
                '!MD5',
                '!DSS',
                '!SHA1',
                '!RC4'
            ])
        
        try:
            context.set_ciphers(cipher_string)
        except ssl.SSLError as e:
            self.logger.warning(f"Failed to set cipher suites: {e}")
        
        # Дополнительные настройки безопасности
        context.options |= ssl.OP_NO_SSLv2
        context.options |= ssl.OP_NO_SSLv3
        context.options |= ssl.OP_NO_COMPRESSION
        context.options |= ssl.OP_CIPHER_SERVER_PREFERENCE
        context.options |= ssl.OP_SINGLE_DH_USE
        context.options |= ssl.OP_SINGLE_ECDH_USE
        
        # Настройка ECDH curves
        try:
            context.set_ecdh_curve('prime256v1')
        except AttributeError:
            pass  # Не все версии Python поддерживают
        
        return context
    
    async def analyze_tls_connection(self, hostname: str, port: int = 443) -> Dict:
        """Анализ TLS соединения"""
        
        analysis_result = {
            'hostname': hostname,
            'port': port,
            'timestamp': time.time(),
            'connection_successful': False,
            'certificate_info': {},
            'protocol_info': {},
            'cipher_info': {},
            'security_score': 0,
            'vulnerabilities': [],
            'recommendations': []
        }
        
        try:
            # Создаем SSL context для анализа
            context = ssl.create_default_context()
            context.check_hostname = False
            context.verify_mode = ssl.CERT_NONE  # Для анализа принимаем любые сертификаты
            
            # Подключаемся
            with socket.create_connection((hostname, port), timeout=10) as sock:
                with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                    analysis_result['connection_successful'] = True
                    
                    # Информация о протоколе
                    analysis_result['protocol_info'] = {
                        'version': ssock.version(),
                        'cipher': ssock.cipher(),
                        'compression': ssock.compression(),
                        'selected_alpn_protocol': getattr(ssock, 'selected_alpn_protocol', lambda: None)()
                    }
                    
                    # Информация о сертификате
                    cert_der = ssock.getpeercert_raw()
                    cert_dict = ssock.getpeercert()
                    
                    if cert_der:
                        cert = x509.load_der_x509_certificate(cert_der)
                        analysis_result['certificate_info'] = self._analyze_certificate(cert, cert_dict)
                    
                    # Анализ cipher suite
                    cipher_info = ssock.cipher()
                    if cipher_info:
                        analysis_result['cipher_info'] = self._analyze_cipher(cipher_info)
                    
                    # Вычисляем security score
                    analysis_result['security_score'] = self._calculate_security_score(analysis_result)
                    
                    # Генерируем рекомендации
                    analysis_result['recommendations'] = self._generate_recommendations(analysis_result)
        
        except Exception as e:
            analysis_result['error'] = str(e)
            self.logger.error(f"TLS analysis failed for {hostname}:{port}: {e}")
        
        return analysis_result
    
    def _analyze_certificate(self, cert: x509.Certificate, cert_dict: Dict) -> Dict:
        """Анализ сертификата"""
        
        current_time = datetime.datetime.now(datetime.timezone.utc)
        
        cert_info = {
            'subject': dict(x[0] for x in cert_dict.get('subject', [])),
            'issuer': dict(x[0] for x in cert_dict.get('issuer', [])),
            'version': cert.version.name,
            'serial_number': str(cert.serial_number),
            'not_before': cert.not_valid_before.isoformat(),
            'not_after': cert.not_valid_after.isoformat(),
            'signature_algorithm': cert.signature_algorithm_oid._name,
            'public_key_algorithm': cert.public_key().__class__.__name__,
            'san_domains': [],
            'key_size': None,
            'is_expired': current_time > cert.not_valid_after,
            'expires_soon': (cert.not_valid_after - current_time).days < 30,
            'is_self_signed': cert.issuer == cert.subject
        }
        
        # Анализ Subject Alternative Names
        try:
            san_ext = cert.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)
            cert_info['san_domains'] = [name.value for name in san_ext.value]
        except x509.ExtensionNotFound:
            pass
        
        # Размер ключа
        public_key = cert.public_key()
        if hasattr(public_key, 'key_size'):
            cert_info['key_size'] = public_key.key_size
        
        # Проверка на слабые алгоритмы
        weak_algorithms = ['md5', 'sha1']
        if any(weak in cert_info['signature_algorithm'].lower() for weak in weak_algorithms):
            cert_info['weak_signature'] = True
        
        return cert_info
    
    def _analyze_cipher(self, cipher_info: Tuple) -> Dict:
        """Анализ cipher suite"""
        
        cipher_name, tls_version, secret_bits = cipher_info
        
        cipher_analysis = {
            'name': cipher_name,
            'tls_version': tls_version,
            'secret_bits': secret_bits,
            'is_secure': True,
            'warnings': []
        }
        
        # Проверяем на слабые cipher'ы
        cipher_lower = cipher_name.lower()
        
        for weak_cipher in self.weak_ciphers:
            if weak_cipher.lower() in cipher_lower:
                cipher_analysis['is_secure'] = False
                cipher_analysis['warnings'].append(f"Uses weak cipher: {weak_cipher}")
        
        # Проверяем силу шифрования
        if secret_bits < 128:
            cipher_analysis['is_secure'] = False
            cipher_analysis['warnings'].append(f"Weak encryption strength: {secret_bits} bits")
        
        # Проверяем наличие Perfect Forward Secrecy
        pfs_indicators = ['ecdhe', 'dhe']
        cipher_analysis['perfect_forward_secrecy'] = any(
            indicator in cipher_lower for indicator in pfs_indicators
        )
        
        if not cipher_analysis['perfect_forward_secrecy']:
            cipher_analysis['warnings'].append("No Perfect Forward Secrecy")
        
        return cipher_analysis
    
    def _calculate_security_score(self, analysis: Dict) -> int:
        """Вычисление security score (0-100)"""
        
        score = 100
        
        # Проверки протокола
        protocol_version = analysis['protocol_info'].get('version', '')
        if 'TLSv1.3' in protocol_version:
            score += 0  # Максимальный балл
        elif 'TLSv1.2' in protocol_version:
            score -= 5
        elif 'TLSv1.' in protocol_version:
            score -= 30  # TLS 1.0/1.1 устарели
        else:
            score -= 50  # SSL или неизвестный протокол
        
        # Проверки сертификата
        cert_info = analysis.get('certificate_info', {})
        
        if cert_info.get('is_expired'):
            score -= 50
        elif cert_info.get('expires_soon'):
            score -= 10
        
        if cert_info.get('is_self_signed'):
            score -= 20
        
        if cert_info.get('weak_signature'):
            score -= 30
        
        key_size = cert_info.get('key_size', 0)
        if key_size < 2048:
            score -= 40
        elif key_size < 4096:
            score -= 5
        
        # Проверки cipher
        cipher_info = analysis.get('cipher_info', {})
        
        if not cipher_info.get('is_secure'):
            score -= 40
        
        if not cipher_info.get('perfect_forward_secrecy'):
            score -= 15
        
        secret_bits = cipher_info.get('secret_bits', 0)
        if secret_bits < 128:
            score -= 30
        elif secret_bits < 256:
            score -= 5
        
        return max(0, min(100, score))
    
    def _generate_recommendations(self, analysis: Dict) -> List[str]:
        """Генерация рекомендаций по улучшению безопасности"""
        
        recommendations = []
        
        # Протокол
        protocol_version = analysis['protocol_info'].get('version', '')
        if 'TLSv1.3' not in protocol_version:
            recommendations.append("Upgrade to TLS 1.3 for better security and performance")
        
        if 'TLSv1.' in protocol_version and 'TLSv1.2' not in protocol_version:
            recommendations.append("Disable TLS 1.0 and 1.1 - they are deprecated")
        
        # Сертификат
        cert_info = analysis.get('certificate_info', {})
        
        if cert_info.get('is_expired'):
            recommendations.append("Certificate has expired - renew immediately")
        elif cert_info.get('expires_soon'):
            recommendations.append("Certificate expires soon - plan renewal")
        
        if cert_info.get('is_self_signed'):
            recommendations.append("Use a certificate from a trusted CA instead of self-signed")
        
        if cert_info.get('weak_signature'):
            recommendations.append("Certificate uses weak signature algorithm - get new certificate")
        
        key_size = cert_info.get('key_size', 0)
        if key_size < 2048:
            recommendations.append("Use at least 2048-bit RSA keys or 256-bit ECDSA keys")
        
        # Cipher
        cipher_info = analysis.get('cipher_info', {})
        
        if not cipher_info.get('is_secure'):
            recommendations.append("Disable weak cipher suites")
        
        if not cipher_info.get('perfect_forward_secrecy'):
            recommendations.append("Enable Perfect Forward Secrecy with ECDHE or DHE")
        
        # Security score
        if analysis['security_score'] < 70:
            recommendations.append("Overall security configuration needs significant improvement")
        elif analysis['security_score'] < 90:
            recommendations.append("Consider implementing additional security measures")
        
        return recommendations

# Certificate Management System
class CertificateManager:
    def __init__(self):
        self.logger = logging.getLogger('CertManager')
        self.certificates: Dict[str, Dict] = {}
    
    def generate_private_key(self, key_size: int = 2048) -> rsa.RSAPrivateKey:
        """Генерация приватного ключа"""
        
        private_key = rsa.generate_private_key(
            public_exponent=65537,
            key_size=key_size,
        )
        
        return private_key
    
    def create_self_signed_cert(self, 
                              common_name: str,
                              san_domains: List[str] = None,
                              days_valid: int = 365,
                              key_size: int = 2048) -> Tuple[bytes, bytes]:
        """Создание self-signed сертификата"""
        
        # Генерируем приватный ключ
        private_key = self.generate_private_key(key_size)
        
        # Создаем subject и issuer
        subject = issuer = x509.Name([
            x509.NameAttribute(NameOID.COUNTRY_NAME, "US"),
            x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, "CA"),
            x509.NameAttribute(NameOID.LOCALITY_NAME, "San Francisco"),
            x509.NameAttribute(NameOID.ORGANIZATION_NAME, "My Company"),
            x509.NameAttribute(NameOID.COMMON_NAME, common_name),
        ])
        
        # Строим сертификат
        builder = x509.CertificateBuilder()
        builder = builder.subject_name(subject)
        builder = builder.issuer_name(issuer)
        builder = builder.public_key(private_key.public_key())
        builder = builder.serial_number(x509.random_serial_number())
        builder = builder.not_valid_before(datetime.datetime.utcnow())
        builder = builder.not_valid_after(
            datetime.datetime.utcnow() + datetime.timedelta(days=days_valid)
        )
        
        # Добавляем расширения
        builder = builder.add_extension(
            x509.SubjectKeyIdentifier.from_public_key(private_key.public_key()),
            critical=False,
        )
        
        builder = builder.add_extension(
            x509.AuthorityKeyIdentifier.from_issuer_public_key(private_key.public_key()),
            critical=False,
        )
        
        builder = builder.add_extension(
            x509.BasicConstraints(ca=False, path_length=None),
            critical=True,
        )
        
        builder = builder.add_extension(
            x509.KeyUsage(
                digital_signature=True,
                content_commitment=False,
                key_encipherment=True,
                data_encipherment=False,
                key_agreement=False,
                key_cert_sign=False,
                crl_sign=False,
                encipher_only=False,
                decipher_only=False,
            ),
            critical=True,
        )
        
        builder = builder.add_extension(
            x509.ExtendedKeyUsage([
                x509.oid.ExtendedKeyUsageOID.SERVER_AUTH,
            ]),
            critical=True,
        )
        
        # Subject Alternative Names
        san_list = [x509.DNSName(common_name)]
        if san_domains:
            san_list.extend([x509.DNSName(domain) for domain in san_domains])
        
        builder = builder.add_extension(
            x509.SubjectAlternativeName(san_list),
            critical=False,
        )
        
        # Подписываем сертификат
        certificate = builder.sign(private_key, hashes.SHA256())
        
        # Сериализуем в PEM формат
        cert_pem = certificate.public_bytes(serialization.Encoding.PEM)
        key_pem = private_key.private_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PrivateFormat.PKCS8,
            encryption_algorithm=serialization.NoEncryption()
        )
        
        return cert_pem, key_pem
    
    def load_certificate_from_file(self, cert_path: str) -> x509.Certificate:
        """Загрузка сертификата из файла"""
        
        with open(cert_path, 'rb') as f:
            cert_data = f.read()
        
        try:
            # Пробуем PEM формат
            certificate = x509.load_pem_x509_certificate(cert_data)
        except ValueError:
            try:
                # Пробуем DER формат
                certificate = x509.load_der_x509_certificate(cert_data)
            except ValueError:
                raise ValueError("Unable to parse certificate file")
        
        return certificate
    
    def check_certificate_expiry(self, certificate: x509.Certificate) -> Dict:
        """Проверка срока действия сертификата"""
        
        current_time = datetime.datetime.now(datetime.timezone.utc)
        not_after = certificate.not_valid_after.replace(tzinfo=datetime.timezone.utc)
        not_before = certificate.not_valid_before.replace(tzinfo=datetime.timezone.utc)
        
        time_until_expiry = not_after - current_time
        days_until_expiry = time_until_expiry.days
        
        return {
            'is_valid': not_before <= current_time <= not_after,
            'is_expired': current_time > not_after,
            'not_yet_valid': current_time < not_before,
            'days_until_expiry': days_until_expiry,
            'expires_soon': days_until_expiry <= 30,
            'not_before': not_before.isoformat(),
            'not_after': not_after.isoformat()
        }

# Демонстрация TLS безопасности
async def demonstrate_tls_security():
    """Демонстрация анализа TLS безопасности"""
    
    print("🔐 TLS Security Analysis Demonstration")
    print("=" * 60)
    
    analyzer = TLSSecurityAnalyzer()
    cert_manager = CertificateManager()
    
    # Тестируем популярные сайты
    test_sites = [
        ('google.com', 443),
        ('github.com', 443),
        ('stackoverflow.com', 443),
        ('badssl.com', 443),  # Сайт для тестирования TLS
        ('expired.badssl.com', 443)  # Сайт с истекшим сертификатом
    ]
    
    print("🔍 Analyzing TLS configurations of popular sites:")
    
    for hostname, port in test_sites:
        print(f"\n📊 Analyzing {hostname}:{port}")
        
        try:
            analysis = await analyzer.analyze_tls_connection(hostname, port)
            
            if analysis['connection_successful']:
                # Основная информация
                protocol_info = analysis['protocol_info']
                cert_info = analysis['certificate_info']
                cipher_info = analysis['cipher_info']
                
                print(f"  TLS Version: {protocol_info.get('version', 'Unknown')}")
                print(f"  Cipher Suite: {cipher_info.get('name', 'Unknown')}")
                print(f"  Key Size: {cert_info.get('key_size', 'Unknown')} bits")
                print(f"  Perfect Forward Secrecy: {'Yes' if cipher_info.get('perfect_forward_secrecy') else 'No'}")
                
                # Security score
                score = analysis['security_score']
                score_grade = 'A+' if score >= 95 else 'A' if score >= 90 else 'B' if score >= 80 else 'C' if score >= 70 else 'F'
                print(f"  Security Score: {score}/100 (Grade: {score_grade})")
                
                # Сертификат
                if cert_info.get('is_expired'):
                    print(f"  ⚠️ Certificate EXPIRED")
                elif cert_info.get('expires_soon'):
                    print(f"  ⚠️ Certificate expires in {cert_info.get('days_until_expiry', 'Unknown')} days")
                else:
                    print(f"  ✅ Certificate valid")
                
                # Рекомендации
                if analysis['recommendations']:
                    print(f"  💡 Recommendations:")
                    for rec in analysis['recommendations'][:3]:  # Показываем первые 3
                        print(f"    • {rec}")
            
            else:
                print(f"  ❌ Connection failed: {analysis.get('error', 'Unknown error')}")
        
        except Exception as e:
            print(f"  ❌ Analysis failed: {e}")
    
    # Демонстрация создания сертификата
    print(f"\n🔧 Certificate Management Demo:")
    
    print("  Creating self-signed certificate...")
    
    try:
        cert_pem, key_pem = cert_manager.create_self_signed_cert(
            common_name="test.example.com",
            san_domains=["api.example.com", "www.example.com"],
            days_valid=365,
            key_size=2048
        )
        
        print(f"  ✅ Certificate created successfully")
        print(f"  Certificate size: {len(cert_pem)} bytes")
        print(f"  Private key size: {len(key_pem)} bytes")
        
        # Анализируем созданный сертификат
        certificate = x509.load_pem_x509_certificate(cert_pem)
        expiry_info = cert_manager.check_certificate_expiry(certificate)
        
        print(f"  Valid from: {expiry_info['not_before']}")
        print(f"  Valid until: {expiry_info['not_after']}")
        print(f"  Days until expiry: {expiry_info['days_until_expiry']}")
        
    except Exception as e:
        print(f"  ❌ Certificate creation failed: {e}")
    
    # Настройка безопасного TLS context
    print(f"\n⚙️ Secure TLS Context Configuration:")
    
    try:
        config = TLSConfig(
            min_version=TLSVersion.TLS_1_2,
            max_version=TLSVersion.TLS_1_3,
            verify_mode=ssl.CERT_REQUIRED,
            check_hostname=True
        )
        
        context = analyzer.create_secure_context(config, server_side=False)
        
        print(f"  ✅ Secure TLS context created")
        print(f"  Minimum TLS version: {context.minimum_version.name}")
        print(f"  Maximum TLS version: {context.maximum_version.name}")
        print(f"  Verify mode: {context.verify_mode.name}")
        print(f"  Check hostname: {context.check_hostname}")
        
        # Тестируем context
        print(f"  Testing secure connection to google.com...")
        
        async with aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(ssl=context)
        ) as session:
            async with session.get('https://google.com') as response:
                print(f"  ✅ Secure connection successful (Status: {response.status})")
    
    except Exception as e:
        print(f"  ❌ TLS context configuration failed: {e}")
    
    print(f"\n✅ TLS Security demonstration completed")

# Запуск демонстрации
if __name__ == "__main__":
    asyncio.run(demonstrate_tls_security())
```

### 📝 Практическое задание Неделя 12

1. Проведите анализ TLS конфигурации всех ваших API endpoints
2. Реализуйте автоматическое обновление TLS сертификатов
3. Настройте мониторинг срока действия сертификатов с алертами
4. Создайте систему для управления сертификатами в микросервисной архитектуре
5. Добавьте TLS security scanning в CI/CD pipeline

### ✅ Контрольные вопросы

- [ ] Понимаете различия между TLS 1.2 и TLS 1.3?
- [ ] Можете настроить secure cipher suites?
- [ ] Знаете как работает Perfect Forward Secrecy?
- [ ] Умеете анализировать безопасность TLS соединений?
- [ ] Понимаете принципы certificate management?

---

# Модуль 7: Балансировка нагрузки и масштабирование {#module-7}
*Недели 13-14 | Время изучения: 16-20 часов*

## Неделя 13: Load Balancing

### 🧠 Концепция: Типы и алгоритмы балансировки нагрузки

Load Balancing - критически важный компонент для масштабирования:

```
Load Balancing Layers:
┌─────────────────────────────────────────────────────────────┐
│ Layer 7 (Application) Load Balancing                       │  
│ ├─ HTTP/HTTPS aware                                        │
│ ├─ Content-based routing                                   │
│ ├─ SSL termination                                         │
│ ├─ Advanced health checks                                  │
│ └─ Examples: HAProxy, Nginx, AWS ALB                       │
├─────────────────────────────────────────────────────────────┤
│ Layer 4 (Transport) Load Balancing                         │
│ ├─ TCP/UDP aware                                           │
│ ├─ IP + Port based routing                                 │
│ ├─ High performance                                        │
│ ├─ Protocol agnostic                                       │
│ └─ Examples: AWS NLB, F5, hardware LB                      │
├─────────────────────────────────────────────────────────────┤
│ Layer 3 (Network) Load Balancing                           │
│ ├─ IP-based routing                                        │
│ ├─ ECMP (Equal Cost Multi-Path)                           │
│ ├─ Router-level distribution                               │
│ └─ Examples: BGP anycast, OSPF                             │
└─────────────────────────────────────────────────────────────┘
```

### ⚖️ Advanced Load Balancer Implementation

**Comprehensive Load Balancer System:**

```python
import asyncio
import time
import random
import statistics
from typing import Dict, List, Optional, Callable, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import aiohttp
import logging
from collections import defaultdict, deque
import hashlib
import json

class LoadBalanceAlgorithm(Enum):
    ROUND_ROBIN = "round_robin"
    WEIGHTED_ROUND_ROBIN = "weighted_round_robin"
    LEAST_CONNECTIONS = "least_connections"
    WEIGHTED_LEAST_CONNECTIONS = "weighted_least_connections"
    LEAST_RESPONSE_TIME = "least_response_time"
    IP_HASH = "ip_hash"
    CONSISTENT_HASH = "consistent_hash"
    RANDOM = "random"
    WEIGHTED_RANDOM = "weighted_random"

class BackendState(Enum):
    HEALTHY = "healthy"
    UNHEALTHY = "unhealthy"
    DRAINING = "draining"
    MAINTENANCE = "maintenance"

@dataclass
class Backend:
    id: str
    host: str
    port: int
    weight: int = 1
    max_connections: int = 1000
    current_connections: int = 0
    state: BackendState = BackendState.HEALTHY
    health_score: float = 1.0
    last_health_check: float = 0
    response_times: deque = field(default_factory=lambda: deque(maxlen=100))
    total_requests: int = 0
    failed_requests: int = 0
    metadata: Dict = field(default_factory=dict)
    
    @property
    def avg_response_time(self) -> float:
        if not self.response_times:
            return 0.0
        return statistics.mean(self.response_times)
    
    @property
    def failure_rate(self) -> float:
        if self.total_requests == 0:
            return 0.0
        return self.failed_requests / self.total_requests
    
    @property
    def connection_utilization(self) -> float:
        return self.current_connections / self.max_connections
    
    @property
    def effective_weight(self) -> float:
        """Эффективный вес с учетом здоровья и нагрузки"""
        base_weight = self.weight * self.health_score
        
        # Снижаем вес при высокой утилизации соединений
        utilization_penalty = 1.0 - (self.connection_utilization * 0.5)
        
        # Снижаем вес при высоком failure rate
        failure_penalty = 1.0 - (self.failure_rate * 0.8)
        
        return base_weight * utilization_penalty * failure_penalty

@dataclass
class HealthCheckConfig:
    interval: float = 30.0
    timeout: float = 5.0
    path: str = "/health"
    expected_status: int = 200
    max_failures: int = 3
    max_successes_to_recover: int = 2

class AdvancedLoadBalancer:
    def __init__(self, algorithm: LoadBalanceAlgorithm = LoadBalanceAlgorithm.ROUND_ROBIN):
        self.algorithm = algorithm
        self.backends: Dict[str, Backend] = {}
        self.backend_order: List[str] = []  # Для round robin
        self.current_index = 0
        
        # Health checking
        self.health_config = HealthCheckConfig()
        self.health_check_tasks: Dict[str, asyncio.Task] = {}
        
        # Statistics
        self.stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'requests_by_backend': defaultdict(int),
            'requests_by_algorithm': defaultdict(int),
            'avg_response_time': 0.0,
            'connection_errors': 0
        }
        
        # Consistent hashing (для CONSISTENT_HASH алгоритма)
        self.hash_ring: Dict[int, str] = {}
        self.virtual_nodes = 150  # Количество виртуальных узлов на backend
        
        # Circuit breaker для backends
        self.circuit_breakers: Dict[str, Dict] = {}
        
        self.logger = logging.getLogger('LoadBalancer')
    
    def add_backend(self, backend: Backend):
        """Добавление backend сервера"""
        
        self.backends[backend.id] = backend
        self.backend_order.append(backend.id)
        
        # Инициализируем circuit breaker
        self.circuit_breakers[backend.id] = {
            'state': 'closed',  # closed, open, half_open
            'failure_count': 0,
            'last_failure': 0,
            'recovery_timeout': 60.0
        }
        
        # Обновляем consistent hash ring
        if self.algorithm == LoadBalanceAlgorithm.CONSISTENT_HASH:
            self._update_hash_ring()
        
        # Запускаем health check
        self._start_health_check(backend.id)
        
        self.logger.info(f"Added backend {backend.id} ({backend.host}:{backend.port})")
    
    def remove_backend(self, backend_id: str):
        """Удаление backend сервера"""
        
        if backend_id in self.backends:
            # Останавливаем health check
            if backend_id in self.health_check_tasks:
                self.health_check_tasks[backend_id].cancel()
                del self.health_check_tasks[backend_id]
            
            # Удаляем backend
            del self.backends[backend_id]
            
            if backend_id in self.backend_order:
                self.backend_order.remove(backend_id)
            
            # Обновляем hash ring
            if self.algorithm == LoadBalanceAlgorithm.CONSISTENT_HASH:
                self._update_hash_ring()
            
            self.logger.info(f"Removed backend {backend_id}")
    
    def set_backend_state(self, backend_id: str, state: BackendState):
        """Установка состояния backend"""
        
        if backend_id in self.backends:
            old_state = self.backends[backend_id].state
            self.backends[backend_id].state = state
            
            self.logger.info(f"Backend {backend_id} state changed: {old_state.value} -> {state.value}")
    
    async def select_backend(self, client_ip: str = None, request_data: Dict = None) -> Optional[Backend]:
        """Выбор backend сервера согласно алгоритму"""
        
        # Фильтруем здоровые backends
        healthy_backends = [
            backend for backend in self.backends.values()
            if backend.state == BackendState.HEALTHY and 
               self._is_circuit_breaker_closed(backend.id)
        ]
        
        if not healthy_backends:
            self.logger.error("No healthy backends available")
            return None
        
        # Применяем алгоритм балансировки
        selected_backend = None
        
        if self.algorithm == LoadBalanceAlgorithm.ROUND_ROBIN:
            selected_backend = self._round_robin_select(healthy_backends)
        
        elif self.algorithm == LoadBalanceAlgorithm.WEIGHTED_ROUND_ROBIN:
            selected_backend = self._weighted_round_robin_select(healthy_backends)
        
        elif self.algorithm == LoadBalanceAlgorithm.LEAST_CONNECTIONS:
            selected_backend = self._least_connections_select(healthy_backends)
        
        elif self.algorithm == LoadBalanceAlgorithm.WEIGHTED_LEAST_CONNECTIONS:
            selected_backend = self._weighted_least_connections_select(healthy_backends)
        
        elif self.algorithm == LoadBalanceAlgorithm.LEAST_RESPONSE_TIME:
            selected_backend = self._least_response_time_select(healthy_backends)
        
        elif self.algorithm == LoadBalanceAlgorithm.IP_HASH:
            selected_backend = self._ip_hash_select(healthy_backends, client_ip)
        
        elif self.algorithm == LoadBalanceAlgorithm.CONSISTENT_HASH:
            selected_backend = self._consistent_hash_select(client_ip)
        
        elif self.algorithm == LoadBalanceAlgorithm.RANDOM:
            selected_backend = self._random_select(healthy_backends)
        
        elif self.algorithm == LoadBalanceAlgorithm.WEIGHTED_RANDOM:
            selected_backend = self._weighted_random_select(healthy_backends)
        
        if selected_backend:
            self.stats['requests_by_algorithm'][self.algorithm.value] += 1
            self.stats['requests_by_backend'][selected_backend.id] += 1
        
        return selected_backend
    
    def _round_robin_select(self, backends: List[Backend]) -> Backend:
        """Round Robin алгоритм"""
        
        if not backends:
            return None
        
        # Находим следующий backend в порядке
        backend_ids = [b.id for b in backends]
        
        # Ищем следующий доступный backend
        attempts = 0
        while attempts < len(backend_ids):
            current_id = self.backend_order[self.current_index % len(self.backend_order)]
            self.current_index += 1
            attempts += 1
            
            if current_id in backend_ids:
                return next(b for b in backends if b.id == current_id)
        
        return backends[0]  # Fallback
    
    def _weighted_round_robin_select(self, backends: List[Backend]) -> Backend:
        """Weighted Round Robin алгоритм"""
        
        # Создаем взвешенный список
        weighted_backends = []
        for backend in backends:
            effective_weight = max(1, int(backend.effective_weight * 10))
            weighted_backends.extend([backend] * effective_weight)
        
        if not weighted_backends:
            return backends[0]
        
        selected = weighted_backends[self.current_index % len(weighted_backends)]
        self.current_index += 1
        
        return selected
    
    def        async def api_call():
            url = 'https://jsonplaceholder.typicode.com/posts'
            async with self.session.post(url, json=order_data) as response:
                if response.status >= 500:
                    raise aiohttp.ClientError(f"Server error: {response.status}")
                return await response.json()
        
        cb = self.circuit_breakers['order_service']
        return await cb(api_call)
    
    async def process_payment(self, payment_data: Dict) -> Dict:
        """Обработка платежа через Circuit Breaker"""
        
        async def api_call():
            # Симулируем ненадежный payment сервис
            import random
            if random.random() < 0.3:  # 30% вероятность ошибки
                raise aiohttp.ClientError("Payment service unavailable")
            
            url = 'https://jsonplaceholder.typicode.com/posts'
            async with self.session.post(url, json=payment_data) as response:
                return await response.json()
        
        cb = self.circuit_breakers['payment_service']
        return await cb(api_call)
    
    def get_all_circuit_breaker_stats(self) -> Dict:
        """Получение статистики всех Circuit Breakers"""
        return {
            name: cb.get_metrics() 
            for name, cb in self.circuit_breakers.items()
        }

# Демонстрация Circuit Breaker в действии
async def demonstrate_circuit_breaker():
    """Демонстрация работы Circuit Breaker"""
    
    print("🔌 Circuit Breaker Pattern Demonstration")
    print("=" * 60)
    
    async with ResilientAPIClient() as client:
        
        print("📊 Initial Circuit Breaker States:")
        for name, stats in client.get_all_circuit_breaker_stats().items():
            print(f"  {name}: {stats['state'].upper()}")
        
        # Тест 1: Нормальная работа
        print(f"\n✅ Test 1: Normal Operations")
        
        try:
            user = await client.get_user(1)
            print(f"  User service: SUCCESS (got user {user.get('id')})")
            
            order = await client.create_order({"title": "Test Order", "body": "Order details"})
            print(f"  Order service: SUCCESS (created order {order.get('id')})")
            
        except Exception as e:
            print(f"  ERROR: {e}")
        
        # Тест 2: Провоцируем ошибки в payment service
        print(f"\n⚠️ Test 2: Triggering Payment Service Failures")
        
        for i in range(8):  # Делаем много запросов чтобы сработал circuit breaker
            try:
                payment = await client.process_payment({"amount": "100.00", "currency": "USD"})
                print(f"  Payment attempt {i+1}: SUCCESS")
            
            except CircuitBreakerOpenException as e:
                print(f"  Payment attempt {i+1}: CIRCUIT OPEN - {e}")
                break
            
            except Exception as e:
                print(f"  Payment attempt {i+1}: FAILED - {e}")
        
        # Показываем состояния Circuit Breakers после ошибок
        print(f"\n📊 Circuit Breaker States After Failures:")
        all_stats = client.get_all_circuit_breaker_stats()
        
        for name, stats in all_stats.items():
            state = stats['state'].upper()
            failure_rate = stats.get('failure_rate_percent', 0)
            total_calls = stats['lifetime_metrics']['total_calls']
            
            print(f"  {name}: {state} (failure rate: {failure_rate:.1f}%, calls: {total_calls})")
            
            if stats['state'] == 'open':
                last_failure = stats.get('last_failure_ago_seconds')
                if last_failure is not None:
                    print(f"    Last failure: {last_failure:.1f}s ago")
        
        # Тест 3: Ждем recovery и тестируем half-open состояние
        payment_cb = client.circuit_breakers['payment_service']
        
        if payment_cb.state == CircuitState.OPEN:
            print(f"\n⏰ Test 3: Waiting for Circuit Breaker Recovery")
            print(f"  Payment circuit is OPEN, waiting for recovery timeout...")
            
            # Форсируем переход в half-open для демонстрации
            payment_cb._last_failure_time = time.time() - payment_cb.config.recovery_timeout - 1
            
            print(f"  Attempting request after recovery timeout...")
            
            try:
                # Этот запрос должен перевести в HALF_OPEN
                payment = await client.process_payment({"amount": "50.00", "currency": "USD"})
                print(f"  Recovery attempt: SUCCESS")
                
                # Делаем еще несколько успешных запросов для закрытия circuit
                for i in range(3):
                    try:
                        payment = await client.process_payment({"amount": "25.00", "currency": "USD"})
                        print(f"  Additional success {i+1}: OK")
                    except:
                        print(f"  Additional attempt {i+1}: FAILED")
                        break
                
            except CircuitBreakerOpenException as e:
                print(f"  Recovery attempt: STILL BLOCKED - {e}")
            except Exception as e:
                print(f"  Recovery attempt: FAILED - {e}")
        
        # Финальные состояния
        print(f"\n📈 Final Circuit Breaker Statistics:")
        final_stats = client.get_all_circuit_breaker_stats()
        
        for name, stats in final_stats.items():
            print(f"\n  🔌 {name.upper()}:")
            print(f"    State: {stats['state'].upper()}")
            print(f"    Total calls: {stats['lifetime_metrics']['total_calls']}")
            print(f"    Success rate: {100 - stats.get('failure_rate_percent', 0):.1f}%")
            print(f"    Circuit opened: {stats['lifetime_metrics']['circuit_opened_count']} times")
            print(f"    Circuit closed: {stats['lifetime_metrics']['circuit_closed_count']} times")
            print(f"    Calls rejected: {stats['lifetime_metrics']['calls_rejected']}")
            
            if 'response_time_stats' in stats:
                rt_stats = stats['response_time_stats']
                print(f"    Avg response time: {rt_stats['avg_ms']:.1f}ms")
                print(f"    P95 response time: {rt_stats['p95_ms']:.1f}ms")

# Запуск демонстрации
if __name__ == "__main__":
    asyncio.run(demonstrate_circuit_breaker())
```

### 🗜️ Data Compression и Transfer Optimization

**Intelligent Compression Strategy:**

```python
import gzip
import brotli
import zlib
import json
import time
import asyncio
from typing import Dict, Any, Optional, Union
from dataclasses import dataclass
from enum import Enum

class CompressionType(Enum):
    NONE = "none"
    GZIP = "gzip"
    DEFLATE = "deflate" 
    BROTLI = "brotli"

@dataclass
class CompressionResult:
    original_size: int
    compressed_size: int
    compression_ratio: float
    compression_time_ms: float
    compression_type: CompressionType
    compressed_data: bytes

class IntelligentCompressor:
    def __init__(self):
        self.compression_stats = {
            CompressionType.GZIP: {'total_bytes_in': 0, 'total_bytes_out': 0, 'total_time_ms': 0, 'uses': 0},
            CompressionType.DEFLATE: {'total_bytes_in': 0, 'total_bytes_out': 0, 'total_time_ms': 0, 'uses': 0},
            CompressionType.BROTLI: {'total_bytes_in': 0, 'total_bytes_out': 0, 'total_time_ms': 0, 'uses': 0}
        }
    
    def _compress_gzip(self, data: bytes, level: int = 6) -> CompressionResult:
        """GZIP сжатие"""
        start_time = time.time()
        compressed = gzip.compress(data, compresslevel=level)
        compression_time = (time.time() - start_time) * 1000
        
        return CompressionResult(
            original_size=len(data),
            compressed_size=len(compressed),
            compression_ratio=len(compressed) / len(data),
            compression_time_ms=compression_time,
            compression_type=CompressionType.GZIP,
            compressed_data=compressed
        )
    
    def _compress_deflate(self, data: bytes, level: int = 6) -> CompressionResult:
        """DEFLATE сжатие"""
        start_time = time.time()
        compressed = zlib.compress(data, level)
        compression_time = (time.time() - start_time) * 1000
        
        return CompressionResult(
            original_size=len(data),
            compressed_size=len(compressed),
            compression_ratio=len(compressed) / len(data),
            compression_time_ms=compression_time,
            compression_type=CompressionType.DEFLATE,
            compressed_data=compressed
        )
    
    def _compress_brotli(self, data: bytes, quality: int = 6) -> CompressionResult:
        """Brotli сжатие"""
        start_time = time.time()
        compressed = brotli.compress(data, quality=quality)
        compression_time = (time.time() - start_time) * 1000
        
        return CompressionResult(
            original_size=len(data),
            compressed_size=len(compressed),
            compression_ratio=len(compressed) / len(data),
            compression_time_ms=compression_time,
            compression_type=CompressionType.BROTLI,
            compressed_data=compressed
        )
    
    def compress_adaptive(self, data: Union[str, bytes, Dict], 
                         min_size_threshold: int = 1024,
                         max_compression_time_ms: float = 100.0) -> CompressionResult:
        """Адаптивное сжатие с выбором лучшего алгоритма"""
        
        # Конвертируем в bytes если нужно
        if isinstance(data, str):
            data_bytes = data.encode('utf-8')
        elif isinstance(data, dict):
            data_bytes = json.dumps(data, separators=(',', ':')).encode('utf-8')
        else:
            data_bytes = data
        
        # Если данные маленькие, не сжимаем
        if len(data_bytes) < min_size_threshold:
            return CompressionResult(
                original_size=len(data_bytes),
                compressed_size=len(data_bytes),
                compression_ratio=1.0,
                compression_time_ms=0.0,
                compression_type=CompressionType.NONE,
                compressed_data=data_bytes
            )
        
        # Пробуем разные алгоритмы
        candidates = []
        
        # GZIP - быстрый и надежный
        try:
            gzip_result = self._compress_gzip(data_bytes, level=6)
            if gzip_result.compression_time_ms <= max_compression_time_ms:
                candidates.append(gzip_result)
        except Exception as e:
            print(f"GZIP compression failed: {e}")
        
        # DEFLATE - немного быстрее GZIP
        try:
            deflate_result = self._compress_deflate(data_bytes, level=6)
            if deflate_result.compression_time_ms <= max_compression_time_ms:
                candidates.append(deflate_result)
        except Exception as e:
            print(f"DEFLATE compression failed: {e}")
        
        # Brotli - лучшее сжатие для текста
        try:
            brotli_result = self._compress_brotli(data_bytes, quality=6)
            if brotli_result.compression_time_ms <= max_compression_time_ms:
                candidates.append(brotli_result)
        except Exception as e:
            print(f"Brotli compression failed: {e}")
        
        # Выбираем лучший результат
        if not candidates:
            # Если ничего не подошло, возвращаем без сжатия
            return CompressionResult(
                original_size=len(data_bytes),
                compressed_size=len(data_bytes),
                compression_ratio=1.0,
                compression_time_ms=0.0,
                compression_type=CompressionType.NONE,
                compressed_data=data_bytes
            )
        
        # Выбираем алгоритм с наилучшим соотношением сжатия/времени
        def score_compression(result: CompressionResult) -> float:
            # Чем меньше размер и время, тем лучше
            size_score = 1.0 - result.compression_ratio  # больше сжатие = лучше
            time_score = 1.0 - (result.compression_time_ms / max_compression_time_ms)  # меньше время = лучше
            return size_score * 0.7 + time_score * 0.3  # 70% вес сжатию, 30% времени
        
        best_result = max(candidates, key=score_compression)
        
        # Обновляем статистику
        stats = self.compression_stats[best_result.compression_type]
        stats['total_bytes_in'] += best_result.original_size
        stats['total_bytes_out'] += best_result.compressed_size
        stats['total_time_ms'] += best_result.compression_time_ms
        stats['uses'] += 1
        
        return best_result
    
    def decompress(self, compressed_data: bytes, compression_type: CompressionType) -> bytes:
        """Декомпрессия данных"""
        
        if compression_type == CompressionType.NONE:
            return compressed_data
        elif compression_type == CompressionType.GZIP:
            return gzip.decompress(compressed_data)
        elif compression_type == CompressionType.DEFLATE:
            return zlib.decompress(compressed_data)
        elif compression_type == CompressionType.BROTLI:
            return brotli.decompress(compressed_data)
        else:
            raise ValueError(f"Unknown compression type: {compression_type}")
    
    def get_compression_stats(self) -> Dict:
        """Получение статистики сжатия"""
        
        stats = {}
        
        for comp_type, data in self.compression_stats.items():
            if data['uses'] > 0:
                avg_compression_ratio = data['total_bytes_out'] / data['total_bytes_in']
                avg_time_per_use = data['total_time_ms'] / data['uses']
                
                stats[comp_type.value] = {
                    'uses': data['uses'],
                    'total_bytes_processed': data['total_bytes_in'],
                    'total_bytes_saved': data['total_bytes_in'] - data['total_bytes_out'],
                    'avg_compression_ratio': avg_compression_ratio,
                    'avg_compression_time_ms': avg_time_per_use,
                    'bandwidth_saved_percent': (1 - avg_compression_ratio) * 100
                }
        
        return stats

# HTTP Transfer Optimization
class OptimizedTransferClient:
    def __init__(self):
        self.compressor = IntelligentCompressor()
        self.transfer_stats = {
            'total_requests': 0,
            'compressed_requests': 0,
            'total_bytes_sent': 0,
            'total_bytes_received': 0,
            'total_transfer_time_ms': 0
        }
    
    async def post_json(self, session: aiohttp.ClientSession, url: str, 
                       data: Dict, compress: bool = True) -> Dict:
        """Оптимизированный POST запрос с JSON"""
        
        start_time = time.time()
        
        # Подготавливаем данные
        json_data = json.dumps(data, separators=(',', ':'))  # Компактный JSON
        headers = {
            'Content-Type': 'application/json',
            'Accept': 'application/json',
            'Accept-Encoding': 'gzip, deflate, br'
        }
        
        request_data = json_data.encode('utf-8')
        original_size = len(request_data)
        
        # Сжимаем если нужно
        if compress:
            compression_result = self.compressor.compress_adaptive(
                request_data,
                min_size_threshold=512,  # Сжимаем от 512 байт
                max_compression_time_ms=50.0  # Максимум 50мс на сжатие
            )
            
            if compression_result.compression_type != CompressionType.NONE:
                request_data = compression_result.compressed_data
                
                # Добавляем заголовок сжатия
                if compression_result.compression_type == CompressionType.GZIP:
                    headers['Content-Encoding'] = 'gzip'
                elif compression_result.compression_type == CompressionType.DEFLATE:
                    headers['Content-Encoding'] = 'deflate'
                elif compression_result.compression_type == CompressionType.BROTLI:
                    headers['Content-Encoding'] = 'br'
                
                self.transfer_stats['compressed_requests'] += 1
                
                print(f"  Compressed {original_size} -> {len(request_data)} bytes "
                      f"({compression_result.compression_ratio:.2f} ratio, "
                      f"{compression_result.compression_time_ms:.1f}ms)")
        
        # Отправляем запрос
        headers['Content-Length'] = str(len(request_data))
        
        try:
            async with session.post(url, data=request_data, headers=headers) as response:
                response_data = await response.read()
                
                end_time = time.time()
                transfer_time = (end_time - start_time) * 1000
                
                # Обновляем статистику
                self.transfer_stats['total_requests'] += 1
                self.transfer_stats['total_bytes_sent'] += len(request_data)
                self.transfer_stats['total_bytes_received'] += len(response_data)
                self.transfer_stats['total_transfer_time_ms'] += transfer_time
                
                # Декомпрессируем ответ если нужно
                response_encoding = response.headers.get('Content-Encoding', '').lower()
                if response_encoding in ['gzip', 'deflate', 'br']:
                    if response_encoding == 'gzip':
                        response_data = gzip.decompress(response_data)
                    elif response_encoding == 'deflate':
                        response_data = zlib.decompress(response_data)
                    elif response_encoding == 'br':
                        response_data = brotli.decompress(response_data)
                
                # Парсим JSON
                if response.content_type == 'application/json':
                    return json.loads(response_data.decode('utf-8'))
                else:
                    return {'raw_data': response_data.decode('utf-8', errors='ignore')}
                
        except Exception as e:
            end_time = time.time()
            transfer_time = (end_time - start_time) * 1000
            self.transfer_stats['total_transfer_time_ms'] += transfer_time
            raise
    
    def get_transfer_stats(self) -> Dict:
        """Получение статистики передачи данных"""
        
        stats = dict(self.transfer_stats)
        
        if stats['total_requests'] > 0:
            stats['avg_transfer_time_ms'] = stats['total_transfer_time_ms'] / stats['total_requests']
            stats['compression_usage_percent'] = (stats['compressed_requests'] / stats['total_requests']) * 100
            stats['avg_bytes_sent_per_request'] = stats['total_bytes_sent'] / stats['total_requests']
            stats['avg_bytes_received_per_request'] = stats['total_bytes_received'] / stats['total_requests']
        
        return stats

# Демонстрация оптимизации передачи данных
async def demonstrate_transfer_optimization():
    """Демонстрация оптимизации передачи данных"""
    
    print("🗜️ Data Transfer Optimization Demonstration")
    print("=" * 60)
    
    # Создаем тестовые данные разных размеров
    test_datasets = {
        'small': {'message': 'Hello World', 'timestamp': time.time()},
        'medium': {
            'users': [
                {'id': i, 'name': f'User {i}', 'email': f'user{i}@example.com', 
                 'profile': {'age': 20 + i % 50, 'city': f'City {i % 10}'}}
                for i in range(100)
            ]
        },
        'large': {
            'data': [
                {
                    'id': i,
                    'description': f'This is a long description for item {i} ' * 10,
                    'metadata': {
                        'created_at': time.time() - i * 3600,
                        'tags': [f'tag{j}' for j in range(10)],
                        'attributes': {f'attr_{k}': f'value_{k}_{i}' for k in range(20)}
                    }
                }
                for i in range(200)
            ]
        }
    }
    
    client = OptimizedTransferClient()
    
    async with aiohttp.ClientSession() as session:
        
        print("📊 Testing Compression Efficiency:")
        
        for dataset_name, dataset in test_datasets.items():
            print(f"\n  Dataset: {dataset_name.upper()}")
            
            # Тестируем без сжатия
            print("    Without compression:")
            try:
                start_time = time.time()
                result = await client.post_json(
                    session, 
                    'https://httpbin.org/post', 
                    dataset, 
                    compress=False
                )
                end_time = time.time()
                
                original_size = len(json.dumps(dataset).encode('utf-8'))
                print(f"      Size: {original_size} bytes")
                print(f"      Time: {(end_time - start_time) * 1000:.1f}ms")
                
            except Exception as e:
                print(f"      ERROR: {e}")
            
            # Тестируем со сжатием
            print("    With compression:")
            try:
                start_time = time.time()
                result = await client.post_json(
                    session, 
                    'https://httpbin.org/post', 
                    dataset, 
                    compress=True
                )
                end_time = time.time()
                
                print(f"      Time: {(end_time - start_time) * 1000:.1f}ms")
                
            except Exception as e:
                print(f"      ERROR: {e}")
        
        # Статистика компрессора
        print(f"\n📈 Compression Statistics:")
        compression_stats = client.compressor.get_compression_stats()
        
        for comp_type, stats in compression_stats.items():
            print(f"  {comp_type.upper()}:")
            print(f"    Uses: {stats['uses']}")
            print(f"    Avg compression ratio: {stats['avg_compression_ratio']:.3f}")
            print(f"    Avg compression time: {stats['avg_compression_time_ms']:.1f}ms")
            print(f"    Bandwidth saved: {stats['bandwidth_saved_percent']:.1f}%")
            print(f"    Total bytes saved: {stats['total_bytes_saved']}")
        
        # Статистика передачи
        print(f"\n🚀 Transfer Statistics:")
        transfer_stats = client.get_transfer_stats()
        
        print(f"  Total requests: {transfer_stats['total_requests']}")
        print(f"  Compressed requests: {transfer_stats['compressed_requests']}")
        print(f"  Compression usage: {transfer_stats.get('compression_usage_percent', 0):.1f}%")
        print(f"  Avg transfer time: {transfer_stats.get('avg_transfer_time_ms', 0):.1f}ms")
        print(f"  Total bytes sent: {transfer_stats['total_bytes_sent']}")
        print(f"  Total bytes received: {transfer_stats['total_bytes_received']}")
        
        if 'avg_bytes_sent_per_request' in transfer_stats:
            print(f"  Avg bytes per request: {transfer_stats['avg_bytes_sent_per_request']:.0f}")

# Запуск демонстрации
if __name__ == "__main__":
    asyncio.run(demonstrate_transfer_optimization())
```

### 📝 Практическое задание Неделя 10

1. Реализуйте enterprise-grade connection pool для ваших микросервисов
2. Добавьте Circuit Breaker pattern для всех внешних API вызовов
3. Настройте интеллектуальное сжатие данных для HTTP API
4. Создайте мониторинг эффективности connection management
5. Проведите нагрузочное тестирование с оптимизированными соединениями

### ✅ Контрольные вопросы

- [ ] Понимаете принципы эффективного управления соединениями?
- [ ] Можете реализовать connection pooling с health checking?
- [ ] Знаете как применять Circuit Breaker pattern?
- [ ] Умеете оптимизировать передачу данных через сжатие?
- [ ] Понимаете trade-offs между производительностью и надежностью?

---

# Модуль 6: Безопасность сетевых соединений {#module-6}
*Недели 11-12 | Время изучения: 16-20 часов*

## Неделя 11: Сетевая безопасность

### 🧠 Концепция: Defense in Depth для сетевой безопасности

Принцип многослойной защиты в сетях:

```
Defense in Depth Network Security:
┌─────────────────────────────────────────────────────────────┐
│ Layer 7: Application Security                               │
│ ├─ Input validation                                         │
│ ├─ Authentication & Authorization                           │
│ ├─ Rate limiting & Throttling                              │
│ └─ OWASP Top 10 protection                                 │
├─────────────────────────────────────────────────────────────┤
│ Layer 4: Transport Security                                │
│ ├─ TLS/SSL encryption                                      │
│ ├─ Certificate validation                                  │
│ ├─ Perfect Forward Secrecy                                │
│ └─ Connection security                                     │
├─────────────────────────────────────────────────────────────┤
│ Layer 3: Network Security                                  │
│ ├─ Firewalls & ACLs                                       │
│ ├─ Network segmentation                                   │
│ ├─ Intrusion detection                                    │
│ └─ Traffic analysis                                       │
├─────────────────────────────────────────────────────────────┤
│ Layer 2: Link Security                                     │
│ ├─ VLANs & Network isolation                             │
│ ├─ MAC address filtering                                  │
│ └─ Switch security                                        │
├─────────────────────────────────────────────────────────────┤
│ Layer 1: Physical Security                                 │
│ ├─ Physical access control                                │
│ ├─ Network equipment security                             │
│ └─ Cable & wireless security                              │
└─────────────────────────────────────────────────────────────┘
```

### 🛡️ DDoS Protection и Rate Limiting

**Advanced Rate Limiting Implementation:**

```python
import asyncio
import time
import redis
import hashlib
import ipaddress
from typing import Dict, List, Optional, Set, Tuple
from dataclasses import dataclass
from enum import Enum
import logging
from collections import defaultdict, deque
import aiohttp
from aiohttp import web
import json

class LimitType(Enum):
    REQUESTS_PER_SECOND = "rps"
    REQUESTS_PER_MINUTE = "rpm"
    REQUESTS_PER_HOUR = "rph"
    BANDWIDTH_PER_SECOND = "bps"
    CONCURRENT_CONNECTIONS = "concurrent"

@dataclass
class RateLimit:
    limit_type: LimitType
    threshold: int
    window_seconds: int
    burst_allowance: int = 0  # Дополнительные запросы в burst режиме
    
class ThreatLevel(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class AdvancedRateLimiter:
    def __init__(self, redis_client: redis.Redis = None):
        self.redis = redis_client or redis.Redis(host='localhost', port=6379, db=1)
        self.local_cache = {}  # Локальный кеш для быстрого доступа
        self.blocked_ips: Set[str] = set()
        self.suspicious_ips: Dict[str, float] = {}  # IP -> timestamp последней подозрительной активности
        
        # Конфигурация лимитов
        self.rate_limits = {
            'default': [
                RateLimit(LimitType.REQUESTS_PER_SECOND, 10, 1),
                RateLimit(LimitType.REQUESTS_PER_MINUTE, 300, 60),
                RateLimit(LimitType.REQUESTS_PER_HOUR, 5000, 3600)
            ],
            'authenticated': [
                RateLimit(LimitType.REQUESTS_PER_SECOND, 50, 1),
                RateLimit(LimitType.REQUESTS_PER_MINUTE, 1500, 60),
                RateLimit(LimitType.REQUESTS_PER_HOUR, 20000, 3600)
            ],
            'premium': [
                RateLimit(LimitType.REQUESTS_PER_SECOND, 100, 1),
                RateLimit(LimitType.REQUESTS_PER_MINUTE, 3000, 60),
                RateLimit(LimitType.REQUESTS_PER_HOUR, 50000, 3600)
            ]
        }
        
        # Whitelist и blacklist
        self.ip_whitelist: Set[str] = {'127.0.0.1', '::1'}
        self.ip_blacklist: Set[str] = set()
        
        # Статистика
        self.stats = {
            'total_requests': 0,
            'blocked_requests': 0,
            'suspicious_requests': 0,
            'rate_limited_requests': 0,
            'threats_detected': defaultdict(int)
        }
        
        # Logger
        self.logger = logging.getLogger('RateLimiter')
    
    def _get_client_identifier(self, request: web.Request) -> str:
        """Получение идентификатора клиента"""
        
        # Проверяем заголовки для real IP
        real_ip = (
            request.headers.get('X-Forwarded-For', '').split(',')[0].strip() or
            request.headers.get('X-Real-IP', '') or
            request.remote
        )
        
        # Для authenticated пользователей используем user ID
        if hasattr(request, 'user_id'):
            return f"user:{request.user_id}"
        
        # Для API ключей
        api_key = request.headers.get('Authorization', '').replace('Bearer ', '')
        if api_key:
            return f"api_key:{hashlib.sha256(api_key.encode()).hexdigest()[:16]}"
        
        return f"ip:{real_ip}"
    
    def _get_rate_limit_key(self, identifier: str, limit: RateLimit) -> str:
        """Генерация ключа для rate limit в Redis"""
        current_window = int(time.time() // limit.window_seconds)
        return f"rate_limit:{identifier}:{limit.limit_type.value}:{current_window}"
    
    async def _check_rate_limit(self, identifier: str, limit: RateLimit) -> Tuple[bool, int, int]:
        """Проверка rate limit для конкретного лимита"""
        
        key = self._get_rate_limit_key(identifier, limit)
        
        try:
            # Атомарная операция increment + expire
            pipe = self.redis.pipeline()
            pipe.incr(key)
            pipe.expire(key, limit.window_seconds)
            results = pipe.execute()
            
            current_count = results[0]
            
            # Проверяем лимит (с учетом burst allowance)
            effective_limit = limit.threshold + limit.burst_allowance
            is_allowed = current_count <= effective_limit
            
            return is_allowed, current_count, effective_limit
            
        except Exception as e:
            self.logger.error(f"Redis error in rate limiting: {e}")
            # При ошибке Redis разрешаем запрос
            return True, 0, limit.threshold
    
    def _detect_suspicious_behavior(self, request: web.Request, identifier: str) -> ThreatLevel:
        """Обнаружение подозрительного поведения"""
        
        threat_indicators = []
        
        # 1. Проверка User-Agent
        user_agent = request.headers.get('User-Agent', '').lower()
        suspicious_ua_patterns = [
            'bot', 'crawler', 'spider', 'scraper', 'curl', 'wget', 
            'python-requests', 'python-urllib', 'go-http-client'
        ]
        
        if any(pattern in user_agent for pattern in suspicious_ua_patterns):
            threat_indicators.append('suspicious_user_agent')
        
        if not user_agent or len(user_agent) < 10:
            threat_indicators.append('missing_or_short_user_agent')
        
        # 2. Проверка заголовков
        required_headers = ['accept', 'accept-encoding']
        missing_headers = [h for h in required_headers if h not in request.headers]
        
        if missing_headers:
            threat_indicators.append('missing_common_headers')
        
        # 3. Проверка HTTP методов
        if request.method in ['OPTIONS', 'TRACE', 'CONNECT']:
            threat_indicators.append('unusual_http_method')
        
        # 4. Проверка путей
        suspicious_paths = [
            '/admin', '/wp-admin', '/.env', '/config', '/api/v1/admin',
            '/.git', '/phpmyadmin', '/xmlrpc.php'
        ]
        
        if any(path in request.path for path in suspicious_paths):
            threat_indicators.append('suspicious_path_access')
        
        # 5. Проверка на SQL injection паттерны в URL
        sql_patterns = ['union', 'select', 'drop', 'insert', 'delete', '1=1', '1\'=\'1']
        query_string = request.query_string.lower()
        
        if any(pattern in query_string for pattern in sql_patterns):
            threat_indicators.append('potential_sql_injection')
        
        # 6. Проверка длины URL
        if len(request.path_qs) > 2000:
            threat_indicators.append('unusually_long_url')
        
        # 7. Проверка на excessive параметры
        if len(request.query) > 50:
            threat_indicators.append('excessive_query_parameters')
        
        # Определяем уровень угрозы
        threat_count = len(threat_indicators)
        
        if threat_count >= 4:
            threat_level = ThreatLevel.CRITICAL
        elif threat_count >= 3:
            threat_level = ThreatLevel.HIGH
        elif threat_count >= 2:
            threat_level = ThreatLevel.MEDIUM
        elif threat_count >= 1:
            threat_level = ThreatLevel.LOW
        else:
            threat_level = ThreatLevel.LOW
        
        # Логируем угрозы
        if threat_indicators:
            self.logger.warning(f"Suspicious behavior from {identifier}: {threat_indicators}")
            self.stats['threats_detected'][threat_level.value] += 1
        
        return threat_level
    
    def _is_ip_whitelisted(self, ip: str) -> bool:
        """Проверка IP в whitelist"""
        if ip in self.ip_whitelist:
            return True
        
        # Проверяем подсети
        try:
            ip_obj = ipaddress.ip_address(ip)
            for whitelisted in self.ip_whitelist:
                try:
                    if '/' in whitelisted:  # CIDR notation
                        network = ipaddress.ip_network(whitelisted, strict=False)
                        if ip_obj in network:
                            return True
                except:
                    continue
        except:
            pass
        
        return False
    
    def _is_ip_blacklisted(self, ip: str) -> bool:
        """Проверка IP в blacklist"""
        return ip in self.ip_blacklist or ip in self.blocked_ips
    
    async def check_request(self, request: web.Request) -> Tuple[bool, str, Dict]:
        """Основная проверка запроса"""
        
        self.stats['total_requests'] += 1
        
        identifier = self._get_client_identifier(request)
        ip = identifier.split(':')[-1] if identifier.startswith('ip:') else 'unknown'
        
        # Проверяем whitelist
        if self._is_ip_whitelisted(ip):
            return True, "whitelisted", {}
        
        # Проверяем blacklist
        if self._is_ip_blacklisted(ip):
            self.stats['blocked_requests'] += 1
            return False, "blacklisted", {'reason': 'IP in blacklist'}
        
        # Определяем тип клиента для выбора лимитов
        if identifier.startswith('user:'):
            limit_category = 'premium' if request.headers.get('X-Premium-User') == 'true' else 'authenticated'
        else:
            limit_category = 'default'
        
        # Проверяем все rate limits
        limits_exceeded = []
        for limit in self.rate_limits[limit_category]:
            is_allowed, current_count, max_allowed = await self._check_rate_limit(identifier, limit)
            
            if not is_allowed:
                limits_exceeded.append({
                    'type': limit.limit_type.value,
                    'current': current_count,
                    'limit': max_allowed,
                    'window': limit.window_seconds
                })
        
        if limits_exceeded:
            self.stats['rate_limited_requests'] += 1
            return False, "rate_limited", {
                'limits_exceeded': limits_exceeded,
                'retry_after': min(l['window'] for l in limits_exceeded)
            }
        
        # Проверяем на подозрительное поведение
        threat_level = self._detect_suspicious_behavior(request, identifier)
        
        if threat_level in [ThreatLevel.CRITICAL, ThreatLevel.HIGH]:
            self.stats['suspicious_requests'] += 1
            
            # Добавляем в suspicious list
            self.suspicious_ips[ip] = time.time()
            
            if threat_level == ThreatLevel.CRITICAL:
                # Временная блокировка
                self.blocked_ips.add(ip)
                self.logger.error(f"IP {ip} temporarily blocked due to critical threat level")
                
                return False, "threat_detected", {
                    'threat_level': threat_level.value,
                    'blocked_duration': 3600  # 1 час
                }
        
        return True, "allowed", {}
    
    def add_to_whitelist(self, ip_or_network: str):
        """Добавление IP или сети в whitelist"""
        self.ip_whitelist.add(ip_or_network)
        self.logger.info(f"Added {ip_or_network} to whitelist")
    
    def add_to_blacklist(self, ip: str, duration_seconds: int = 3600):
        """Добавление IP в blacklist"""
        self.ip_blacklist.add(ip)
        
        # Автоматическое удаление через время
        async def remove_from_blacklist():
            await asyncio.sleep(duration_seconds)
            self.ip_blacklist.discard(ip)
            self.logger.info(f"Removed {ip} from blacklist after {duration_seconds}s")
        
        asyncio.create_task(remove_from_blacklist())
        self.logger.info(f"Added {ip} to blacklist for {duration_seconds}s")
    
    def get_stats(self) -> Dict:
        """Получение статистики"""
        
        total_requests = self.stats['total_requests']
        if total_requests == 0:
            return dict(self.stats)
        
        stats = dict(self.stats)
        stats.update({
            'block_rate_percent': (self.stats['blocked_requests'] / total_requests) * 100,
            'suspicious_rate_percent': (self.stats['suspicious_requests'] / total_requests) * 100,
            'rate_limit_rate_percent': (self.stats['rate_limited_requests'] / total_requests) * 100,
            'whitelist_size': len(self.ip_whitelist),
            'blacklist_size': len(self.ip_blacklist),
            'currently_blocked_ips': len(self.blocked_ips),
            'suspicious_ips': len(self.suspicious_ips)
        })
        
        return stats

# Rate Limiting Middleware для aiohttp
def create_rate_limiting_middleware(rate_limiter: AdvancedRateLimiter):
    
    @web.middleware
    async def rate_limiting_middleware(request: web.Request, handler):
        
        # Проверяем запрос через rate limiter
        is_allowed, status, details = await rate_limiter.check_request(request)
        
        if not is_allowed:
            # Подготавливаем ответ об ошибке
            error_response = {
                'error': 'Request blocked',
                'status': status,
                'details': details,
                'timestamp': time.time()
            }
            
            # Определяем HTTP status code
            if status == 'rate_limited':
                http_status = 429  # Too Many Requests
                headers = {}
                if 'retry_after' in details:
                    headers['Retry-After'] = str(details['retry_after'])
            elif status == 'blacklisted':
                http_status = 403  # Forbidden  
                headers = {}
            elif status == 'threat_detected':
                http_status = 403  # Forbidden
                headers = {}
            else:
                http_status = 400  # Bad Request
                headers = {}
            
            return web.json_response(
                error_response, 
                status=http_status,
                headers=headers
            )
        
        # Запрос разрешен, продолжаем обработку
        return await handler(request)
    
    return rate_limiting_middleware

# Пример приложения с защитой от DDoS
async def create_protected_app():
    """Создание защищенного веб-приложения"""
    
    # Создаем rate limiter
    rate_limiter = AdvancedRateLimiter()
    
    # Добавляем доверенные сети в whitelist
    rate_limiter.add_to_whitelist('127.0.0.0/8')    # localhost
    rate_limiter.add_to_whitelist('10.0.0.0/8')     # private network
    rate_limiter.add_to_whitelist('192.168.0.0/16') # private network
    
    # Создаем приложение
    app = web.Application(middlewares=[
        create_rate_limiting_middleware(rate_limiter)
    ])
    
    # API endpoints
    async def health_check(request):
        return web.json_response({'status': 'healthy', 'timestamp': time.time()})
    
    async def api_data(request):
        # Симуляция получения данных
        await asyncio.sleep(0.1)  # имитация работы с БД
        
        return web.json_response({
            'data': [{'id': i, 'value': f'item_{i}'} for i in range(10)],
            'timestamp': time.time()
        })
    
    async def api_stats(request):
        """Эндпоинт для просмотра статистики защиты"""
        stats = rate_limiter.get_stats()
        return web.json_response(stats)
    
    async def admin_block_ip(request):
        """Административный эндпоинт для блокировки IP"""
        data = await request.json()
        ip = data.get('ip')
        duration = data.get('duration', 3600)
        
        if ip:
            rate_limiter.add_to_blacklist(ip, duration)
            return web.json_response({'message': f'IP {ip} blocked for {duration}s'})
        else:
            return web.json_response({'error': 'IP address required'}, status=400)
    
    # Регистрируем routes
    app.router.add_get('/health', health_check)
    app.router.add_get('/api/data', api_data)
    app.router.add_get('/api/stats', api_stats)
    app.router.add_post('/admin/block-ip', admin_block_ip)
    
    return app, rate_limiter

# Демонстрация DDoS защиты
async def demonstrate_ddos_protection():
    """Демонстрация работы DDoS защиты"""
    
    print("🛡️ DDoS Protection System Demonstration")
    print("=" * 60)
    
    app, rate_limiter = await create_protected_app()
    
    # Запускаем сервер в фоне
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, 'localhost', 8080)
    await site.start()
    
    print("✅ Protected server started on http://localhost:8080")
    
    try:
        # Симулируем нормальный трафик
        print("\n📊 Test 1: Normal Traffic")
        
        async with aiohttp.ClientSession() as session:
            for i in range(5):
                async with session.get('http://localhost:8080/api/data') as response:
                    print(f"  Request {i+1}: {response.status}")
                await asyncio.sleep(0.5)
        
        # Симулируем burst трафик (rate limiting)
        print("\n⚡ Test 2: Burst Traffic (Rate Limiting)")
        
        async with aiohttp.ClientSession() as session:
            tasks = []
            for i in range(15):  # Превышаем лимит 10 req/sec
                task = session.get('http://localhost:8080/api/data')
                tasks.append(task)
            
            responses = await asyncio.gather(*tasks, return_exceptions=True)
            
            status_codes = {}
            for i, response in enumerate(responses):
                if isinstance(response, Exception):
                    status_codes[f'Error_{i}'] = str(response)
                else:
                    status = response.status
                    response.close()
                    status_codes[status] = status_codes.get(status, 0) + 1
            
            print(f"  Response status codes: {status_codes}")
        
        # Симулируем подозрительный трафик
        print("\n🚨 Test 3: Suspicious Traffic")
        
        suspicious_headers = {
            'User-Agent': 'python-requests/2.25.1',  # Bot-like UA
            'Accept': '*/*'  # Minimal headers
        }
        
        async with aiohttp.ClientSession(headers=suspicious_headers) as session:
            # Подозрительные запросы
            suspicious_paths = [
                '/admin/users',
                '/api/data?id=1 UNION SELECT * FROM users',
                '/.env',
                '/config/database.yml'
            ]
            
            for path in suspicious_paths:
                try:
                    async with session.get(f'http://localhost:8080{path}') as response:
                        print(f"  Suspicious request to {path}: {response.status}")
                except Exception as e:
                    print(f"  Suspicious request to {path}: ERROR - {e}")
        
        # Показываем статистику
        print("\n📈 Protection Statistics:")
        async with aiohttp.ClientSession() as session:
            async with session.get('http://localhost:8080/api/stats') as response:
                stats = await response.json()
                
                print(f"  Total requests: {stats['total_requests']}")
                print(f"  Blocked requests: {stats['blocked_requests']}")
                print(f"  Rate limited: {stats['rate_limited_requests']}")
                print(f"  Suspicious requests: {stats['suspicious_requests']}")
                print(f"  Block rate: {stats.get('block_rate_percent', 0):.1f}%")
                print(f"  Threats detected: {dict(stats['threats_detected'])}")
    
    finally:
        # Останавливаем сервер
        await runner.cleanup()
        print("\n🛑 Protected server stopped")

# Запуск демонстрации
if __name__ == "__main__":
    asyncio.run(demonstrate_ddos_protection())
```

### 🔍 Network Intrusion Detection

**Real-time Traffic Analysis:**

```python
import asyncio
import time
import statistics
from typing import Dict, List, Set, Optional, Tuple
from dataclasses import dataclass, field
from collections import defaultdict, deque
import ipaddress
import re
import json
import logging

@dataclass
class NetworkPacket:
    timestamp: float
    source_ip: str
    dest_ip: str
    source_port: int
    dest_port: int
    protocol: str
    payload_size: int
    flags: Set[str] = field(default_factory=set)
    payload_preview: str = ""

@dataclass
class ThreatSignature:
    name: str
    pattern: str
    severity: str
    description: str
    category: str

class NetworkIntrusionDetector:
    def __init__(self):
        # Traffic monitoring
        self.connection_tracking: Dict[str, List[NetworkPacket]] = defaultdict(list)
        self.ip_statistics: Dict[str, Dict] = defaultdict(lambda: {
            'packet_count': 0,
            'byte_count': 0,
            'connections': set(),
            'first_seen': time.time(),
            'last_seen': time.time(),
            'suspicious_activity': []
        })
        
        # Threat detection signatures
        self.threat_signatures = [
            ThreatSignature(
                "port_scan",
                r"multiple_ports_single_ip",
                "medium",
                "Port scanning activity detected",
                "reconnaissance" 
            ),
            ThreatSignature(
                "syn_flood", 
                r"excessive_syn_no_ack",
                "high",
                "SYN flood attack detected",
                "dos"
            ),
            ThreatSignature(
                "brute_force",
                r"repeated_failed_auth",
                "high", 
                "Brute force login attempt",
                "authentication"
            ),
            ThreatSignature(
                "data_exfiltration",
                r"large_outbound_transfer",
                "critical",
                "Potential data exfiltration",
                "data_theft"
            ),
            ThreatSignature(
                "malware_callback",
                r"suspicious_dns_query",
                "high",
                "Potential malware callback",
                "malware"
            )
        ]
        
        # Detection thresholds
        self.thresholds = {
            'port_scan_ports': 10,        # Подключений к разным портам
            'port_scan_time': 60,         # За 60 секунд
            'syn_flood_rate': 100,        # SYN пакетов в секунду
            'connection_rate': 50,        # Соединений в секунду
            'bandwidth_threshold': 1024 * 1024 * 10,  # 10MB/s
            'failed_auth_attempts': 5,    # Неудачных попыток авторизации
            'dns_query_rate': 20          # DNS запросов в секунду
        }
        
        # Alerts and statistics
        self.alerts: List[Dict] = deque(maxlen=1000)
        self.statistics = {
            'total_packets': 0,
            'total_bytes': 0,
            'unique_ips': set(),
            'active_connections': 0,
            'threats_detected': defaultdict(int),
            'blocked_ips': set()
        }
        
        # Logger
        self.logger = logging.getLogger('NetworkIDS')
        
        # Whitelist для исключения ложных срабатываний
        self.whitelist_networks = [
            ipaddress.ip_network('127.0.0.0/8'),    # localhost
            ipaddress.ip_network('10.0.0.0/8'),     # private
            ipaddress.ip_network('192.168.0.0/16'), # private
            ipaddress.ip_network('172.16.0.0/12')   # private
        ]
    
    def _is_whitelisted(self, ip: str) -> bool:
        """Проверка IP в whitelist"""
        try:
            ip_obj = ipaddress.ip_address(ip)
            return any(ip_obj in network for network in self.whitelist_networks)
        except:
            return False
    
    def _get_connection_key(self, packet: NetworkPacket) -> str:
        """Генерация ключа соединения"""
        return f"{packet.source_ip}:{packet.source_port}->{packet.dest_ip}:{packet.dest_port}"
    
    async def process_packet(self, packet: NetworkPacket):
        """Обработка сетевого пакета"""
        
        self.statistics['total_packets'] += 1
        self.statistics['total_bytes'] += packet.payload_size
        self.statistics['unique_ips'].add(packet.source_ip)
        self.statistics['unique_ips'].add(packet.dest_ip)
        
        # Обновляем статистику IP адресов
        for ip in [packet.source_ip, packet.dest_ip]:
            stats = self.ip_statistics[ip]
            stats['packet_count'] += 1
            stats['byte_count'] += packet.payload_size
            stats['last_seen'] = packet.timestamp
            
            connection_key = self._get_connection_key(packet)
            stats['connections'].add(connection_key)
        
        # Отслеживаем соединения
        connection_key = self._get_connection_key(packet)
        self.connection_tracking[connection_key].append(packet)
        
        # Ограничиваем историю соединения
        if len(self.connection_tracking[connection_key]) > 100:
            self.connection_tracking[connection_key] = self.connection_tracking[connection_key][-50:]
        
        # Запускаем детекцию угроз
        await self._detect_threats(packet)
    
    async def _detect_threats(self, packet: NetworkPacket):
        """Детекция угроз в реальном времени"""
        
        # Пропускаем whitelisted IPs
        if self._is_whitelisted(packet.source_ip):
            return
        
        current_time = packet.timestamp
        
        # 1. Детекция Port Scanning
        await self._detect_port_scan(packet, current_time)
        
        # 2. Детекция SYN Flood
        await self._detect_syn_flood(packet, current_time)
        
        # 3. Детекция высокой скорости соединений
        await self._detect_connection_flood(packet, current_time)
        
        # 4. Детекция подозрительного трафика по объему
        await self._detect_bandwidth_anomaly(packet, current_time)
        
        # 5. Детекция подозрительных портов
        await self._detect_suspicious_ports(packet)
        
        # 6. Детекция аномального поведения DNS
        await self._detect_dns_anomalies(packet, current_time)
    
    async def _detect_port_scan(self, packet: NetworkPacket, current_time: float):
        """Детекция сканирования портов"""
        
        source_ip = packet.source_ip
        time_window = self.thresholds['port_scan_time']
        
        # Получаем недавние соединения от этого IP
        ip_stats = self.ip_statistics[source_ip]
        recent_connections = [
            conn for conn in ip_stats['connections']
            if current_time - self._get_connection_time(conn) <= time_window
        ]
        
        # Анализируем уникальные порты назначения
        dest_ports = set()
        for conn in recent_connections:
            try:
                dest_port = int(conn.split('->')[1].split(':')[1])
                dest_ports.add(dest_port)
            except:
                continue
        
        # Проверяем превышение порога
        if len(dest_ports) >= self.thresholds['port_scan_ports']:
            await self._create_alert(
                "port_scan",
                f"Port scan detected from {source_ip}",
                {
                    'source_ip': source_ip,
                    'scanned_ports': list(dest_ports),
                    'scan_duration': time_window,
                    'severity': 'medium'
                },
                current_time
            )
            
            self.statistics['threats_detected']['port_scan'] += 1
    
    async def _detect_syn_flood(self, packet: NetworkPacket, current_time: float):
        """Детекция SYN flood атаки"""
        
        if 'SYN' not in packet.flags or 'ACK' in packet.flags:
            return
        
        source_ip = packet.source_ip
        time_window = 1.0  # 1 секунда
        
        # Подсчитываем SYN пакеты за последнюю секунду
        connection_key = self._get_connection_key(packet)
        recent_packets = [
            p for p in self.connection_tracking[connection_key]
            if current_time - p.timestamp <= time_window and 'SYN' in p.flags
        ]
        
        syn_rate = len(recent_packets) / time_window
        
        if syn_rate >= self.thresholds['syn_flood_rate']:
            await self._create_alert(
                "syn_flood",
                f"SYN flood attack detected from {source_ip}",
                {
                    'source_ip': source_ip,
                    'syn_rate': syn_rate,
                    'threshold': self.thresholds['syn_flood_rate'],
                    'severity': 'high'
                },
                current_time
            )
            
            self.statistics['threats_detected']['syn_flood'] += 1
    
    async def _detect_connection_flood(self, packet: NetworkPacket, current_time: float):
        """Детекция флуда соединений"""
        
        source_ip = packet.source_ip
        time_window = 1.0  # 1 секунда
        
        # Подсчитываем новые соединения за последнюю секунду
        ip_stats = self.ip_statistics[source_ip]
        recent_connections = [
            conn for conn in ip_stats['connections']
            if current_time - self._get_connection_time(conn) <= time_window
        ]
        
        connection_rate = len(recent_connections) / time_window
        
        if connection_rate >= self.thresholds['connection_rate']:
            await self._create_alert(
                "connection_flood",
                f"Connection flood detected from {source_ip}",
                {
                    'source_ip': source_ip,
                    'connection_rate': connection_rate,
                    'threshold': self.thresholds['connection_rate'],
                    'severity': 'medium'
                },
                current_time
            )
            
            self.statistics['threats_detected']['connection_flood'] += 1
    
    async def _detect_bandwidth_anomaly(self, packet: NetworkPacket, current_time: float):
        """Детекция аномалий пропускной способности"""
        
        source_ip = packet.source_ip
        time_window = 1.0  # 1 секунда
        
        # Подсчитываем объем данных за последнюю секунду
        ip_stats = self.ip_statistics[source_ip]
        
        # Считаем только исходящий трафик (потенциальная экфильтрация данных)
        if packet.source_ip == source_ip:
            recent_bytes = 0
            connection_key = self._get_connection_key(packet)
            
            for p in self.connection_tracking[connection_key]:
                if (current_time - p.timestamp <= time_window and 
                    p.source_ip == source_ip):
                    recent_bytes += p.payload_size
            
            bandwidth_usage = recent_bytes / time_window  # bytes per second
            
            if bandwidth_usage >= self.thresholds['bandwidth_threshold']:
                await self._create_alert(
                    "data_exfiltration",
                    f"Potential data exfiltration from {source_ip}",
                    {
                        'source_ip': source_ip,
                        'bandwidth_mbps': bandwidth_usage / (1024 * 1024),
                        'threshold_mbps': self.thresholds['bandwidth_threshold'] / (1024 * 1024),
                        'severity': 'critical'
                    },
                    current_time
                )
                
                self.statistics['threats_detected']['data_exfiltration'] += 1
    
    async def _detect_suspicious_ports(self, packet: NetworkPacket):
        """Детекция подозрительных портов"""
        
        # Список подозрительных портов
        suspicious_ports = {
            1433: 'SQL Server',
            3389: 'RDP',
            5432: 'PostgreSQL',
            6379: 'Redis',
            27017: 'MongoDB',
            9200: 'Elasticsearch',
            11211: 'Memcached',
            50070: 'Hadoop NameNode'
        }
        
        if packet.dest_port in suspicious_ports:
            service_name = suspicious_ports[packet.dest_port]
            
            await self._create_alert(
                "suspicious_port_access",
                f"Access to suspicious port {packet.dest_port} ({service_name})",
                {
                    'source_ip': packet.source_ip,
                    'dest_ip': packet.dest_ip,
                    'dest_port': packet.dest_port,
                    'service': service_name,
                    'severity': 'medium'
                },
                packet.timestamp
            )
            
            self.statistics['threats_detected']['suspicious_port'] += 1
    
    async def _detect_dns_anomalies(self, packet: NetworkPacket, current_time: float):
        """Детекция DNS аномалий"""
        
        # Детектируем только DNS трафик (порт 53)
        if packet.dest_port != 53:
            return
        
        source_ip = packet.source_ip
        time_window = 1.0  # 1 секунда
        
        # Подсчитываем DNS запросы за последнюю секунду
        dns_queries = 0
        for conn_key, packets in self.connection_tracking.items():
            for p in packets:
                if (p.source_ip == source_ip and 
                    p.dest_port == 53 and
                    current_time - p.timestamp <= time_window):
                    dns_queries += 1
        
        dns_rate = dns_queries / time_window
        
        if dns_rate >= self.thresholds['dns_query_rate']:
            await self._create_alert(
                "dns_anomaly",
                f"Excessive DNS queries from {source_ip}",
                {
                    'source_ip': source_ip,
                    'dns_rate': dns_rate,
                    'threshold': self.thresholds['dns_query_rate'],
                    'severity': 'medium'
                },
                current_time
            )
            
            self.statistics['threats_detected']['dns_anomaly'] += 1
        
        # Детекция подозрительных DNS доменов
        if packet.payload_preview:
            suspicious_domains = [
                '.tk', '.ml', '.ga', '.cf',  # Бесплатные домены верхнего уровня
                'dyn.', 'ddns.', 'dynamic.',  # Динамические DNS
                'raw.githubusercontent.com',  # Часто используется malware
                'pastebin.com', 'paste.ee'   # Часто используется для C&C
            ]
            
            payload_lower = packet.payload_preview.lower()
            for domain in suspicious_domains:
                if domain in payload_lower:
                    await self._create_alert(
                        "suspicious_dns_query",
                        f"Suspicious DNS query to {domain}",
                        {
                            'source_ip': source_ip,
                            'suspicious_domain': domain,
                            'severity': 'high'
                        },
                        current_time
                    )
                    
                    self.statistics['threats_detected']['suspicious_dns'] += 1
                    break
    
    def _get_connection_time(self, connection_key: str) -> float:
        """Получение времени первого пакета соединения"""
        if connection_key in self.connection_tracking:
            packets = self.connection_tracking[connection_key]
            if packets:
                return packets[0].timestamp
        
        return time.time()
    
    async def _create_alert(self, alert_type: str, message: str, 
                          details: Dict, timestamp: float):
        """Создание alert'а"""
        
        alert = {
            'id': f"{alert_type}_{int(timestamp)}_{hash(message) % 1000}",
            'type': alert_type,
            'message': message,
            'details': details,
            'timestamp': timestamp,
            'source': 'NetworkIDS'
        }
        
        self.alerts.append(alert)
        
        # Логируем критичные alert'ы
        severity = details.get('severity', 'medium')
        if severity in ['high', 'critical']:
            self.logger.error(f"SECURITY ALERT: {message} - {details}")
        else:
            self.logger.warning(f"Security notice: {message}")
        
        # Автоматическая блокировка для критичных угроз
        if severity == 'critical' and 'source_ip' in details:
            source_ip = details['source_ip']
            if not self._is_whitelisted(source_ip):
                self.statistics['blocked_ips'].add(source_ip)
                self.logger.error(f"Automatically blocked IP {source_ip} due to critical threat")
    
    def get_recent_alerts(self, count: int = 10) -> List[Dict]:
        """Получение недавних alert'ов"""
        return list(self.alerts)[-count:]
    
    def get_statistics(self) -> Dict:
        """Получение статистики IDS"""
        
        current_time = time.time()
        
        # Top talking IPs
        top_ips_by_traffic = sorted(
            self.ip_statistics.items(),
            key=lambda x: x[1]['byte_count'],
            reverse=True
        )[:10]
        
        # Active connections count
        active_connections = len([
            conn for conn, packets in self.connection_tracking.items()
            if packets and current_time - packets[-1].timestamp <= 300  # 5 минут
        ])
        
        return {
            'total_packets': self.statistics['total_packets'],
            'total_bytes': self.statistics['total_bytes'],
            'unique_ips_count': len(self.statistics['unique_ips']),
            'active_connections': active_connections,
            'threats_detected': dict(self.statistics['threats_detected']),
            'blocked_ips_count': len(self.statistics['blocked_ips']),
            'total_alerts': len(self.alerts),
            'top_ips_by_traffic': [
                {
                    'ip': ip,
                    'bytes': stats['byte_count'],
                    'packets': stats['packet_count'],
                    'connections': len(stats['connections'])
                }
                for ip, stats in top_ips_by_traffic
            ],
            'threat_breakdown': {
                threat_type: count 
                for threat_type, count in self.statistics['threats_detected'].items()
            }
        }

# Симулятор сетевого трафика для тестирования IDS
class NetworkTrafficSimulator:
    def __init__(self, ids: NetworkIntrusionDetector):
        self.ids = ids
        self.simulation_ips = [
            '192.168.1.100',  # Нормальный клиент
            '10.0.0.50',      # Внутренний сервис
            '203.0.113.10',   # Внешний клиент
            '198.51.100.5',   # Подозрительный IP
            '203.0.113.99'    # Атакующий IP
        ]
    
    async def simulate_normal_traffic(self, duration_seconds: int = 60):
        """Симуляция нормального трафика"""
        
        print(f"🔄 Simulating normal traffic for {duration_seconds} seconds...")
        
        start_time = time.time()
        packet_id = 0
        
        while time.time() - start_time < duration_seconds:
            current_time = time.time()
            
            # Генерируем нормальные пакеты
            for _ in range(5):  # 5 пакетов за итерацию
                packet_id += 1
                
                source_ip = self.simulation_ips[packet_id % 3]  # Первые 3 IP - нормальные
                dest_ip = '93.184.216.34'  # example.com
                
                packet = NetworkPacket(
                    timestamp=current_time,
                    source_ip=source_ip,
                    dest_ip=dest_ip,
                    source_port=10000 + (packet_id % 50000),
                    dest_port=443,  # HTTPS
                    protocol='TCP',
                    payload_size=1024 + (packet_id % 4096),
                    flags={'ACK'},
                    payload_preview=f"GET /api/data HTTP/1.1\r\nHost: example.com"
                )
                
                await self.ids.process_packet(packet)
            
            await asyncio.sleep(0.1)  # 100ms между итерациями
    
    async def simulate_port_scan(self):
        """Симуляция сканирования портов"""
        
        print("🔍 Simulating port scan attack...")
        
        attacker_ip = '198.51.100.5'
        target_ip = '10.0.0.100'
        current_time = time.time()
        
        # Сканируем порты от 1 до 100
        for port in range(1, 101):
            packet = NetworkPacket(
                timestamp=current_time + (port * 0.01),  # 10ms между пакетами
                source_ip=attacker_ip,
                dest_ip=target_ip,
                source_port=50000 + port,
                dest_port=port,
                protocol='TCP',
                payload_size=40,  # Только TCP заголовок
                flags={'SYN'},
                payload_preview=""
            )
            
            await self.ids.process_packet(packet)
            await asyncio.sleep(0.01)  # 10ms задержка
    
    async def simulate_syn_flood(self):
        """Симуляция SYN flood атаки"""
        
        print("⚡ Simulating SYN flood attack...")
        
        attacker_ip = '203.0.113.99'
        target_ip = '10.0.0.200'
        current_time = time.time()
        
        # Отправляем 200 SYN пакетов за секунду
        for i in range(200):
            packet = NetworkPacket(
                timestamp=current_time + (i * 0.005),  # 5ms между пакетами
                source_ip=attacker_ip,
                dest_ip=target_ip,
                source_port=20000 + i,
                dest_port=80,
                protocol='TCP',
                payload_size=40,
                flags={'SYN'},
                payload_preview=""
            )
            
            await self.ids.process_packet(packet)
            await asyncio.sleep(0.005)  # 5ms задержка
    
    async def simulate_data_exfiltration(self):
        """Симуляция кражи данных"""
        
        print("📤 Simulating data exfiltration...")
        
        insider_ip = '10.0.0.150'  # Внутренний IP
        external_ip = '203.0.113.50'  # Внешний сервер
        current_time = time.time()
        
        # Отправляем большие объемы данных
        for i in range(20):
            packet = NetworkPacket(
                timestamp=current_time + (i * 0.1),
                source_ip=insider_ip,
                dest_ip=external_ip,
                source_port=45000 + i,
                dest_port=443,
                protocol='TCP',
                payload_size=1024 * 1024,  # 1MB на пакет
                flags={'ACK', 'PSH'},
                payload_preview="POST /upload HTTP/1.1\r\nContent-Length: 1048576"
            )
            
            await self.ids.process_packet(packet)
            await asyncio.sleep(0.1)
    
    async def simulate_suspicious_dns(self):
        """Симуляция подозрительных DNS запросов"""
        
        print("🔍 Simulating suspicious DNS queries...")
        
        infected_ip = '192.168.1.200'
        dns_server = '8.8.8.8'
        current_time = time.time()
        
        suspicious_domains = [
            'malware-c2.tk',
            'bot.dynamic-dns.net',
            'evil.pastebin.com',
            'raw.githubusercontent.com/malware/config'
        ]
        
        # Высокочастотные DNS запросы к подозрительным доменам
        for i in range(50):
            domain = suspicious_domains[i % len(suspicious_domains)]
            
            packet = NetworkPacket(
                timestamp=current_time + (i * 0.02),  # 20ms между запросами
                source_ip=infected_ip,
                dest_ip=dns_server,
                source_port=53000 + i,
                dest_port=53,
                protocol='UDP',
                payload_size=64,
                flags=set(),
                payload_preview=f"DNS query for {domain}"
            )
            
            await self.ids.process_packet(packet)
            await asyncio.sleep(0.02)

# Демонстрация Network IDS
async def demonstrate_network_ids():
    """Демонстрация работы Network IDS"""
    
    print("🔍 Network Intrusion Detection System Demonstration")
    print("=" * 70)
    
    # Создаем IDS
    ids = NetworkIntrusionDetector()
    simulator = NetworkTrafficSimulator(ids)
    
    print("✅ Network IDS initialized")
    
    # Фоновая задача для нормального трафика
    normal_traffic_task = asyncio.create_task(
        simulator.simulate_normal_traffic(120)  # 2 минуты нормального трафика
    )
    
    # Ждем немного для установления базовой линии
    await asyncio.sleep(5)
    
    print("\n📊 Baseline Statistics:")
    stats = ids.get_statistics()
    print(f"  Total packets: {stats['total_packets']}")
    print(f"  Unique IPs: {stats['unique_ips_count']}")
    print(f"  Active connections: {stats['active_connections']}")
    
    # Симуляция различных атак
    print("\n🚨 Starting Attack Simulations:")
    
    # 1. Port Scan
    await simulator.simulate_port_scan()
    await asyncio.sleep(2)
    
    # 2. SYN Flood  
    await simulator.simulate_syn_flood()
    await asyncio.sleep(2)
    
    # 3. Data Exfiltration
    await simulator.simulate_data_exfiltration()
    await asyncio.sleep(2)
    
    # 4. Suspicious DNS
    await simulator.simulate_suspicious_dns()
    await asyncio.sleep(3)
    
    # Останавливаем нормальный трафик
    normal_traffic_task.cancel()
    
    print("\n🔍 Final IDS Analysis:")
    
    # Финальная статистика
    final_stats = ids.get_statistics()
    print(f"\n📈 Traffic Statistics:")
    print(f"  Total packets processed: {final_stats['total_packets']}")
    print(f"  Total bytes processed: {final_stats['total_bytes']:,}")
    print(f"  Unique IP addresses: {final_stats['unique_ips_count']}")
    print(f"  Active connections: {final_stats['active_connections']}")
    
    print(f"\n🚨 Threat Detection Results:")
    if final_stats['threats_detected']:
        for threat_type, count in final_stats['threats_detected'].items():
            print(f"  {threat_type.replace('_', ' ').title()}: {count} incidents")
    else:
        print("  No threats detected")
    
    print(f"  Blocked IPs: {final_stats['blocked_ips_count']}")
    print(f"  Total alerts generated: {final_stats['total_alerts']}")
    
    # Последние alert'ы
    print(f"\n⚠️ Recent Security Alerts:")
    recent_alerts = ids.get_recent_alerts(10)
    
    for alert in recent_alerts[-10:]:
        severity = alert['details'].get('severity', 'medium')
        timestamp_str = time.strftime('%H:%M:%S', time.localtime(alert['timestamp']))
        
        severity_icon = {
            'low': '💡',
            'medium': '⚠️',
            'high': '🚨',
            'critical': '🔥'
        }.get(severity, '⚠️')
        
        print(f"  {severity_icon} [{timestamp_str}] {alert['message']}")
        
        if severity in ['high', 'critical']:
            details = alert['details']
            if 'source_ip' in details:
                print(f"      Source IP: {details['source_ip']}")
            if 'severity' in details:
                print(f"      Severity: {details['severity'].upper()}")
    
    # Top IPs по трафику
    print(f"\n📊 Top IPs by Traffic:")
    for ip_info in final_stats['top_ips_by_traffic'][:5]:
        print(f"  {ip_info['ip']}: {ip_info['bytes']:,} bytes "
              f"({ip_info['packets']} packets, {ip_info['connections']} connections)")
    
    print(f"\n✅ Network IDS demonstration completed")

# Запуск демонстрации
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(demonstrate_network_ids())# Компьютерные сети для Backend разработчика
## От новичка до эксперта за 16 недель

> **Цель курса:** Получить глубокое понимание сетевых технологий для создания высокопроизводительных, масштабируемых и надежных backend-приложений.

---

## 🎯 Зачем backend-разработчику изучать сети?

Представьте: ваш API отвечает за 500мс вместо 50мс. Пользователи жалуются на тормоза. Где искать проблему?

**80% проблем производительности backend-приложений связаны с сетью:**
- Неэффективное использование HTTP keep-alive
- Плохая настройка TCP буферов
- Неоптимальная балансировка нагрузки
- Проблемы с DNS резолюцией

**Этот курс научит вас:**
- Диагностировать сетевые проблемы за минуты
- Оптимизировать производительность на 70-90%
- Проектировать отказоустойчивые архитектуры
- Обеспечивать безопасность соединений

---

## 📚 Содержание

### Модуль 1: [Основы сетевых технологий](#module-1)
### Модуль 2: [Транспортный уровень](#module-2)
### Модуль 3: [HTTP и веб-протоколы](#module-3)
### Модуль 4: [DNS и сервис обнаружения](#module-4)
### Модуль 5: [Производительность и оптимизация](#module-5)
### Модуль 6: [Безопасность сетевых соединений](#module-6)
### Модуль 7: [Балансировка нагрузки и масштабирование](#module-7)
### Модуль 8: [Современные протоколы и паттерны](#module-8)

---

# Модуль 1: Основы сетевых технологий {#module-1}
*Недели 1-2 | Время изучения: 16-20 часов*

## Неделя 1: Модель OSI и TCP/IP стек

### 🧠 Концепция: Зачем нужны уровни сетевых протоколов?

Представьте отправку письма по почте. Есть несколько этапов:
1. **Написание** (данные приложения)
2. **Упаковка в конверт** (добавление заголовков)
3. **Указание адреса** (маршрутизация)
4. **Физическая доставка** (передача по проводам)

Точно так же работают сетевые протоколы - каждый уровень решает свою задачу.

### 📊 Модель OSI vs TCP/IP

```
OSI (7 уровней)          TCP/IP (4 уровня)         Примеры
┌─────────────────┐      ┌─────────────────┐
│  Application    │ ───→ │  Application    │ ← HTTP, SMTP, FTP
│  Presentation   │      │                 │
│  Session        │      │                 │
├─────────────────┤      ├─────────────────┤
│  Transport      │ ───→ │  Transport      │ ← TCP, UDP
├─────────────────┤      ├─────────────────┤
│  Network        │ ───→ │  Internet       │ ← IP, ICMP
├─────────────────┤      ├─────────────────┤
│  Data Link      │ ───→ │  Network Access │ ← Ethernet, WiFi
│  Physical       │      │                 │
└─────────────────┘      └─────────────────┘
```

### 💡 Почему это важно для backend?

**Пример проблемы:** API тормозит при большой нагрузке

**Диагностика по уровням:**
- **Application (L7):** Медленная обработка запросов в коде
- **Transport (L4):** Исчерпание TCP соединений
- **Network (L3):** Неоптимальная маршрутизация
- **Data Link (L2):** Проблемы с коммутатором

### 🔧 Практический пример: Инкапсуляция данных

Когда ваш код делает HTTP запрос:

```python
import requests
response = requests.get('https://api.example.com/users')
```

Происходит следующее:

```
1. Application Layer (HTTP):
   GET /users HTTP/1.1
   Host: api.example.com
   
2. Transport Layer (TCP):
   [TCP Header: src_port=12345, dst_port=443, seq=100, ack=200]
   
3. Network Layer (IP):
   [IP Header: src=192.168.1.10, dst=93.184.216.34, protocol=TCP]
   
4. Data Link Layer (Ethernet):
   [Ethernet Header: src_mac=AA:BB:CC:DD:EE:FF, dst_mac=11:22:33:44:55:66]
```

### 🛠️ Инструмент для анализа: Wireshark

Посмотрим на реальный HTTP запрос:

```bash
# Захват трафика на интерфейсе
tcpdump -i eth0 -A host api.example.com

# Результат показывает все уровни:
# Ethernet header → IP header → TCP header → HTTP data
```

### ⚡ Влияние на производительность

**MTU (Maximum Transmission Unit) и фрагментация:**

```bash
# Проверка MTU
ip route get 8.8.8.8
# 8.8.8.8 via 192.168.1.1 dev eth0 src 192.168.1.10 mtu 1500

# Тест фрагментации
ping -M do -s 1472 google.com  # OK
ping -M do -s 1473 google.com  # Фрагментация!
```

**Почему это важно:**
- Фрагментированные пакеты = больше overhead
- В datacenter обычно MTU = 9000 (jumbo frames)
- Неправильный MTU может снизить производительность на 30%

### 📝 Практическое задание

1. Установите Wireshark и захватите HTTP трафик вашего приложения
2. Найдите заголовки каждого уровня в одном пакете
3. Измерьте MTU до вашего API сервера
4. Проверьте, используются ли jumbo frames в вашей инфраструктуре

---

## Неделя 2: IP протокол и маршрутизация

### 🧠 Концепция: Как пакеты находят путь в Интернете?

IP - это система адресации и доставки пакетов. Как GPS для данных.

### 📊 Структура IP пакета (IPv4)

```
IPv4 Header (20 байт минимум):
┌─────────────────────────────────────────────────────────────┐
│ Version │  IHL  │    ToS    │         Total Length          │
├─────────────────────────────────────────────────────────────┤
│        Identification         │Flags│   Fragment Offset     │
├─────────────────────────────────────────────────────────────┤
│    TTL    │  Protocol │           Header Checksum          │
├─────────────────────────────────────────────────────────────┤
│                    Source IP Address                        │
├─────────────────────────────────────────────────────────────┤
│                 Destination IP Address                      │
└─────────────────────────────────────────────────────────────┘
```

**Ключевые поля для backend:**
- **TTL (Time To Live):** Сколько роутеров может пройти пакет
- **Protocol:** TCP (6), UDP (17), ICMP (1)
- **Fragment Offset:** Для сборки фрагментированных пакетов

### 🔧 Практический пример: CIDR и подсети

Ваш сервер в подсети `10.0.1.0/24`:

```bash
# Информация о сети
ip addr show eth0
# inet 10.0.1.100/24 brd 10.0.1.255 scope global eth0

# Это означает:
# - Сеть: 10.0.1.0
# - Маска: 255.255.255.0 (/24)
# - Доступные адреса: 10.0.1.1 - 10.0.1.254
# - Broadcast: 10.0.1.255
```

**Расчет подсетей для микросервисов:**

```python
import ipaddress

# Основная сеть для всех сервисов
main_network = ipaddress.IPv4Network('10.0.0.0/16')

# Разделение на подсети для разных сервисов
subnets = list(main_network.subnets(new_prefix=24))

print("Подсети для микросервисов:")
for i, subnet in enumerate(subnets[:10]):
    service_name = ['auth', 'users', 'orders', 'payments', 'inventory', 
                   'notifications', 'analytics', 'logs', 'cache', 'db'][i]
    print(f"{service_name}: {subnet}")

# Результат:
# auth: 10.0.0.0/24
# users: 10.0.1.0/24
# orders: 10.0.2.0/24
# ...
```

### 🌐 Маршрутизация: Как пакеты находят путь?

**Таблица маршрутизации на сервере:**

```bash
# Просмотр таблицы маршрутизации
ip route show
```

```
default via 10.0.1.1 dev eth0               # Шлюз по умолчанию
10.0.1.0/24 dev eth0 proto kernel scope link # Локальная сеть
127.0.0.0/8 dev lo scope host                # Loopback
```

**Трассировка маршрута до API:**

```bash
# Проследим путь пакета
traceroute api.example.com

# Результат показывает каждый hop:
1  gateway (10.0.1.1)          1.234 ms
2  isp-router (192.168.100.1)  15.678 ms
3  backbone-1 (203.0.113.1)    25.432 ms
4  cloudflare (104.16.133.229) 45.123 ms
5  api.example.com (93.184.216.34) 50.789 ms
```

### ⚡ BGP и влияние на производительность API

**Почему некоторые запросы медленные?**

BGP (Border Gateway Protocol) определяет маршруты между провайдерами:

```bash
# Проверка AS (Autonomous System) пути
mtr --report --report-cycles 5 api.example.com

# Может показать:
# Hop 1: AS64512 (Ваш провайдер)
# Hop 2: AS174 (Cogent) - медленный пиринг!
# Hop 3: AS13335 (Cloudflare)
```

**Практический пример оптимизации:**

```yaml
# docker-compose.yml для тестирования разных регионов
version: '3.8'
services:
  api-test:
    image: tutum/curl
    command: |
      sh -c "
        echo 'Testing US East:'
        time curl -s us-east.api.example.com/health
        echo 'Testing EU West:'
        time curl -s eu-west.api.example.com/health
        echo 'Testing Asia:'
        time curl -s asia.api.example.com/health
      "
```

### 🔍 IPv6 для современных приложений

**Почему IPv6 важен для backend:**

```bash
# Проверка поддержки IPv6
curl -6 ipv6.google.com
# Если работает - у вас есть IPv6 connectivity

# Конфигурация nginx для dual-stack
# /etc/nginx/sites-available/api
server {
    listen 80;
    listen [::]:80;  # IPv6
    listen 443 ssl http2;
    listen [::]:443 ssl http2;  # IPv6 SSL
    
    server_name api.example.com;
}
```

**Преимущества IPv6 для backend:**
- Больше адресов = прямые соединения без NAT
- Лучшая производительность (нет трансляции адресов)
- Обязательная поддержка IPSec = лучшая безопасность

### 🛠️ Практические инструменты диагностики

```bash
# 1. Проверка связности
ping -c 4 api.example.com

# 2. Проверка конкретного порта
nc -zv api.example.com 443

# 3. Анализ маршрута с потерями
mtr --report api.example.com

# 4. Информация о IP
whois 93.184.216.34

# 5. DNS + IP информация
dig +short api.example.com
nslookup api.example.com
```

### 📊 Мониторинг сетевых метрик

**Python скрипт для мониторинга:**

```python
import time
import subprocess
import json
from datetime import datetime

def check_network_health(target_host):
    """Проверка сетевого здоровья до целевого хоста"""
    
    # Ping тест
    ping_result = subprocess.run(
        ['ping', '-c', '4', target_host], 
        capture_output=True, text=True
    )
    
    # Парсинг результата ping
    if ping_result.returncode == 0:
        lines = ping_result.stdout.split('\n')
        stats_line = [l for l in lines if 'min/avg/max' in l][0]
        avg_latency = float(stats_line.split('/')[4])
    else:
        avg_latency = None
    
    # Traceroute для анализа пути
    trace_result = subprocess.run(
        ['traceroute', '-n', '-q', '1', target_host], 
        capture_output=True, text=True
    )
    
    hop_count = len([l for l in trace_result.stdout.split('\n') 
                    if l.strip() and not l.startswith('traceroute')])
    
    return {
        'timestamp': datetime.now().isoformat(),
        'target': target_host,
        'ping_success': ping_result.returncode == 0,
        'avg_latency_ms': avg_latency,
        'hop_count': hop_count
    }

# Мониторинг ваших API endpoints
endpoints = ['api.example.com', 'auth.example.com', 'db.example.com']

while True:
    for endpoint in endpoints:
        health = check_network_health(endpoint)
        print(json.dumps(health, indent=2))
        
        # Алерт если латентность > 100ms
        if health['avg_latency_ms'] and health['avg_latency_ms'] > 100:
            print(f"⚠️ HIGH LATENCY to {endpoint}: {health['avg_latency_ms']}ms")
    
    time.sleep(60)  # Проверка каждую минуту
```

### 📝 Практическое задание

1. Создайте схему сети для вашего приложения с подсетями
2. Проследите путь пакета от клиента до вашего API
3. Настройте мониторинг сетевых метрик
4. Проверьте поддержку IPv6 в вашей инфраструктуре
5. Найдите узкие места в маршрутизации до ваших серверов

### ✅ Контрольные вопросы

- [ ] Можете объяснить, что происходит с пакетом на каждом уровне OSI?
- [ ] Умеете рассчитывать подсети для микросервисной архитектуры?
- [ ] Знаете, как диагностировать проблемы маршрутизации?
- [ ] Понимаете влияние MTU на производительность?
- [ ] Можете настроить dual-stack (IPv4 + IPv6) сервер?

---

# Модуль 2: Транспортный уровень {#module-2}
*Недели 3-4 | Время изучения: 16-20 часов*

## Неделя 3: TCP протокол глубоко

### 🧠 Концепция: Почему TCP называют "надежным" протоколом?

TCP - это как заказная почта с уведомлением о доставке. Каждый пакет подтверждается, порядок гарантируется.

### 📊 Жизненный цикл TCP соединения

```
Клиент                    Сервер
   │                         │
   │      SYN (seq=100)      │
   │────────────────────────→│  1. Запрос соединения
   │                         │
   │   SYN-ACK (seq=200,     │
   │    ack=101)             │
   │←────────────────────────│  2. Подтверждение + свой запрос
   │                         │
   │      ACK (ack=201)      │
   │────────────────────────→│  3. Финальное подтверждение
   │                         │
   │    ESTABLISHED          │
   │◄═══════════════════════►│  4. Соединение установлено
```

### 🔧 Практический пример: 3-way handshake в реальности

**Захват TCP handshake:**

```bash
# Запускаем tcpdump
sudo tcpdump -i eth0 -nn host api.example.com and port 443

# В другом терминале делаем запрос
curl -I https://api.example.com/health

# Видим в tcpdump:
# 12:34:56.789 IP 192.168.1.10.12345 > 93.184.216.34.443: Flags [S], seq 1234567890
# 12:34:56.825 IP 93.184.216.34.443 > 192.168.1.10.12345: Flags [S.], seq 987654321, ack 1234567891
# 12:34:56.826 IP 192.168.1.10.12345 > 93.184.216.34.443: Flags [.], ack 987654322
```

### ⚡ TCP параметры критичные для backend

**1. TCP буферы - влияют на пропускную способность:**

```bash
# Проверка текущих настроек
sysctl net.core.rmem_max        # Максимальный receive buffer
sysctl net.core.wmem_max        # Максимальный send buffer
sysctl net.ipv4.tcp_rmem        # TCP receive buffer: min default max
sysctl net.ipv4.tcp_wmem        # TCP send buffer: min default max

# Оптимизация для высоконагруженных серверов
echo 'net.core.rmem_max = 134217728' >> /etc/sysctl.conf    # 128MB
echo 'net.core.wmem_max = 134217728' >> /etc/sysctl.conf    # 128MB
echo 'net.ipv4.tcp_rmem = 4096 65536 134217728' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_wmem = 4096 65536 134217728' >> /etc/sysctl.conf
```

**2. TCP_NODELAY и алгоритм Nagle:**

```python
import socket

# Плохо для real-time API:
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
# По умолчанию Nagle включен - мелкие пакеты буферизуются

# Хорошо для real-time API:
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
# Отправляем данные немедленно, без буферизации
```

**Пример в Node.js сервере:**

```javascript
const net = require('net');

const server = net.createServer((socket) => {
    // Отключаем алгоритм Nagle для WebSocket или gaming API
    socket.setNoDelay(true);
    
    // Настройка буферов
    socket.setKeepAlive(true, 60000);  // Keep-alive каждые 60 сек
    
    socket.on('data', (data) => {
        // Обработка данных без задержек
        const response = processRequest(data);
        socket.write(response);  // Отправляется немедленно
    });
});

server.listen(8080);
```

### 🔄 TCP Window и управление потоком

**Sliding Window Protocol:**

```
Отправитель                           Получатель
     │                                    │
     │ DATA (seq=1000, len=1000)         │
     │──────────────────────────────────→│ Window: 64KB
     │                                    │
     │ DATA (seq=2000, len=1000)         │
     │──────────────────────────────────→│ Window: 63KB
     │                                    │
     │          ACK (ack=2000, win=32KB) │
     │←──────────────────────────────────│ Получатель перегружен!
     │                                    │
     │ Снижаем скорость отправки         │
```

**Мониторинг TCP окон:**

```bash
# Статистика TCP соединений
ss -i state established '( dport = :443 or sport = :443 )'

# Результат показывает:
# tcp   ESTAB   0   0   192.168.1.10:12345   93.184.216.34:443
#       cubic rto:201 rtt:50.5/1.2 ato:40 mss:1448 pmtu:1500 rcvmss:1448
#       advmss:1448 cwnd:10 ssthresh:7 bytes_acked:23456 bytes_received:54321
#       segs_out:45 segs_in:67 data_segs_out:23 data_segs_in:34
#       send 2.3Mbps lastsnd:1234 lastrcv:567 lastack:567 pacing_rate 2.8Mbps
#       delivery_rate 1.9Mbps busy:12345ms unacked:0 rcv_space:29200 rcv_ssthresh:29200
```

### 🛠️ Connection Pooling - критично для производительности

**Проблема без пула соединений:**

```python
import requests
import time

# ПЛОХО: Новое соединение на каждый запрос
def bad_api_calls():
    for i in range(100):
        start = time.time()
        response = requests.get('https://api.example.com/data')
        end = time.time()
        print(f"Request {i}: {end - start:.3f}s")  # ~200-300ms на каждый

# Результат: 100 запросов = 25-30 секунд
```

**Решение с пулом соединений:**

```python
import requests
import time
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# ХОРОШО: Переиспользование соединений
def good_api_calls():
    session = requests.Session()
    
    # Настройка connection pool
    adapter = HTTPAdapter(
        pool_connections=10,    # Количество хостов в пуле
        pool_maxsize=20,        # Максимум соединений на хост
        max_retries=Retry(
            total=3,
            backoff_factor=0.3,
            status_forcelist=[429, 500, 502, 503, 504]
        )
    )
    
    session.mount('https://', adapter)
    session.mount('http://', adapter)
    
    for i in range(100):
        start = time.time()
        response = session.get('https://api.example.com/data')
        end = time.time()
        print(f"Request {i}: {end - start:.3f}s")  # ~50-80ms после первого

# Результат: 100 запросов = 6-8 секунд (в 4 раза быстрее!)
```

### 📊 TCP состояния и диагностика проблем

**Мониторинг состояний TCP:**

```bash
# Количество соединений в каждом состоянии
ss -s

# Детальная информация
netstat -tan | awk '{print $6}' | sort | uniq -c

# Типичный результат для веб-сервера:
#    245 ESTABLISHED    ← Активные соединения
#     12 TIME_WAIT      ← Закрывающиеся соединения
#      8 LISTEN         ← Слушающие порты
#      3 SYN_RECV       ← Входящие подключения
```

**Проблема: слишком много TIME_WAIT:**

```bash
# Проверка TIME_WAIT соединений
ss -tan state time-wait | wc -l

# Если много (>1000), настраиваем:
echo 'net.ipv4.tcp_tw_reuse = 1' >> /etc/sysctl.conf
echo 'net.ipv4.tcp_fin_timeout = 30' >> /etc/sysctl.conf
sysctl -p
```

### 🔧 Практический пример: Оптимизация веб-сервера

**Конфигурация Nginx для высокой нагрузки:**

```nginx
# /etc/nginx/nginx.conf
worker_processes auto;
worker_rlimit_nofile 65535;

events {
    worker_connections 4096;
    use epoll;
    multi_accept on;
}

http {
    # TCP оптимизация
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    
    # Keep-alive настройки
    keepalive_timeout 65s;
    keepalive_requests 1000;
    
    # Upstream с connection pooling
    upstream api_backend {
        keepalive 32;  # Пул соединений к backend
        
        server backend1.example.com:8080 max_fails=3 fail_timeout=30s;
        server backend2.example.com:8080 max_fails=3 fail_timeout=30s;
        server backend3.example.com:8080 max_fails=3 fail_timeout=30s;
    }
    
    server {
        listen 80;
        
        location /api/ {
            proxy_pass http://api_backend;
            proxy_http_version 1.1;
            proxy_set_header Connection "";  # Важно для keepalive
            proxy_set_header Host $host;
        }
    }
}
```

### 📝 Практическое задание

1. Настройте TCP параметры на вашем сервере для оптимальной производительности
2. Реализуйте connection pooling в вашем приложении
3. Проанализируйте TCP трафик между вашими микросервисами
4. Создайте мониторинг TCP состояний с алертами

---

## Неделя 4: UDP и современные протоколы

### 🧠 Концепция: Когда скорость важнее надежности

UDP - это как отправка открытки. Быстро, дешево, но без гарантий доставки.

### 📊 UDP vs TCP сравнение

```
TCP                          UDP
┌─────────────────────┐     ┌─────────────────────┐
│ ✅ Надежная доставка │     │ ⚡ Высокая скорость  │
│ ✅ Порядок пакетов   │     │ ⚡ Низкая латентность│
│ ✅ Контроль потока   │     │ ⚡ Минимум overhead  │
│ ❌ Больше overhead   │     │ ❌ Нет гарантий     │
│ ❌ Выше латентность  │     │ ❌ Может потеряться │
│ ❌ Установка соедин. │     │ ❌ Нарушен порядок  │
└─────────────────────┘     └─────────────────────┘

Использование:                Использование:
• HTTP/HTTPS                  • DNS запросы
• API вызовы                  • Video streaming
• Файловые передачи           • Online игры
• Email                       • Метрики/логи
```

### 🔧 Практический пример: UDP для метрик

**Отправка метрик через UDP (StatsD):**

```python
import socket
import time
import json

class UDPMetrics:
    def __init__(self, host='localhost', port=8125):
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.host = host
        self.port = port
    
    def counter(self, metric, value=1, tags=None):
        """Счетчик событий"""
        message = f"{metric}:{value}|c"
        if tags:
            message += f"|#{','.join(tags)}"
        self._send(message)
    
    def gauge(self, metric, value, tags=None):
        """Текущее значение"""
        message = f"{metric}:{value}|g"
        if tags:
            message += f"|#{','.join(tags)}"
        self._send(message)
    
    def timing(self, metric, value, tags=None):
        """Время выполнения"""
        message = f"{metric}:{value}|ms"
        if tags:
            message += f"|#{','.join(tags)}"
        self._send(message)
    
    def _send(self, message):
        try:
            self.sock.sendto(message.encode(), (self.host, self.port))
        except Exception:
            # Ошибки UDP игнорируем - метрики не должны ломать приложение
            pass

# Использование в API
metrics = UDPMetrics()

def api_endpoint():
    start_time = time.time()
    
    try:
        # Бизнес логика
        result = process_request()
        
        # Метрики успеха
        metrics.counter('api.requests.success', tags=['endpoint:users'])
        metrics.timing('api.response_time', 
                      int((time.time() - start_time) * 1000),
                      tags=['endpoint:users'])
        
        return result
        
    except Exception as e:
        # Метрики ошибок
        metrics.counter('api.requests.error', 
                       tags=['endpoint:users', f'error:{type(e).__name__}'])
        raise
```

**Почему UDP для метрик?**
- Если метрика потеряется - не критично
- Высокая производительность (тысячи метрик в секунду)
- Не блокирует основной код при проблемах с сетью

### 🌐 QUIC - революция в транспортных протоколах

**Проблемы TCP, которые решает QUIC:**

```
TCP проблемы:                 QUIC решения:
┌─────────────────────┐      ┌─────────────────────┐
│ Head-of-line        │  →   │ Независимые потоки │
│ blocking            │      │ (streams)           │
├─────────────────────┤      ├─────────────────────┤
│ 3-way handshake     │  →   │ 0-RTT установка     │
│ задержка            │      │ соединения          │
├─────────────────────┤      ├─────────────────────┤
│ Проблемы с NAT/     │  →   │ Встроенная миграция │
│ сменой сети         │      │ соединений          │
├─────────────────────┤      ├─────────────────────┤
│ Медленный старт     │  →   │ Быстрая адаптация   │
│ после потерь        │      │ после потерь        │
└─────────────────────┘      └─────────────────────┘
```

### 🔧 Практический пример: HTTP/3 с QUIC

**Тестирование QUIC поддержки:**

```bash
# Проверка поддержки HTTP/3
curl --http3 -I https://cloudflare.com
# HTTP/3 200
# alt-svc: h3=":443"; ma=86400

# Сравнение производительности
echo "HTTP/1.1:"
curl -w "@curl-format.txt" -s -o /dev/null https://api.example.com/test

echo "HTTP/2:"
curl --http2 -w "@curl-format.txt" -s -o /dev/null https://api.example.com/test

echo "HTTP/3 (QUIC):"
curl --http3 -w "@curl-format.txt" -s -o /dev/null https://api.example.com/test

# curl-format.txt:
#      time_namelookup:  %{time_namelookup}\n
#         time_connect:  %{time_connect}\n
#      time_appconnect:  %{time_appconnect}\n
#       time_pretransfer: %{time_pretransfer}\n
#          time_starttransfer: %{time_starttransfer}\n
#                     ----------\n
#              time_total:  %{time_total}\n
```

### 🛠️ Настройка QUIC в современных серверах

**Caddy сервер с HTTP/3:**

```caddyfile
# Caddyfile
api.example.com {
    # HTTP/3 включен по умолчанию
    reverse_proxy localhost:8080
    
    # Форсированное использование HTTP/3
    header Alt-Svc "h3=\":443\"; ma=86400"
    
    # Логирование версий HTTP
    log {
        output file /var/log/caddy/api.log
        format json
        level INFO
    }
}
```

**Nginx с QUIC (экспериментальная поддержка):**

```nginx
# Компиляция с QUIC поддержкой
# ./configure --with-http_v3_module --with-stream_quic_module

server {
    listen 443 quic;
    listen 443 ssl http2;
    
    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;
    
    # Указываем клиентам о QUIC поддержке
    add_header Alt-Svc 'h3=":443"; ma=86400';
    
    location / {
        proxy_pass http://backend;
    }
}
```

### 📊 WebRTC для P2P соединений

**Пример WebRTC data channel:**

```javascript
// Сервер Node.js для WebRTC signaling
const WebSocket = require('ws');
const wss = new WebSocket.Server({ port: 8080 });

const rooms = new Map();

wss.on('connection', (ws) => {
    ws.on('message', (message) => {
        const data = JSON.parse(message);
        
        switch (data.type) {
            case 'join':
                if (!rooms.has(data.room)) {
                    rooms.set(data.room, new Set());
                }
                rooms.get(data.room).add(ws);
                ws.room = data.room;
                break;
                
            case 'offer':
            case 'answer':
            case 'ice-candidate':
                // Пересылаем WebRTC сигналы другим участникам
                if (rooms.has(ws.room)) {
                    rooms.get(ws.room).forEach(client => {
                        if (client !== ws && client.readyState === WebSocket.OPEN) {
                            client.send(message);
                        }
                    });
                }
                break;
        }
    });
});
```

**Клиентская часть:**

```javascript
// Установка P2P соединения через UDP
const pc = new RTCPeerConnection({
    iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
});

// Создание data channel для бинарных данных
const dataChannel = pc.createDataChannel('metrics', {
    ordered: false,    // UDP-подобное поведение
    maxRetransmits: 0  // Не повторяем потерянные пакеты
});

dataChannel.onopen = () => {
    // Отправка метрик напрямую между сервисами
    setInterval(() => {
        const metrics = {
            timestamp: Date.now(),
            cpu: process.cpuUsage(),
            memory: process.memoryUsage(),
            connections: getConnectionCount()
        };
        
        dataChannel.send(JSON.stringify(metrics));
    }, 1000);
};
```

### ⚡ Сравнение производительности протоколов

**Бенчмарк тест:**

```python
import socket
import time
import threading
import statistics

def tcp_benchmark(host, port, message_count=1000):
    """Тест TCP производительности"""
    times = []
    
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.connect((host, port))
    
    for i in range(message_count):
        start = time.time()
        sock.send(f"message_{i}".encode())
        response = sock.recv(1024)
        end = time.time()
        times.append((end - start) * 1000)  # в миллисекундах
    
    sock.close()
    
    return {
        'protocol': 'TCP',
        'avg_latency': statistics.mean(times),
        'median_latency': statistics.median(times),
        'p95_latency': statistics.quantiles(times, n=20)[18],  # 95-й перцентиль
        'total_time': sum(times)
    }

def udp_benchmark(host, port, message_count=1000):
    """Тест UDP производительности"""
    times = []
    
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    
    for i in range(message_count):
        start = time.time()
        sock.sendto(f"message_{i}".encode(), (host, port))
        # UDP - не ждем ответа, измеряем только время отправки
        end = time.time()
        times.append((end - start) * 1000)
    
    sock.close()
    
    return {
        'protocol': 'UDP',
        'avg_latency': statistics.mean(times),
        'median_latency': statistics.median(times),
        'p95_latency': statistics.quantiles(times, n=20)[18],
        'total_time': sum(times)
    }

# Запуск тестов
tcp_results = tcp_benchmark('localhost', 8080)
udp_results = udp_benchmark('localhost', 8081)

print("Результаты бенчмарка:")
print(f"TCP: {tcp_results['avg_latency']:.2f}ms средняя, {tcp_results['p95_latency']:.2f}ms p95")
print(f"UDP: {udp_results['avg_latency']:.2f}ms средняя, {udp_results['p95_latency']:.2f}ms p95")
print(f"UDP быстрее в {tcp_results['avg_latency'] / udp_results['avg_latency']:.1f} раз")
```

### 📝 Практическое задание

1. Реализуйте UDP-сервер для сбора метрик от ваших микросервисов
2. Протестируйте HTTP/3 на одном из ваших API endpoints
3. Сравните производительность TCP и UDP для вашего use case
4. Настройте WebRTC data channel для передачи данных между сервисами

### ✅ Контрольные вопросы

- [ ] Понимаете разницу в overhead между TCP и UDP?
- [ ] Можете объяснить, когда использовать UDP вместо TCP?
- [ ] Знаете принципы работы QUIC и его преимущества?
- [ ] Умеете настроить HTTP/3 на веб-сервере?
- [ ] Понимаете применение WebRTC для backend систем?

---

# Модуль 3: HTTP и веб-протоколы {#module-3}
*Недели 5-7 | Время изучения: 24-30 часов*

## Неделя 5: HTTP/1.1 фундаментальные знания

### 🧠 Концепция: HTTP как язык общения между клиентом и сервером

HTTP - это протокол "вопрос-ответ". Клиент задает вопрос (request), сервер дает ответ (response).

### 📊 Анатомия HTTP запроса и ответа

```
HTTP Request:
┌─────────────────────────────────────────────────────────────┐
│ POST /api/users HTTP/1.1                    ← Стартовая строка │
│ Host: api.example.com                       ← Заголовки       │
│ Content-Type: application/json              │
│ Content-Length: 58                          │
│ Authorization: Bearer eyJhbGciOiJIUzI1NiI... │
│ User-Agent: MyApp/1.0                       │
│ Connection: keep-alive                      │
│                                             ← Пустая строка   │
│ {"name": "John", "email": "john@example.com"} ← Тело запроса │
└─────────────────────────────────────────────────────────────┘

HTTP Response:
┌─────────────────────────────────────────────────────────────┐
│ HTTP/1.1 201 Created                        ← Статус строка  │
│ Content-Type: application/json              ← Заголовки       │
│ Content-Length: 87                          │
│ Location: /api/users/12345                  │
│ Set-Cookie: session=abc123; HttpOnly        │
│ Cache-Control: no-cache                     │
│ X-RateLimit-Remaining: 99                   │
│                                             ← Пустая строка   │
│ {"id": 12345, "name": "John", "created_at": "2024-01-15"} ← Тело │
└─────────────────────────────────────────────────────────────┘
```

### 🔧 Практический пример: Правильное использование HTTP методов

**RESTful API с правильной семантикой:**

```python
from flask import Flask, request, jsonify, make_response
import time

app = Flask(__name__)

# GET - безопасный и идемпотентный
@app.route('/api/users/<int:user_id>', methods=['GET'])
def get_user(user_id):
    # Добавляем заголовки кеширования
    response = make_response(jsonify({
        'id': user_id,
        'name': 'John Doe',
        'last_modified': '2024-01-15T10:30:00Z'
    }))
    
    # Кеш на 5 минут
    response.headers['Cache-Control'] = 'public, max-age=300'
    response.headers['ETag'] = f'"user-{user_id}-v1"'
    response.headers['Last-Modified'] = 'Mon, 15 Jan 2024 10:30:00 GMT'
    
    return response

# POST - создание ресурса (не идемпотентный)
@app.route('/api/users', methods=['POST'])
def create_user():
    data = request.json
    
    # Валидация
    if not data or 'name' not in data:
        return jsonify({'error': 'Name is required'}), 400
    
    # Создание пользователя
    new_user = {
        'id': 12345,  # Генерируется сервером
        'name': data['name'],
        'created_at': time.strftime('%Y-%m-%dT%H:%M:%SZ')
    }
    
    response = make_response(jsonify(new_user), 201)
    response.headers['Location'] = f'/api/users/{new_user["id"]}'
    
    return response

# PUT - полное обновление (идемпотентный)
@app.route('/api/users/<int:user_id>', methods=['PUT'])
def update_user(user_id):
    data = request.json
    
    # PUT заменяет весь ресурс
    updated_user = {
        'id': user_id,
        'name': data.get('name', ''),
        'email': data.get('email', ''),
        'updated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ')
    }
    
    return jsonify(updated_user)

# PATCH - частичное обновление (идемпотентный)
@app.route('/api/users/<int:user_id>', methods=['PATCH'])
def patch_user(user_id):
    data = request.json
    
    # PATCH обновляет только переданные поля
    current_user = get_current_user(user_id)  # Получаем текущие данные
    
    # Обновляем только переданные поля
    for field in ['name', 'email', 'phone']:
        if field in data:
            current_user[field] = data[field]
    
    current_user['updated_at'] = time.strftime('%Y-%m-%dT%H:%M:%SZ')
    
    return jsonify(current_user)

# DELETE - удаление (идемпотентный)
@app.route('/api/users/<int:user_id>', methods=['DELETE'])
def delete_user(user_id):
    # Проверяем существование
    if not user_exists(user_id):
        return '', 404
    
    # Удаляем
    delete_user_from_db(user_id)
    
    # 204 = успешно удалено, нет контента
    return '', 204

# HEAD - только заголовки (для проверки существования)
@app.route('/api/users/<int:user_id>', methods=['HEAD'])
def head_user(user_id):
    if not user_exists(user_id):
        return '', 404
    
    response = make_response('', 200)
    response.headers['Content-Type'] = 'application/json'
    response.headers['Content-Length'] = '156'  # Размер полного ответа
    response.headers['ETag'] = f'"user-{user_id}-v1"'
    
    return response
```

### 📈 HTTP Status Codes - говорящие коды ответов

**Правильное использование статус кодов:**

```python
class APIResponse:
    """Класс для стандартизации API ответов"""
    
    @staticmethod
    def success(data=None, message="Success"):
        """200 - Успешный запрос"""
        return jsonify({
            'success': True,
            'message': message,
            'data': data
        }), 200
    
    @staticmethod
    def created(data=None, location=None):
        """201 - Ресурс создан"""
        response = make_response(jsonify({
            'success': True,
            'message': 'Resource created',
            'data': data
        }), 201)
        
        if location:
            response.headers['Location'] = location
        
        return response
    
    @staticmethod
    def no_content():
        """204 - Успешно, но нет контента"""
        return '', 204
    
    @staticmethod
    def not_modified():
        """304 - Ресурс не изменился"""
        return '', 304
    
    @staticmethod
    def bad_request(message="Bad request", errors=None):
        """400 - Некорректный запрос"""
        return jsonify({
            'success': False,
            'message': message,
            'errors': errors or []
        }), 400
    
    @staticmethod
    def unauthorized(message="Unauthorized"):
        """401 - Требуется аутентификация"""
        response = make_response(jsonify({
            'success': False,
            'message': message
        }), 401)
        response.headers['WWW-Authenticate'] = 'Bearer'
        return response
    
    @staticmethod
    def forbidden(message="Forbidden"):
        """403 - Доступ запрещен"""
        return jsonify({
            'success': False,
            'message': message
        }), 403
    
    @staticmethod
    def not_found(message="Resource not found"):
        """404 - Ресурс не найден"""
        return jsonify({
            'success': False,
            'message': message
        }), 404
    
    @staticmethod
    def conflict(message="Conflict"):
        """409 - Конфликт (например, email уже существует)"""
        return jsonify({
            'success': False,
            'message': message
        }), 409
    
    @staticmethod
    def rate_limited(retry_after=3600):
        """429 - Превышен лимит запросов"""
        response = make_response(jsonify({
            'success': False,
            'message': 'Rate limit exceeded'
        }), 429)
        response.headers['Retry-After'] = str(retry_after)
        return response
    
    @staticmethod
    def server_error(message="Internal server error"):
        """500 - Внутренняя ошибка сервера"""
        return jsonify({
            'success': False,
            'message': message
        }), 500
    
    @staticmethod
    def service_unavailable(retry_after=60):
        """503 - Сервис недоступен"""
        response = make_response(jsonify({
            'success': False,
            'message': 'Service temporarily unavailable'
        }), 503)
        response.headers['Retry-After'] = str(retry_after)
        return response

# Использование в API
@app.route('/api/users', methods=['POST'])
def create_user():
    try:
        data = request.json
        
        # Валидация
        if not data or 'email' not in data:
            return APIResponse.bad_request(
                message="Email is required",
                errors=[{'field': 'email', 'message': 'This field is required'}]
            )
        
        # Проверка существования
        if user_exists(data['email']):
            return APIResponse.conflict("User with this email already exists")
        
        # Создание
        user = create_user_in_db(data)
        return APIResponse.created(
            data=user,
            location=f'/api/users/{user["id"]}'
        )
        
    except ValidationError as e:
        return APIResponse.bad_request(str(e))
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        return APIResponse.server_error()
```

### 🔄 Connection Management и Keep-Alive

**Проблема без Keep-Alive:**

```
Клиент                           Сервер
   │                                │
   │ ───── TCP Connect ────────────→ │ (100ms)
   │ ←──── TCP Connected ────────── │
   │                                │
   │ ───── HTTP Request ──────────→ │ (50ms)
   │ ←──── HTTP Response ────────── │
   │                                │
   │ ───── TCP Close ─────────────→ │ (20ms)
   │ ←──── TCP Closed ──────────── │
   
   Total: 170ms на один запрос
```

**Решение с Keep-Alive:**

```
Клиент                           Сервер
   │                                │
   │ ───── TCP Connect ────────────→ │ (100ms) - только один раз
   │ ←──── TCP Connected ────────── │
   │                                │
   │ ───── HTTP Request #1 ───────→ │ (50ms)
   │ ←──── HTTP Response #1 ─────── │
   │                                │
   │ ───── HTTP Request #2 ───────→ │ (50ms) - без установки соединения
   │ ←──── HTTP Response #2 ─────── │
   │                                │
   │ ───── HTTP Request #3 ───────→ │ (50ms)
   │ ←──── HTTP Response #3 ─────── │
   
   Total: 250ms на три запроса (было бы 510ms без keep-alive)
```

**Конфигурация Keep-Alive в разных серверах:**

```nginx
# Nginx
http {
    keepalive_timeout 65s;           # Время жизни соединения
    keepalive_requests 1000;         # Максимум запросов на соединение
    
    upstream backend {
        server backend1:8080;
        keepalive 32;                # Пул соединений к backend
    }
    
    server {
        location / {
            proxy_pass http://backend;
            proxy_http_version 1.1;
            proxy_set_header Connection "";  # Важно!
        }
    }
}
```

```javascript
// Node.js Express
const express = require('express');
const app = express();

// Настройка HTTP keep-alive
const server = app.listen(3000, () => {
    // Настройки keep-alive для сервера
    server.keepAliveTimeout = 65000;  // 65 секунд
    server.headersTimeout = 66000;    // Больше чем keepAliveTimeout
});

// Настройка keep-alive для исходящих запросов
const http = require('http');
const https = require('https');

const httpAgent = new http.Agent({
    keepAlive: true,
    keepAliveMsecs: 1000,
    maxSockets: 50,
    maxFreeSockets: 10,
    timeout: 60000
});

const httpsAgent = new https.Agent({
    keepAlive: true,
    keepAliveMsecs: 1000,
    maxSockets: 50,
    maxFreeSockets: 10,
    timeout: 60000
});

// Использование в запросах
const axios = require('axios');
const client = axios.create({
    httpAgent: httpAgent,
    httpsAgent: httpsAgent
});
```

### 🎯 Content Negotiation и заголовки

**Продвинутая content negotiation:**

```python
from flask import Flask, request, jsonify
import json
import xml.etree.ElementTree as ET

app = Flask(__name__)

@app.route('/api/users/<int:user_id>')
def get_user(user_id):
    # Получаем данные пользователя
    user_data = {
        'id': user_id,
        'name': 'John Doe',
        'email': 'john@example.com'
    }
    
    # Анализируем Accept заголовок
    accept_header = request.headers.get('Accept', 'application/json')
    
    if 'application/json' in accept_header:
        response = make_response(jsonify(user_data))
        response.headers['Content-Type'] = 'application/json'
        
    elif 'application/xml' in accept_header:
        # Генерируем XML
        root = ET.Element('user')
        for key, value in user_data.items():
            elem = ET.SubElement(root, key)
            elem.text = str(value)
        
        xml_string = ET.tostring(root, encoding='unicode')
        response = make_response(xml_string)
        response.headers['Content-Type'] = 'application/xml'
        
    elif 'text/csv' in accept_header:
        # Генерируем CSV
        csv_data = ','.join(user_data.keys()) + '\n'
        csv_data += ','.join(str(v) for v in user_data.values())
        
        response = make_response(csv_data)
        response.headers['Content-Type'] = 'text/csv'
        response.headers['Content-Disposition'] = f'attachment; filename=user_{user_id}.csv'
        
    else:
        # Неподдерживаемый формат
        return jsonify({
            'error': 'Not Acceptable',
            'supported_formats': ['application/json', 'application/xml', 'text/csv']
        }), 406
    
    # Общие заголовки
    response.headers['Vary'] = 'Accept'  # Важно для кеширования
    response.headers['Cache-Control'] = 'public, max-age=300'
    
    return response

# Обработка сжатия
@app.after_request
def after_request(response):
    # Проверяем поддержку сжатия клиентом
    accept_encoding = request.headers.get('Accept-Encoding', '')
    
    # Сжимаем только текстовые форматы больше 1KB
    if (response.content_length and response.content_length > 1024 and
        response.content_type.startswith(('application/json', 'text/', 'application/xml'))):
        
        if 'gzip' in accept_encoding:
            response.headers['Content-Encoding'] = 'gzip'
            # В реальности используйте middleware для сжатия
        elif 'deflate' in accept_encoding:
            response.headers['Content-Encoding'] = 'deflate'
    
    return response
```

### 📊 Мониторинг HTTP производительности

**Детальный мониторинг HTTP метрик:**

```python
import time
import functools
from collections import defaultdict
from flask import Flask, request, g

app = Flask(__name__)

# Хранилище метрик
http_metrics = {
    'requests_total': defaultdict(int),
    'request_duration': defaultdict(list),
    'response_sizes': defaultdict(list),
    'status_codes': defaultdict(int)
}

def monitor_http_request(f):
    """Декоратор для мониторинга HTTP запросов"""
    @functools.wraps(f)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        
        # Метрики запроса
        method = request.method
        endpoint = request.endpoint or 'unknown'
        user_agent = request.headers.get('User-Agent', 'unknown')
        
        try:
            # Выполняем обработчик
            response = f(*args, **kwargs)
            
            # Метрики ответа
            if hasattr(response, 'status_code'):
                status_code = response.status_code
                content_length = len(response.get_data()) if hasattr(response, 'get_data') else 0
            else:
                status_code = 200
                content_length = len(str(response))
            
            # Сохраняем метрики
            duration = (time.time() - start_time) * 1000  # в миллисекундах
            
            metrics_key = f"{method}_{endpoint}"
            http_metrics['requests_total'][metrics_key] += 1
            http_metrics['request_duration'][metrics_key].append(duration)
            http_metrics['response_sizes'][metrics_key].append(content_length)
            http_metrics['status_codes'][f"{metrics_key}_{status_code}"] += 1
            
            # Добавляем заголовки производительности
            if hasattr(response, 'headers'):
                response.headers['X-Response-Time'] = f"{duration:.2f}ms"
                response.headers['X-Content-Length'] = str(content_length)
            
            return response
            
        except Exception as e:
            # Метрики ошибок
            duration = (time.time() - start_time) * 1000
            metrics_key = f"{method}_{endpoint}"
            http_metrics['requests_total'][f"{metrics_key}_error"] += 1
            http_metrics['request_duration'][metrics_key].append(duration)
            http_metrics['status_codes'][f"{metrics_key}_500"] += 1
            
            raise
    
    return wrapper

# Применяем мониторинг ко всем маршрутам
@app.before_request
def before_request():
    g.start_time = time.time()

@app.after_request
def after_request(response):
    if hasattr(g, 'start_time'):
        duration = (time.time() - g.start_time) * 1000
        response.headers['X-Response-Time'] = f"{duration:.2f}ms"
    return response

# Endpoint для метрик (Prometheus формат)
@app.route('/metrics')
def metrics():
    """Endpoint для экспорта метрик в Prometheus формате"""
    import statistics
    
    metrics_output = []
    
    # Общее количество запросов
    metrics_output.append("# HELP http_requests_total Total HTTP requests")
    metrics_output.append("# TYPE http_requests_total counter")
    
    for key, value in http_metrics['requests_total'].items():
        method, endpoint = key.split('_', 1)
        metrics_output.append(f'http_requests_total{{method="{method}",endpoint="{endpoint}"}} {value}')
    
    # Время ответа
    metrics_output.append("\n# HELP http_request_duration_ms HTTP request duration in milliseconds")
    metrics_output.append("# TYPE http_request_duration_ms histogram")
    
    for key, durations in http_metrics['request_duration'].items():
        if durations:
            method, endpoint = key.split('_', 1)
            avg_duration = statistics.mean(durations)
            p95_duration = statistics.quantiles(durations, n=20)[18] if len(durations) > 1 else durations[0]
            
            metrics_output.append(f'http_request_duration_ms_avg{{method="{method}",endpoint="{endpoint}"}} {avg_duration:.2f}')
            metrics_output.append(f'http_request_duration_ms_p95{{method="{method}",endpoint="{endpoint}"}} {p95_duration:.2f}')
    
    # Статус коды
    metrics_output.append("\n# HELP http_responses_total Total HTTP responses by status code")
    metrics_output.append("# TYPE http_responses_total counter")
    
    for key, value in http_metrics['status_codes'].items():
        parts = key.rsplit('_', 1)
        if len(parts) == 2:
            method_endpoint, status_code = parts
            method, endpoint = method_endpoint.split('_', 1)
            metrics_output.append(f'http_responses_total{{method="{method}",endpoint="{endpoint}",status_code="{status_code}"}} {value}')
    
    return '\n'.join(metrics_output), 200, {'Content-Type': 'text/plain'}

# Пример использования
@app.route('/api/users/<int:user_id>')
@monitor_http_request
def get_user(user_id):
    # Симуляция работы
    time.sleep(0.05)  # 50ms
    return jsonify({'id': user_id, 'name': 'John Doe'})
```

### 🔧 Практический пример: HTTP Pipelining

**Проблемы HTTP/1.1 pipelining:**

```python
import asyncio
import aiohttp
import time

async def sequential_requests():
    """Последовательные запросы - медленно"""
    start_time = time.time()
    
    async with aiohttp.ClientSession() as session:
        urls = [f'https://api.example.com/users/{i}' for i in range(1, 11)]
        
        responses = []
        for url in urls:
            async with session.get(url) as response:
                data = await response.json()
                responses.append(data)
    
    end_time = time.time()
    print(f"Sequential: {end_time - start_time:.2f}s for 10 requests")
    return responses

async def pipelined_requests():
    """Конкурентные запросы - быстрее"""
    start_time = time.time()
    
    async with aiohttp.ClientSession() as session:
        urls = [f'https://api.example.com/users/{i}' for i in range(1, 11)]
        
        # Отправляем все запросы сразу
        tasks = [session.get(url) for url in urls]
        responses = await asyncio.gather(*tasks)
        
        # Читаем все ответы
        data = []
        for response in responses:
            json_data = await response.json()
            data.append(json_data)
            response.close()
    
    end_time = time.time()
    print(f"Pipelined: {end_time - start_time:.2f}s for 10 requests")
    return data

# Запуск тестов
async def main():
    print("Testing HTTP request patterns:")
    await sequential_requests()  # ~5-10 секунд
    await pipelined_requests()   # ~0.5-1 секунда
    
    # Результат показывает преимущество параллельных запросов
    # даже в HTTP/1.1

if __name__ == "__main__":
    asyncio.run(main())
```

### 📝 Практическое задание Неделя 5

1. Создайте RESTful API с правильным использованием HTTP методов и статус кодов
2. Настройте HTTP keep-alive для своего веб-сервера
3. Реализуйте content negotiation для разных форматов (JSON, XML, CSV)
4. Добавьте мониторинг HTTP метрик в Prometheus формате
5. Проведите нагрузочное тестирование с/без keep-alive

---

## Неделя 6: HTTP/2 и современные возможности

### 🧠 Концепция: Решение проблем HTTP/1.1

HTTP/2 был создан для решения fundamental проблем HTTP/1.1:
- **Head-of-line blocking** - блокировка очереди
- **Множественные соединения** - браузеры открывают 6-8 соединений
- **Неэффективные заголовки** - повторение одинаковых заголовков

### 📊 HTTP/1.1 vs HTTP/2 Architecture

```
HTTP/1.1 (проблемы):
┌─────────────────────────────────────────────────────────────┐
│ Соединение 1: [Req1] ──→ [Resp1] ──→ [Req2] ──→ [Resp2]   │
│ Соединение 2: [Req3] ──→ [Resp3] ──→ [Req4] ──→ [Resp4]   │
│ Соединение 3: [Req5] ──→ [Resp5] ──→ [Req6] ──→ [Resp6]   │
├─────────────────────────────────────────────────────────────┤
│ ❌ Head-of-line blocking: Req2 ждет Resp1                   │
│ ❌ Множество соединений: overhead TCP handshake            │
│ ❌ Повторяющиеся заголовки: User-Agent в каждом запросе    │
└─────────────────────────────────────────────────────────────┘

HTTP/2 (решения):
┌─────────────────────────────────────────────────────────────┐
│ Одно соединение:                                            │
│ Stream 1: [Req1] ═══════════════════════════ [Resp1]       │
│ Stream 3: [Req3] ═════ [Resp3]                             │
│ Stream 5: [Req5] ═══════════ [Resp5]                       │
│ Stream 7: [Req7] ═ [Resp7]                                 │
├─────────────────────────────────────────────────────────────┤
│ ✅ Multiplexing: все запросы параллельно                    │
│ ✅ Одно TCP соединение: меньше overhead                     │
│ ✅ HPACK сжатие: заголовки сжимаются                       │
│ ✅ Server Push: сервер может отправлять ресурсы заранее    │
└─────────────────────────────────────────────────────────────┘
```

### 🔧 HTTP/2 Binary Framing Layer

**Как работает binary framing:**

```
HTTP/1.1 (текстовый):
GET /api/users HTTP/1.1\r\n
Host: api.example.com\r\n
User-Agent: MyApp/1.0\r\n
\r\n

HTTP/2 (бинарный):
┌─────────────────────────────────────────────┐
│ Frame Type: HEADERS (0x1)                   │
│ Stream ID: 1                                │
│ Flags: END_HEADERS (0x4)                    │
│ Length: 42                                  │
├─────────────────────────────────────────────┤
│ Compressed Headers (HPACK):                 │
│ :method: GET                                │
│ :path: /api/users                          │
│ :authority: api.example.com                │
│ user-agent: MyApp/1.0                      │
└─────────────────────────────────────────────┘
```

### ⚡ Практический пример: HTTP/2 Multiplexing

**Сравнение производительности:**

```javascript
// Node.js HTTP/2 сервер
const http2 = require('http2');
const fs = require('fs');

// Создаем HTTP/2 сервер
const server = http2.createSecureServer({
    key: fs.readFileSync('private-key.pem'),
    cert: fs.readFileSync('certificate.pem')
});

server.on('stream', (stream, headers) => {
    const path = headers[':path'];
    const method = headers[':method'];
    
    console.log(`HTTP/2 ${method} ${path} - Stream ID: ${stream.id}`);
    
    // Обработка разных путей
    if (path === '/api/users') {
        // Симуляция медленного запроса
        setTimeout(() => {
            stream.respond({
                'content-type': 'application/json',
                ':status': 200
            });
            stream.end(JSON.stringify({
                users: Array.from({length: 100}, (_, i) => ({
                    id: i + 1,
                    name: `User ${i + 1}`
                }))
            }));
        }, 2000); // 2 секунды задержка
        
    } else if (path === '/api/quick') {
        // Быстрый запрос
        stream.respond({
            'content-type': 'application/json',
            ':status': 200
        });
        stream.end(JSON.stringify({ message: 'Quick response' }));
        
    } else {
        stream.respond({ ':status': 404 });
        stream.end('Not found');
    }
});

server.listen(3000, () => {
    console.log('HTTP/2 server running on https://localhost:3000');
});
```

**Тестирование multiplexing:**

```bash
# Установка nghttp2-client для тестирования
# Ubuntu: sudo apt-get install nghttp2-client
# macOS: brew install nghttp2

# HTTP/2 тест с несколькими запросами
nghttp -v https://localhost:3000/api/users https://localhost:3000/api/quick https://localhost:3000/api/users

# Результат показывает:
# Stream 1: /api/users   (2000ms) - но не блокирует другие
# Stream 3: /api/quick   (50ms)   - завершается быстро
# Stream 5: /api/users   (2000ms) - идет параллельно
# 
# Общее время: ~2050ms вместо 4050ms в HTTP/1.1
```

### 🗜️ HPACK Header Compression

**Как HPACK экономит bandwidth:**

```python
# Демонстрация экономии заголовков
import json

# Типичные заголовки API запроса
http1_headers = {
    'GET /api/users/1 HTTP/1.1': '',
    'Host': 'api.example.com',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'application/json',
    'Accept-Encoding': 'gzip, deflate, br',
    'Accept-Language': 'en-US,en;q=0.9',
    'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
    'Pragma': 'no-cache'
}

# Подсчет размера в HTTP/1.1
http1_size = sum(len(f"{k}: {v}\r\n") for k, v in http1_headers.items())
http1_size += len("GET /api/users/1 HTTP/1.1\r\n\r\n")

print(f"HTTP/1.1 headers size: {http1_size} bytes")

# В HTTP/2 с HPACK (симуляция)
# Первый запрос - полные заголовки (но сжатые)
first_request_size = http1_size * 0.7  # HPACK сжимает ~30%

# Последующие запросы - только изменения
subsequent_requests = [
    {'path': '/api/users/2'},  # Только path изменился
    {'path': '/api/users/3'},  # Только path изменился
    {'path': '/api/orders/1'}, # Только path изменился
]

subsequent_size = 20  # Только новый path + служебная информация

total_http1 = http1_size * 4  # 4 запроса
total_http2 = first_request_size + (subsequent_size * 3)

print(f"HTTP/1.1 total: {total_http1} bytes")
print(f"HTTP/2 total: {total_http2:.0f} bytes")
print(f"Savings: {((total_http1 - total_http2) / total_http1 * 100):.1f}%")

# Результат:
# HTTP/1.1 headers size: 456 bytes
# HTTP/1.1 total: 1824 bytes  
# HTTP/2 total: 379 bytes
# Savings: 79.2%
```

### 🚀 Server Push - проактивная отправка ресурсов

**Когда Server Push полезен:**

```javascript
// HTTP/2 Server Push пример
const http2 = require('http2');
const fs = require('fs');
const path = require('path');

const server = http2.createSecureServer({
    key: fs.readFileSync('private-key.pem'),
    cert: fs.readFileSync('certificate.pem')
});

server.on('stream', (stream, headers) => {
    const reqPath = headers[':path'];
    
    if (reqPath === '/api/user/profile') {
        // При запросе профиля, push связанные ресурсы
        
        // Push user preferences (понадобится на фронтенде)
        const pushStream1 = stream.pushStream({
            ':method': 'GET',
            ':path': '/api/user/preferences',
            ':scheme': 'https',
            ':authority': headers[':authority']
        });
        
        pushStream1.respond({ ':status': 200, 'content-type': 'application/json' });
        pushStream1.end(JSON.stringify({
            theme: 'dark',
            language: 'en',
            notifications: true
        }));
        
        // Push user notifications (понадобится тоже)
        const pushStream2 = stream.pushStream({
            ':method': 'GET',
            ':path': '/api/user/notifications',
            ':scheme': 'https',
            ':authority': headers[':authority']
        });
        
        pushStream2.respond({ ':status': 200, 'content-type': 'application/json' });
        pushStream2.end(JSON.stringify({
            unread_count: 3,
            latest: ['Message 1', 'Message 2', 'Message 3']
        }));
        
        // Основной ответ
        stream.respond({
            ':status': 200,
            'content-type': 'application/json',
            'link': '</api/user/preferences>; rel=preload, </api/user/notifications>; rel=preload'
        });
        
        stream.end(JSON.stringify({
            id: 123,
            name: 'John Doe',
            email: 'john@example.com'
        }));
    }
});

server.listen(3000);
```

**Осторожно с Server Push:**

```javascript
// Мониторинг эффективности Server Push
const pushMetrics = {
    pushed: 0,
    used: 0,
    wasted: 0
};

server.on('stream', (stream, headers) => {
    // Отслеживаем pushed ресурсы
    stream.on('push', (pushStream) => {
        pushMetrics.pushed++;
        
        pushStream.on('close', () => {
            if (pushStream.rstCode === http2.constants.NGHTTP2_NO_ERROR) {
                pushMetrics.used++;
            } else if (pushStream.rstCode === http2.constants.NGHTTP2_CANCEL) {
                pushMetrics.wasted++;
                console.log('Client cancelled push - resource not needed');
            }
        });
    });
});

// Endpoint для метрик
server.on('stream', (stream, headers) => {
    if (headers[':path'] === '/metrics/push') {
        const efficiency = (pushMetrics.used / pushMetrics.pushed * 100).toFixed(1);
        const waste = (pushMetrics.wasted / pushMetrics.pushed * 100).toFixed(1);
        
        stream.respond({ ':status': 200, 'content-type': 'application/json' });
        stream.end(JSON.stringify({
            pushed_total: pushMetrics.pushed,
            used_total: pushMetrics.used,
            wasted_total: pushMetrics.wasted,
            efficiency_percent: efficiency,
            waste_percent: waste
        }));
    }
});
```

### 🔧 Настройка HTTP/2 в production

**Nginx с HTTP/2:**

```nginx
# /etc/nginx/sites-available/api
server {
    listen 443 ssl http2;  # HTTP/2 включен
    listen [::]:443 ssl http2;
    
    server_name api.example.com;
    
    # SSL настройки для HTTP/2
    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE+AESGCM:ECDHE+CHACHA20:DHE+AESGCM:DHE+CHACHA20:!aNULL:!SHA1:!WEAK;
    ssl_prefer_server_ciphers off;
    
    # HTTP/2 оптимизации
    http2_max_field_size 8k;        # Размер заголовков
    http2_max_header_size 32k;      # Общий размер заголовков
    http2_body_preread_size 64k;    # Preread body для лучшей производительности
    
    # Server Push для статических ресурсов
    location = /app.js {
        add_header Link "</style.css>; rel=preload; as=style" always;
        add_header Link "</config.json>; rel=preload; as=fetch" always;
    }
    
    # API endpoints
    location /api/ {
        proxy_pass http://backend;
        proxy_http_version 1.1;  # Backend может быть HTTP/1.1
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection '';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Буферизация для лучшей производительности HTTP/2
        proxy_buffering on;
        proxy_buffer_size 128k;
        proxy_buffers 4 256k;
        proxy_busy_buffers_size 256k;
    }
}

# Редирект HTTP -> HTTPS
server {
    listen 80;
    listen [::]:80;
    server_name api.example.com;
    return 301 https://$server_name$request_uri;
}
```

**Мониторинг HTTP/2 метрик:**

```bash
# Проверка HTTP/2 соединений в nginx
tail -f /var/log/nginx/access.log | grep "HTTP/2"

# Детальные метрики HTTP/2 в nginx (требует модуль)
curl -s localhost/nginx_status | grep http2

# Мониторинг производительности
# /etc/nginx/nginx.conf
http {
    log_format http2_log '$remote_addr - $remote_user [$time_local] '
                         '"$request" $status $body_bytes_sent '
                         '"$http_referer" "$http_user_agent" '
                         'rt=$request_time '
                         'ua="$upstream_addr" us="$upstream_status" '
                         'ut="$upstream_response_time" ul="$upstream_response_length" '
                         'h2="$http2"';  # HTTP/2 flag
                         
    access_log /var/log/nginx/http2.log http2_log;
}
```

### 📊 Производительность HTTP/2 на практике

**Benchmark HTTP/1.1 vs HTTP/2:**

```python
import asyncio
import aiohttp
import time
from statistics import mean, median

async def benchmark_http_version(urls, http_version='1.1'):
    """Бенчмарк HTTP/1.1 vs HTTP/2"""
    
    # Настройка коннектора
    if http_version == '2.0':
        # Примечание: aiohttp пока не поддерживает HTTP/2 клиент
        # Используем curl для HTTP/2 тестирования
        return await benchmark_with_curl(urls, '--http2')
    else:
        return await benchmark_with_aiohttp(urls)

async def benchmark_with_aiohttp(urls):
    """Тест с aiohttp (HTTP/1.1)"""
    start_time = time.time()
    times = []
    
    connector = aiohttp.TCPConnector(
        limit=10,  # Максимум соединений
        limit_per_host=6  # Как в браузере для HTTP/1.1
    )
    
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = []
        for url in urls:
            task = fetch_with_timing(session, url)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks)
        times = [r['time'] for r in results if r['success']]
    
    total_time = time.time() - start_time
    
    return {
        'version': 'HTTP/1.1',
        'total_time': total_time,
        'avg_time': mean(times) if times else 0,
        'median_time': median(times) if times else 0,
        'success_count': len(times),
        'total_requests': len(urls)
    }

async def fetch_with_timing(session, url):
    """Fetch с измерением времени"""
    start = time.time()
    try:
        async with session.get(url) as response:
            await response.text()
            return {
                'success': True,
                'time': (time.time() - start) * 1000,  # в мс
                'status': response.status
            }
    except Exception as e:
        return {
            'success': False,
            'time': (time.time() - start) * 1000,
            'error': str(e)
        }

async def benchmark_with_curl(urls, http_flag):
    """Тест с curl (поддерживает HTTP/2)"""
    import subprocess
    import json
    
    start_time = time.time()
    times = []
    
    # Параллельные curl запросы
    processes = []
    for url in urls:
        cmd = [
            'curl', http_flag, '-s', '-o', '/dev/null',
            '-w', '%{time_total},%{http_code}',
            url
        ]
        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        processes.append((proc, url))
    
    # Ждем завершения всех
    for proc, url in processes:
        stdout, stderr = proc.communicate()
        if proc.returncode == 0:
            try:
                time_total, status_code = stdout.decode().strip().split(',')
                times.append(float(time_total) * 1000)  # в мс
            except ValueError:
                pass
    
    total_time = time.time() - start_time
    
    return {
        'version': 'HTTP/2' if '--http2' in http_flag else 'HTTP/1.1',
        'total_time': total_time,
        'avg_time': mean(times) if times else 0,
        'median_time': median(times) if times else 0,
        'success_count': len(times),
        'total_requests': len(urls)
    }

# Тестирование
async def main():
    # Тестовые URL (замените на ваши)
    test_urls = [
        'https://api.example.com/users',
        'https://api.example.com/orders',
        'https://api.example.com/products',
        'https://api.example.com/analytics',
        'https://api.example.com/reports'
    ] * 10  # 50 запросов всего
    
    print("Benchmarking HTTP versions...")
    
    # HTTP/1.1 тест
    http1_result = await benchmark_with_aiohttp(test_urls)
    print(f"HTTP/1.1: {http1_result}")
    
    # HTTP/2 тест
    http2_result = await benchmark_with_curl(test_urls, '--http2')
    print(f"HTTP/2: {http2_result}")
    
    # Сравнение
    if http1_result['avg_time'] > 0 and http2_result['avg_time'] > 0:
        improvement = (http1_result['avg_time'] - http2_result['avg_time']) / http1_result['avg_time'] * 100
        print(f"HTTP/2 faster by: {improvement:.1f}%")

if __name__ == "__main__":
    asyncio.run(main())
```

### 📝 Практическое задание Неделя 6

1. Настройте HTTP/2 на вашем веб-сервере
2. Реализуйте HTTP/2 server push для критических ресурсов API
3. Проведите нагрузочное тестирование HTTP/1.1 vs HTTP/2
4. Настройте мониторинг эффективности Server Push
5. Оптимизируйте заголовки для лучшего HPACK сжатия

---

## Неделя 7: HTTP/3 и HTTPS

### 🧠 Концепция: Революция HTTP/3 с QUIC

HTTP/3 - это не просто улучшение HTTP/2. Это полная замена транспортного уровня с TCP на QUIC (UDP-based).

### 📊 Эволюция HTTP протоколов

```
HTTP/1.1 (1997):        HTTP/2 (2015):         HTTP/3 (2022):
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│ Application     │    │ Application     │    │ Application     │
│ HTTP/1.1        │    │ HTTP/2          │    │ HTTP/3          │
├─────────────────┤    ├─────────────────┤    ├─────────────────┤
│ TLS (optional)  │    │ TLS 1.2+        │    │ TLS 1.3         │
├─────────────────┤    ├─────────────────┤    │ (integrated)    │
│ TCP             │    │ TCP             │    ├─────────────────┤
├─────────────────┤    ├─────────────────┤    │ QUIC            │
│ IP              │    │ IP              │    ├─────────────────┤
└─────────────────┘    └─────────────────┘    │ UDP             │
                                              ├─────────────────┤
Проблемы:              Решения HTTP/2:        │ IP              │
• Head-of-line         • Multiplexing         └─────────────────┘
  blocking             • Binary framing       
• Множество            • Header compression   Решения HTTP/3:
  соединений           • Server push          • Нет TCP blocking
• Повторы заголовков   • Одно соединение     • 0-RTT handshake
                                              • Connection migration
                                              • Better congestion control
```

### 🚀 QUIC преимущества над TCP

**1. Устранение Head-of-Line Blocking:**

```
TCP (проблема):
Stream 1: [████████████████] ← блокирует все остальные
Stream 2: [    ждет...     ]
Stream 3: [    ждет...     ]

QUIC (решение):
Stream 1: [████████████████]
Stream 2: [██████████      ] ← работает независимо
Stream 3: [████████████████] ← завершился первым
```

**2. 0-RTT Connection Establishment:**

```bash
# Тестирование 0-RTT подключения
curl --http3 -w "@curl-timing.txt" https://cloudflare.com

# curl-timing.txt:
#     time_namelookup:  %{time_namelookup}
#        time_connect:  %{time_connect}     ← для QUIC может быть 0!
#     time_appconnect:  %{time_appconnect}
#    time_pretransfer:  %{time_pretransfer}
#   time_starttransfer: %{time_starttransfer}
#            time_total: %{time_total}

# Результат для QUIC 0-RTT:
# time_connect: 0.000
# time_appconnect: 0.000  ← подключение мгновенное!
```

### 🔧 Практическая настройка HTTP/3

**Caddy сервер (нативная поддержка HTTP/3):**

```caddyfile
# Caddyfile
{
    # Глобальные настройки HTTP/3
    experimental_http3
    log {
        level DEBUG
    }
}

api.example.com {
    # HTTP/3 включен автоматически
    reverse_proxy localhost:8080 {
        # Настройки для backend
        header_up Host {http.request.host}
        header_up X-Real-IP {http.request.remote}
        header_up X-Forwarded-For {http.request.remote}
        header_up X-Forwarded-Proto {http.request.scheme}
        
        # Health check
        health_uri /health
        health_interval 30s
        health_timeout 5s
    }
    
    # Принудительный HTTP/3 для современных клиентов
    @supports_h3 {
        header Alt-Svc h3=":443"
    }
    
    # Логирование с версией протокола
    log {
        output file /var/log/caddy/api.log {
            roll_size 100mb
            roll_keep 10
        }
        format json {
            time_format "2006-01-02T15:04:05.000Z07:00"
            level_format "upper"
        }
        level INFO
    }
    
    # CORS для HTTP/3
    header {
        Access-Control-Allow-Origin *
        Access-Control-Allow-Methods "GET, POST, PUT, DELETE, OPTIONS"
        Access-Control-Allow-Headers "Content-Type, Authorization"
        # Важно для HTTP/3 preflight
        Access-Control-Max-Age 86400
    }
}
```

**Node.js HTTP/3 сервер (экспериментальная поддержка):**

```javascript
// Требует Node.js 15+ с флагом --experimental-quic
const { createQuicSocket } = require('quic');
const fs = require('fs');

// Создание QUIC сокета
const socket = createQuicSocket({
    endpoint: { port: 1234 },
    server: {
        key: fs.readFileSync('server-key.pem'),
        cert: fs.readFileSync('server-cert.pem'),
        ca: fs.readFileSync('ca-cert.pem'),  // опционально
        requestCert: false,
        rejectUnauthorized: false,
        alpn: 'h3'  // HTTP/3 ALPN identifier
    }
});

socket.on('session', (session) => {
    console.log('New QUIC session established');
    
    // Мониторинг состояния сессии
    session.on('secure', () => {
        console.log('Session is secure');
        console.log('Cipher:', session.cipher);
        console.log('ALPN:', session.alpnProtocol);
    });
    
    session.on('stream', (stream) => {
        console.log('New stream created:', stream.id);
        
        let requestData = '';
        
        stream.on('data', (chunk) => {
            requestData += chunk.toString();
        });
        
        stream.on('end', () => {
            // Простой HTTP/3 ответ
            const response = JSON.stringify({
                message: 'Hello from HTTP/3!',
                timestamp: new Date().toISOString(),
                stream_id: stream.id,
                protocol: 'HTTP/3'
            });
            
            // HTTP/3 headers
            const headers = [
                [':status', '200'],
                ['content-type', 'application/json'],
                ['content-length', response.length.toString()],
                ['server', 'Node.js QUIC'],
                ['date', new Date().toUTCString()]
            ];
            
            // Отправка headers
            stream.sendHeaders(headers);
            
            // Отправка body
            stream.end(response);
        });
        
        stream.on('error', (err) => {
            console.error('Stream error:', err);
        });
    });
    
    session.on('close', () => {
        console.log('Session closed');
    });
});

socket.on('error', (err) => {
    console.error('Socket error:', err);
});

socket.listen()
    .then(() => console.log('QUIC server listening on port 1234'))
    .catch(console.error);

// Graceful shutdown
process.on('SIGINT', () => {
    console.log('Shutting down...');
    socket.close(() => {
        process.exit(0);
    });
});
```

### 🔐 TLS 1.3 интеграция в QUIC

**Особенности TLS в QUIC:**

```python
# Демонстрация TLS 1.3 handshake в QUIC
import ssl
import socket
import time

def analyze_tls_handshake(hostname, port=443):
    """Анализ TLS handshake для HTTP/3"""
    
    # Создание контекста для TLS 1.3
    context = ssl.create_default_context()
    context.minimum_version = ssl.TLSVersion.TLSv1_3
    context.maximum_version = ssl.TLSVersion.TLSv1_3
    
    # ALPN для HTTP/3
    context.set_alpn_protocols(['h3', 'h3-29', 'h3-28'])
    
    start_time = time.time()
    
    try:
        # Создание соединения
        sock = socket.create_connection((hostname, port), timeout=10)
        
        # TLS wrap
        tls_sock = context.wrap_socket(sock, server_hostname=hostname)
        
        end_time = time.time()
        
        # Информация о соединении
        info = {
            'hostname': hostname,
            'handshake_time': (end_time - start_time) * 1000,  # мс
            'tls_version': tls_sock.version(),
            'cipher': tls_sock.cipher(),
            'alpn_protocol': tls_sock.selected_alpn_protocol(),
            'compression': tls_sock.compression(),
            'certificate': {
                'subject': dict(x[0] for x in tls_sock.getpeercert()['subject']),
                'issuer': dict(x[0] for x in tls_sock.getpeercert()['issuer']),
                'version': tls_sock.getpeercert()['version'],
                'serial_number': tls_sock.getpeercert()['serialNumber']
            }
        }
        
        tls_sock.close()
        
        return info
        
    except Exception as e:
        return {'error': str(e), 'handshake_time': (time.time() - start_time) * 1000}

# Сравнение разных сайтов
sites = [
    'cloudflare.com',  # Поддерживает HTTP/3
    'google.com',      # Поддерживает HTTP/3
    'facebook.com',    # Поддерживает HTTP/3
    'github.com'       # HTTP/2 only
]

print("TLS 1.3 / HTTP/3 Support Analysis:")
print("=" * 50)

for site in sites:
    result = analyze_tls_handshake(site)
    
    if 'error' not in result:
        print(f"\n{site}:")
        print(f"  TLS Version: {result['tls_version']}")
        print(f"  Handshake Time: {result['handshake_time']:.1f}ms")
        print(f"  Cipher: {result['cipher'][0]} (strength: {result['cipher'][2]})")
        print(f"  ALPN: {result['alpn_protocol']}")
        print(f"  HTTP/3 Support: {'Yes' if result['alpn_protocol'] and 'h3' in result['alpn_protocol'] else 'No'}")
    else:
        print(f"\n{site}: ERROR - {result['error']}")
```

### 📊 Connection Migration в QUIC

**Почему это революционно для мобильных API:**

```javascript
// Симуляция connection migration
const quicMetrics = {
    connections: new Map(),
    migrations: 0,
    migrationTime: []
};

// Мониторинг QUIC connection events
socket.on('session', (session) => {
    const connectionId = session.id;
    
    quicMetrics.connections.set(connectionId, {
        created: Date.now(),
        ip_changes: 0,
        last_ip: session.remoteAddress,
        active: true
    });
    
    // Обработка смены IP (connection migration)
    session.on('pathValidation', (pathData) => {
        const conn = quicMetrics.connections.get(connectionId);
        
        if (conn && pathData.remoteAddress !== conn.last_ip) {
            const migrationStart = Date.now();
            
            console.log(`Connection migration detected:`);
            console.log(`  From: ${conn.last_ip}`);
            console.log(`  To: ${pathData.remoteAddress}`);
            
            // Обновляем метрики
            conn.ip_changes++;
            conn.last_ip = pathData.remoteAddress;
            quicMetrics.migrations++;
            
            // Время восстановления соединения
            const migrationTime = Date.now() - migrationStart;
            quicMetrics.migrationTime.push(migrationTime);
            
            console.log(`  Migration completed in: ${migrationTime}ms`);
            
            // В TCP это было бы разрыв соединения!
            // В QUIC - бесшовное переключение
        }
    });
    
    session.on('close', () => {
        const conn = quicMetrics.connections.get(connectionId);
        if (conn) {
            conn.active = false;
            conn.duration = Date.now() - conn.created;
        }
    });
});

// API для мониторинга connection migration
app.get('/api/quic/metrics', (req, res) => {
    const activeConnections = Array.from(quicMetrics.connections.values())
        .filter(conn => conn.active).length;
    
    const avgMigrationTime = quicMetrics.migrationTime.length > 0 
        ? quicMetrics.migrationTime.reduce((a, b) => a + b, 0) / quicMetrics.migrationTime.length
        : 0;
    
    res.json({
        active_connections: activeConnections,
        total_migrations: quicMetrics.migrations,
        avg_migration_time_ms: avgMigrationTime.toFixed(2),
        migration_success_rate: 100,  // QUIC migrations почти всегда успешны
        tcp_equivalent_failures: quicMetrics.migrations  // В TCP это были бы разрывы
    });
});
```

### 🛡️ Security Headers для HTTP/3

**Усиленные security headers:**

```python
from flask import Flask, make_response, request
import secrets

app = Flask(__name__)

@app.after_request
def add_security_headers(response):
    """Добавление security headers для HTTP/3"""
    
    # Strict Transport Security с preload
    response.headers['Strict-Transport-Security'] = (
        'max-age=63072000; includeSubDomains; preload'
    )
    
    # Content Security Policy
    nonce = secrets.token_urlsafe(16)
    response.headers['Content-Security-Policy'] = (
        f"default-src 'self'; "
        f"script-src 'self' 'nonce-{nonce}' 'strict-dynamic'; "
        f"style-src 'self' 'unsafe-inline'; "
        f"img-src 'self' data: https:; "
        f"font-src 'self' data:; "
        f"connect-src 'self' wss: https:; "
        f"frame-ancestors 'none'; "
        f"base-uri 'self'; "
        f"form-action 'self'"
    )
    
    # Предотвращение MIME sniffing
    response.headers['X-Content-Type-Options'] = 'nosniff'
    
    # Защита от clickjacking
    response.headers['X-Frame-Options'] = 'DENY'
    
    # XSS Protection
    response.headers['X-XSS-Protection'] = '1; mode=block'
    
    # Referrer Policy
    response.headers['Referrer-Policy'] = 'strict-origin-when-cross-origin'
    
    # Feature Policy / Permissions Policy
    response.headers['Permissions-Policy'] = (
        'geolocation=(), microphone=(), camera=(), '
        'payment=(), usb=(), magnetometer=(), gyroscope=()'
    )
    
    # Cross-Origin Embedder Policy (важно для HTTP/3)
    response.headers['Cross-Origin-Embedder-Policy'] = 'require-corp'
    
    # Cross-Origin Opener Policy
    response.headers['Cross-Origin-Opener-Policy'] = 'same-origin'
    
    # Cross-Origin Resource Policy
    response.headers['Cross-Origin-Resource-Policy'] = 'same-origin'
    
    # Cache control для API
    if request.path.startswith('/api/'):
        response.headers['Cache-Control'] = 'no-store, no-cache, must-revalidate'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '0'
    
    # Alt-Svc header для HTTP/3 upgrade
    response.headers['Alt-Svc'] = 'h3=":443"; ma=86400, h3-29=":443"; ma=86400'
    
    return response

# Endpoint для проверки security headers
@app.route('/api/security/check')
def security_check():
    """Проверка security headers"""
    headers_to_check = [
        'Strict-Transport-Security',
        'Content-Security-Policy',
        'X-Content-Type-Options',
        'X-Frame-Options',
        'X-XSS-Protection',
        'Referrer-Policy',
        'Permissions-Policy',
        'Alt-Svc'
    ]
    
    present_headers = {}
    for header in headers_to_check:
        if header in request.headers:
            present_headers[header] = request.headers[header]
    
    return {
        'security_headers_present': len(present_headers),
        'security_headers_total': len(headers_to_check),
        'security_score': len(present_headers) / len(headers_to_check) * 100,
        'headers': present_headers,
        'recommendations': [
            'Enable HSTS with preload',
            'Implement strict CSP',
            'Use HTTP/3 Alt-Svc header',
            'Set proper CORS policies',
            'Enable security monitoring'
        ]
    }
```

### 📈 Performance Monitoring HTTP/3

**Comprehensive HTTP/3 metrics:**

```javascript
// HTTP/3 Performance Monitor
class HTTP3Monitor {
    constructor() {
        this.metrics = {
            connections: new Map(),
            streams: new Map(),
            performance: {
                handshake_times: [],
                first_byte_times: [],
                transfer_rates: [],
                error_rates: new Map()
            }
        };
        
        this.startTime = Date.now();
    }
    
    recordConnection(sessionId, details) {
        this.metrics.connections.set(sessionId, {
            id: sessionId,
            started: Date.now(),
            handshake_time: details.handshakeTime,
            cipher: details.cipher,
            alpn: details.alpn,
            streams: 0,
            bytes_transferred: 0,
            errors: 0
        });
        
        this.metrics.performance.handshake_times.push(details.handshakeTime);
    }
    
    recordStream(sessionId, streamId, details) {
        const connection = this.metrics.connections.get(sessionId);
        if (connection) {
            connection.streams++;
            connection.bytes_transferred += details.bytes || 0;
        }
        
        this.metrics.streams.set(streamId, {
            session_id: sessionId,
            started: details.startTime,
            ended: details.endTime,
            bytes: details.bytes,
            status: details.status,
            first_byte_time: details.firstByteTime
        });
        
        if (details.firstByteTime) {
            this.metrics.performance.first_byte_times.push(details.firstByteTime);
        }
        
        if (details.bytes && details.duration) {
            const rate = details.bytes / details.duration * 8; // bits per second
            this.metrics.performance.transfer_rates.push(rate);
        }
    }
    
    recordError(sessionId, errorType, errorDetails) {
        const connection = this.metrics.connections.get(sessionId);
        if (connection) {
            connection.errors++;
        }
        
        if (!this.metrics.performance.error_rates.has(errorType)) {
            this.metrics.performance.error_rates.set(errorType, 0);
        }
        this.metrics.performance.error_rates.set(
            errorType, 
            this.metrics.performance.error_rates.get(errorType) + 1
        );
    }
    
    getStatistics() {
        const now = Date.now();
        const uptime = now - this.startTime;
        
        const activeConnections = Array.from(this.metrics.connections.values())
            .filter(conn => !conn.ended).length;
        
        const totalStreams = this.metrics.streams.size;
        const totalErrors = Array.from(this.metrics.performance.error_rates.values())
            .reduce((sum, count) => sum + count, 0);
        
        // Статистика производительности
        const avgHandshakeTime = this.average(this.metrics.performance.handshake_times);
        const avgFirstByteTime = this.average(this.metrics.performance.first_byte_times);
        const avgTransferRate = this.average(this.metrics.performance.transfer_rates);
        
        return {
            uptime_ms: uptime,
            connections: {
                active: activeConnections,
                total: this.metrics.connections.size
            },
            streams: {
                total: totalStreams,
                errors: totalErrors,
                success_rate: ((totalStreams - totalErrors) / totalStreams * 100).toFixed(2)
            },
            performance: {
                avg_handshake_time_ms: avgHandshakeTime.toFixed(2),
                avg_first_byte_time_ms: avgFirstByteTime.toFixed(2),
                avg_transfer_rate_mbps: (avgTransferRate / 1024 / 1024).toFixed(2),
                error_breakdown: Object.fromEntries(this.metrics.performance.error_rates)
            },
            http3_advantages: {
                zero_rtt_connections: this.metrics.performance.handshake_times.filter(t => t < 1).length,
                connection_migrations: this.getConnectionMigrations(),
                multiplexing_efficiency: this.getMultiplexingEfficiency()
            }
        };
    }
    
    average(array) {
        return array.length > 0 ? array.reduce((a, b) => a + b, 0) / array.length : 0;
    }
    
    getConnectionMigrations() {
        // Подсчет успешных connection migration событий
        return Array.from(this.metrics.connections.values())
            .reduce((sum, conn) => sum + (conn.migrations || 0), 0);
    }
    
    getMultiplexingEfficiency() {
        // Средние количество streams на соединение
        const connections = Array.from(this.metrics.connections.values());
        const totalStreams = connections.reduce((sum, conn) => sum + conn.streams, 0);
        return connections.length > 0 ? (totalStreams / connections.length).toFixed(2) : 0;
    }
}

// Использование в QUIC сервере
const monitor = new HTTP3Monitor();

socket.on('session', (session) => {
    const startTime = Date.now();
    
    session.on('secure', () => {
        const handshakeTime = Date.now() - startTime;
        
        monitor.recordConnection(session.id, {
            handshakeTime: handshakeTime,
            cipher: session.cipher,
            alpn: session.alpnProtocol
        });
    });
    
    session.on('stream', (stream) => {
        const streamStart = Date.now();
        let firstByteTime = null;
        let bytesReceived = 0;
        
        stream.on('data', (chunk) => {
            if (firstByteTime === null) {
                firstByteTime = Date.now() - streamStart;
            }
            bytesReceived += chunk.length;
        });
        
        stream.on('end', () => {
            const streamEnd = Date.now();
            
            monitor.recordStream(session.id, stream.id, {
                startTime: streamStart,
                endTime: streamEnd,
                duration: streamEnd - streamStart,
                bytes: bytesReceived,
                firstByteTime: firstByteTime,
                status: 'completed'
            });
        });
        
        stream.on('error', (err) => {
            monitor.recordError(session.id, err.code || 'UNKNOWN', err.message);
        });
    });
});

// API endpoint для метрик
app.get('/api/http3/metrics', (req, res) => {
    res.json(monitor.getStatistics());
});
```

### 📝 Практическое задание Неделя 7

1. Настройте HTTP/3 сервер с помощью Caddy или экспериментального Node.js
2. Протестируйте 0-RTT подключения и connection migration
3. Реализуйте comprehensive security headers для HTTP/3
4. Создайте детальный мониторинг HTTP/3 производительности
5. Сравните производительность HTTP/1.1, HTTP/2 и HTTP/3 в реальных условиях

### ✅ Контрольные вопросы

- [ ] Понимаете принципиальные отличия QUIC от TCP?
- [ ] Можете объяснить преимущества 0-RTT handshake?
- [ ] Знаете, как работает connection migration в QUIC?
- [ ] Умеете настроить HTTP/3 сервер в production?
- [ ] Понимаете security implications HTTP/3?

---

# Модуль 4: DNS и сервис обнаружения {#module-4}
*Неделя 8 | Время изучения: 8-10 часов*

## DNS глубокое погружение

### 🧠 Концепция: DNS как критическая инфраструктура

DNS - это не просто "телефонная книга интернета". Для backend-систем это:
- Service discovery механизм
- Load balancing инструмент
- Failover система
- Security boundary

**Каждый DNS запрос добавляет латентность к вашему API!**

### 📊 DNS Resolution Process

```
Полный DNS Resolution Chain:
┌─────────────────────────────────────────────────────────────┐
│ 1. Browser Cache (0ms)                                     │
│    ├─ Hit: api.example.com → 93.184.216.34               │
│    └─ Miss: продолжаем поиск                              │
├─────────────────────────────────────────────────────────────┤
│ 2. OS Cache (1-2ms)                                       │
│    ├─ /etc/hosts check                                    │
│    ├─ System DNS cache                                    │
│    └─ Miss: идем к DNS resolver                           │
├─────────────────────────────────────────────────────────────┤
│ 3. DNS Resolver (10-50ms)                                 │
│    ├─ ISP DNS или 8.8.8.8                                │
│    ├─ Проверяем кеш resolver'а                            │
│    └─ Miss: начинаем рекурсивный поиск                    │
├─────────────────────────────────────────────────────────────┤
│ 4. Root Nameserver (50-100ms)                             │
│    Query: api.example.com                                  │
│    Response: см. .com nameservers                          │
├─────────────────────────────────────────────────────────────┤
│ 5. TLD Nameserver (50-100ms)                              │
│    Query: api.example.com                                  │
│    Response: см. example.com nameservers                   │
├─────────────────────────────────────────────────────────────┤
│ 6. Authoritative Nameserver (20-80ms)                     │
│    Query: api.example.com                                  │
│    Response: A 93.184.216.34 TTL 300                      │
└─────────────────────────────────────────────────────────────┘
Total: от 0ms (cache hit) до 300ms (cold resolution)
```

### 🔧 Практический пример: DNS Performance Monitoring

```python
import socket
import time
import dns.resolver
import dns.query
import dns.message
from concurrent.futures import ThreadPoolExecutor
import statistics

class DNSMonitor:
    def __init__(self):
        self.resolver = dns.resolver.Resolver()
        self.resolver.timeout = 5
        self.resolver.lifetime = 10
        
    def measure_dns_resolution(self, hostname, record_type='A'):
        """Измерение времени DNS разрешения"""
        start_time = time.time()
        
        try:
            # DNS query
            answer = self.resolver.resolve(hostname, record_type)
            end_time = time.time()
            
            resolution_time = (end_time - start_time) * 1000  # в мс
            
            return {
                'hostname': hostname,
                'record_type': record_type,
                'resolution_time_ms': resolution_time,
                'success': True,
                'answers': [str(rdata) for rdata in answer],
                'ttl': answer.ttl
            }
            
        except Exception as e:
            end_time = time.time()
            return {
                'hostname': hostname,
                'record_type': record_type,
                'resolution_time_ms': (end_time - start_time) * 1000,
                'success': False,
                'error': str(e)
            }
    
    def test_dns_servers(self, hostname, dns_servers):
        """Тестирование разных DNS серверов"""
        results = []
        
        for dns_server in dns_servers:
            # Создаем отдельный resolver для каждого DNS сервера
            resolver = dns.resolver.Resolver()
            resolver.nameservers = [dns_server]
            resolver.timeout = 3
            
            start_time = time.time()
            
            try:
                answer = resolver.resolve(hostname, 'A')
                end_time = time.time()
                
                results.append({
                    'dns_server': dns_server,
                    'hostname': hostname,
                    'resolution_time_ms': (end_time - start_time) * 1000,
                    'success': True,
                    'answer': str(answer[0]),
                    'ttl': answer.ttl
                })
                
            except Exception as e:
                end_time = time.time()
                results.append({
                    'dns_server': dns_server,
                    'hostname': hostname,
                    'resolution_time_ms': (end_time - start_time) * 1000,
                    'success': False,
                    'error': str(e)
                })
        
        return results
    
    def trace_dns_resolution(self, hostname):
        """Трассировка DNS разрешения по всей цепочке"""
        trace_results = []
        
        # 1. Получаем root nameservers
        start_time = time.time()
        try:
            root_ns = self.resolver.resolve('.', 'NS')
            trace_results.append({
                'step': 'root_nameservers',
                'time_ms': (time.time() - start_time) * 1000,
                'servers': [str(ns) for ns in root_ns]
            })
        except Exception as e:
            trace_results.append({
                'step': 'root_nameservers',
                'error': str(e)
            })
        
        # 2. Получаем TLD nameservers для домена
        domain_parts = hostname.split('.')
        tld = '.'.join(domain_parts[-2:])  # example.com
        
        start_time = time.time()
        try:
            tld_ns = self.resolver.resolve(tld, 'NS')
            trace_results.append({
                'step': f'tld_nameservers_{tld}',
                'time_ms': (time.time() - start_time) * 1000,
                'servers': [str(ns) for ns in tld_ns]
            })
        except Exception as e:
            trace_results.append({
                'step': f'tld_nameservers_{tld}',
                'error': str(e)
            })
        
        # 3. Финальное разрешение
        start_time = time.time()
        try:
            final_answer = self.resolver.resolve(hostname, 'A')
            trace_results.append({
                'step': 'final_resolution',
                'time_ms': (time.time() - start_time) * 1000,
                'answer': str(final_answer[0]),
                'ttl': final_answer.ttl
            })
        except Exception as e:
            trace_results.append({
                'step': 'final_resolution',
                'error': str(e)
            })
        
        return trace_results

# Использование DNS Monitor
monitor = DNSMonitor()

# Тестирование критических сервисов
critical_services = [
    'api.example.com',
    'auth.example.com', 
    'db.example.com',
    'cache.example.com'
]

# Популярные DNS серверы для сравнения
dns_servers = [
    '8.8.8.8',        # Google
    '1.1.1.1',        # Cloudflare
    '208.67.222.222', # OpenDNS
    '9.9.9.9'         # Quad9
]

print("DNS Performance Analysis")
print("=" * 50)

for service in critical_services:
    print(f"\nTesting {service}:")
    
    # Тест разных DNS серверов
    results = monitor.test_dns_servers(service, dns_servers)
    
    for result in results:
        if result['success']:
            print(f"  {result['dns_server']}: {result['resolution_time_ms']:.1f}ms (TTL: {result['ttl']}s)")
        else:
            print(f"  {result['dns_server']}: FAILED - {result['error']}")
    
    # Находим самый быстрый DNS сервер
    successful_results = [r for r in results if r['success']]
    if successful_results:
        fastest = min(successful_results, key=lambda x: x['resolution_time_ms'])
        print(f"  → Fastest: {fastest['dns_server']} ({fastest['resolution_time_ms']:.1f}ms)")
```

### ⚡ DNS Caching Strategies

**Оптимизация DNS кеширования:**

```python
import redis
import json
import time
from functools import wraps

class DNSCache:
    def __init__(self, redis_client, default_ttl=300):
        self.redis = redis_client
        self.default_ttl = default_ttl
        self.cache_hits = 0
        self.cache_misses = 0
    
    def get_cached_dns(self, hostname, record_type='A'):
        """Получение DNS записи из кеша"""
        cache_key = f"dns:{hostname}:{record_type}"
        
        try:
            cached_data = self.redis.get(cache_key)
            if cached_data:
                self.cache_hits += 1
                data = json.loads(cached_data)
                
                # Проверяем не истек ли TTL
                if time.time() - data['cached_at'] < data['ttl']:
                    return data['answer']
                else:
                    # TTL истек, удаляем из кеша
                    self.redis.delete(cache_key)
            
            self.cache_misses += 1
            return None
            
        except Exception as e:
            print(f"Cache error: {e}")
            self.cache_misses += 1
            return None
    
    def cache_dns(self, hostname, record_type, answer, ttl=None):
        """Кеширование DNS записи"""
        cache_key = f"dns:{hostname}:{record_type}"
        
        cache_data = {
            'answer': answer,
            'ttl': ttl or self.default_ttl,
            'cached_at': time.time(),
            'hostname': hostname,
            'record_type': record_type
        }
        
        try:
            # Кешируем на время TTL + 10% буфер
            cache_ttl = int((ttl or self.default_ttl) * 1.1)
            self.redis.setex(
                cache_key, 
                cache_ttl, 
                json.dumps(cache_data)
            )
        except Exception as e:
            print(f"Cache write error: {e}")
    
    def get_cache_stats(self):
        """Статистика кеша"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = (self.cache_hits / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate_percent': round(hit_rate, 2),
            'total_requests': total_requests
        }

def dns_cached(cache, ttl=300):
    """Декоратор для кеширования DNS запросов"""
    def decorator(func):
        @wraps(func)
        def wrapper(hostname, record_type='A', *args, **kwargs):
            # Проверяем кеш
            cached_result = cache.get_cached_dns(hostname, record_type)
            if cached_result:
                return {
                    'hostname': hostname,
                    'record_type': record_type,
                    'answer': cached_result,
                    'cached': True,
                    'resolution_time_ms': 0  # Из кеша мгновенно
                }
            
            # Кеша нет, делаем DNS запрос
            result = func(hostname, record_type, *args, **kwargs)
            
            # Кешируем результат
            if result.get('success') and result.get('answers'):
                cache.cache_dns(
                    hostname, 
                    record_type, 
                    result['answers'],
                    result.get('ttl', ttl)
                )
                result['cached'] = False
            
            return result
            
        return wrapper
    return decorator

# Использование с кешированием
redis_client = redis.Redis(host='localhost', port=6379, db=0)
dns_cache = DNSCache(redis_client)

@dns_cached(dns_cache, ttl=600)
def resolve_hostname(hostname, record_type='A'):
    """DNS resolution с кешированием"""
    monitor = DNSMonitor()
    return monitor.measure_dns_resolution(hostname, record_type)

# Тестирование эффективности кеша
print("Testing DNS Cache Efficiency:")
test_hostnames = ['api.example.com', 'google.com', 'github.com'] * 5

start_time = time.time()
for hostname in test_hostnames:
    result = resolve_hostname(hostname)
    cache_status = "HIT" if result.get('cached') else "MISS"
    print(f"{hostname}: {result.get('resolution_time_ms', 0):.1f}ms [{cache_status}]")

total_time = time.time() - start_time
cache_stats = dns_cache.get_cache_stats()

print(f"\nCache Statistics:")
print(f"Total time: {total_time:.2f}s")
print(f"Cache hit rate: {cache_stats['hit_rate_percent']}%")
print(f"Cache hits: {cache_stats['cache_hits']}")
print(f"Cache misses: {cache_stats['cache_misses']}")
```

### 🔄 DNS Load Balancing

**DNS Round Robin и Weighted Records:**

```python
import random
import time
from collections import defaultdict

class DNSLoadBalancer:
    def __init__(self):
        self.backends = {}
        self.health_checks = {}
        self.request_counts = defaultdict(int)
        self.response_times = defaultdict(list)
    
    def add_backend(self, hostname, backends_config):
        """Добавление backend серверов для hostname"""
        self.backends[hostname] = backends_config
        
        # Инициализация health checks
        for backend in backends_config:
            backend_ip = backend['ip']
            self.health_checks[backend_ip] = {
                'healthy': True,
                'last_check': time.time(),
                'failures': 0,
                'response_time': 0
            }
    
    def health_check(self, backend_ip, port=80):
        """Проверка здоровья backend сервера"""
        import socket
        
        start_time = time.time()
        try:
            sock = socket.create_connection((backend_ip, port), timeout=5)
            sock.close()
            
            response_time = (time.time() - start_time) * 1000
            
            # Обновляем статус
            self.health_checks[backend_ip].update({
                'healthy': True,
                'last_check': time.time(),
                'failures': 0,
                'response_time': response_time
            })
            
            return True
            
        except Exception as e:
            response_time = (time.time() - start_time) * 1000
            
            # Увеличиваем счетчик неудач
            self.health_checks[backend_ip]['failures'] += 1
            self.health_checks[backend_ip]['last_check'] = time.time()
            self.health_checks[backend_ip]['response_time'] = response_time
            
            # Помечаем как нездоровый после 3 неудач
            if self.health_checks[backend_ip]['failures'] >= 3:
                self.health_checks[backend_ip]['healthy'] = False
            
            return False
    
    def get_backend_round_robin(self, hostname):
        """Round Robin балансировка"""
        if hostname not in self.backends:
            return None
        
        backends = self.backends[hostname]
        healthy_backends = [
            b for b in backends 
            if self.health_checks.get(b['ip'], {}).get('healthy', True)
        ]
        
        if not healthy_backends:
            # Если нет здоровых серверов, возвращаем любой
            healthy_backends = backends
        
        # Простой round robin по количеству запросов
        selected = min(healthy_backends, key=lambda b: self.request_counts[b['ip']])
        self.request_counts[selected['ip']] += 1
        
        return selected['ip']
    
    def get_backend_weighted(self, hostname):
        """Weighted балансировка"""
        if hostname not in self.backends:
            return None
        
        backends = self.backends[hostname]
        healthy_backends = [
            b for b in backends 
            if self.health_checks.get(b['ip'], {}).get('healthy', True)
        ]
        
        if not healthy_backends:
            healthy_backends = backends
        
        # Weighted random selection
        total_weight = sum(b.get('weight', 1) for b in healthy_backends)
        random_weight = random.uniform(0, total_weight)
        
        current_weight = 0
        for backend in healthy_backends:
            current_weight += backend.get('weight', 1)
            if random_weight <= current_weight:
                self.request_counts[backend['ip']] += 1
                return backend['ip']
        
        # Fallback
        return healthy_backends[0]['ip']
    
    def get_backend_least_connections(self, hostname):
        """Least connections балансировка"""
        if hostname not in self.backends:
            return None
        
        backends = self.backends[hostname]
        healthy_backends = [
            b for b in backends 
            if self.health_checks.get(b['ip'], {}).get('healthy', True)
        ]
        
        if not healthy_backends:
            healthy_backends = backends
        
        # Выбираем backend с наименьшим количеством соединений
        selected = min(healthy_backends, key=lambda b: self.request_counts[b['ip']])
        self.request_counts[selected['ip']] += 1
        
        return selected['ip']
    
    def get_backend_fastest_response(self, hostname):
        """Fastest response балансировка"""
        if hostname not in self.backends:
            return None
        
        backends = self.backends[hostname]
        healthy_backends = [
            b for b in backends 
            if self.health_checks.get(b['ip'], {}).get('healthy', True)
        ]
        
        if not healthy_backends:
            healthy_backends = backends
        
        # Выбираем backend с лучшим response time
        selected = min(
            healthy_backends, 
            key=lambda b: self.health_checks.get(b['ip'], {}).get('response_time', float('inf'))
        )
        self.request_counts[selected['ip']] += 1
        
        return selected['ip']
    
    def get_stats(self):
        """Статистика балансировщика"""
        return {
            'backends': dict(self.backends),
            'health_checks': dict(self.health_checks),
            'request_counts': dict(self.request_counts),
            'total_requests': sum(self.request_counts.values())
        }

# Настройка DNS Load Balancer
lb = DNSLoadBalancer()

# Добавляем backend серверы для API
lb.add_backend('api.example.com', [
    {'ip': '10.0.1.10', 'weight': 3},  # Более мощный сервер
    {'ip': '10.0.1.11', 'weight': 2},  # Средний сервер
    {'ip': '10.0.1.12', 'weight': 1},  # Слабый сервер
])

# Добавляем backend серверы для базы данных
lb.add_backend('db.example.com', [
    {'ip': '10.0.2.10', 'weight': 1},  # Primary DB
    {'ip': '10.0.2.11', 'weight': 1},  # Secondary DB
])

# Проверка здоровья серверов
print("Health checking backends...")
for hostname, backends in lb.backends.items():
    for backend in backends:
        is_healthy = lb.health_check(backend['ip'], 80)
        status = "HEALTHY" if is_healthy else "UNHEALTHY"
        print(f"{hostname} -> {backend['ip']}: {status}")

# Тестирование разных алгоритмов балансировки
print("\nTesting load balancing algorithms:")

algorithms = {
    'round_robin': lb.get_backend_round_robin,
    'weighted': lb.get_backend_weighted,
    'least_connections': lb.get_backend_least_connections,
    'fastest_response': lb.get_backend_fastest_response
}

for alg_name, alg_func in algorithms.items():
    print(f"\n{alg_name.upper()}:")
    
    # Сброс счетчиков для честного сравнения
    lb.request_counts.clear()
    
    # Делаем 20 запросов
    for i in range(20):
        backend_ip = alg_func('api.example.com')
        print(f"  Request {i+1}: {backend_ip}")
    
    # Показываем распределение
    print(f"  Distribution: {dict(lb.request_counts)}")
```

### 🌐 Service Discovery с DNS

**Microservices Service Discovery:**

```python
import consul
import dns.resolver
import json
from typing import List, Dict, Optional

class ServiceDiscovery:
    def __init__(self, consul_host='localhost', consul_port=8500):
        self.consul = consul.Consul(host=consul_host, port=consul_port)
        self.dns_resolver = dns.resolver.Resolver()
        self.service_cache = {}
        
    def register_service(self, service_name: str, service_id: str, 
                        address: str, port: int, tags: List[str] = None,
                        health_check: Dict = None):
        """Регистрация сервиса в Consul"""
        
        # Настройка health check по умолчанию
        if health_check is None:
            health_check = {
                'http': f'http://{address}:{port}/health',
                'interval': '30s',
                'timeout': '10s',
                'deregister_critical_service_after': '60s'
            }
        
        # Регистрация в Consul
        self.consul.agent.service.register(
            name=service_name,
            service_id=service_id,
            address=address,
            port=port,
            tags=tags or [],
            check=health_check
        )
        
        print(f"Service registered: {service_name} ({service_id}) at {address}:{port}")
    
    def discover_service(self, service_name: str, tag: str = None) -> List[Dict]:
        """Обнаружение сервисов через Consul"""
        try:
            # Получаем здоровые сервисы
            _, services = self.consul.health.service(
                service_name, 
                passing=True,  # Только здоровые
                tag=tag
            )
            
            discovered_services = []
            for service in services:
                service_info = service['Service']
                discovered_services.append({
                    'id': service_info['ID'],
                    'name': service_info['Service'],
                    'address': service_info['Address'],
                    'port': service_info['Port'],
                    'tags': service_info['Tags'],
                    'datacenter': service['Node']['Datacenter']
                })
            
            # Кешируем результат
            cache_key = f"{service_name}:{tag or 'all'}"
            self.service_cache[cache_key] = {
                'services': discovered_services,
                'cached_at': time.time(),
                'ttl': 30  # 30 секунд TTL
            }
            
            return discovered_services
            
        except Exception as e:
            print(f"Service discovery error: {e}")
            
            # Пытаемся использовать кеш при ошибке
            cache_key = f"{service_name}:{tag or 'all'}"
            if cache_key in self.service_cache:
                cached = self.service_cache[cache_key]
                if time.time() - cached['cached_at'] < cached['ttl'] * 2:  # Расширенный TTL
                    print(f"Using cached services for {service_name}")
                    return cached['services']
            
            return []
    
    def discover_service_dns(self, service_name: str, domain: str = 'service.consul') -> List[str]:
        """Обнаружение сервисов через DNS запросы к Consul"""
        dns_name = f"{service_name}.{domain}"
        
        try:
            # SRV запрос для получения портов
            srv_records = self.dns_resolver.resolve(dns_name, 'SRV')
            
            services = []
            for srv in srv_records:
                # Получаем A запись для хоста
                try:
                    a_records = self.dns_resolver.resolve(str(srv.target), 'A')
                    for a_record in a_records:
                        services.append({
                            'address': str(a_record),
                            'port': srv.port,
                            'priority': srv.priority,
                            'weight': srv.weight,
                            'target': str(srv.target)
                        })
                except:
                    continue
            
            return services
            
        except Exception as e:
            print(f"DNS service discovery error: {e}")
            return []
    
    def get_service_endpoint(self, service_name: str, load_balance: str = 'round_robin') -> Optional[str]:
        """Получение endpoint сервиса с балансировкой нагрузки"""
        services = self.discover_service(service_name)
        
        if not services:
            return None
        
        if load_balance == 'round_robin':
            # Простой round robin по количеству запросов
            cache_key = f"requests:{service_name}"
            if cache_key not in self.service_cache:
                self.service_cache[cache_key] = 0
            
            index = self.service_cache[cache_key] % len(services)
            self.service_cache[cache_key] += 1
            
            selected = services[index]
            
        elif load_balance == 'random':
            import random
            selected = random.choice(services)
            
        else:
            # По умолчанию первый
            selected = services[0]
        
        return f"http://{selected['address']}:{selected['port']}"
    
    def health_check_service(self, service_name: str) -> Dict:
        """Проверка здоровья всех инстансов сервиса"""
        services = self.discover_service(service_name)
        
        health_status = {
            'service_name': service_name,
            'total_instances': len(services),
            'healthy_instances': 0,
            'unhealthy_instances': 0,
            'instances': []
        }
        
        for service in services:
            try:
                # Простая проверка доступности порта
                import socket
                sock = socket.create_connection(
                    (service['address'], service['port']), 
                    timeout=5
                )
                sock.close()
                
                health_status['healthy_instances'] += 1
                health_status['instances'].append({
                    'id': service['id'],
                    'address': f"{service['address']}:{service['port']}",
                    'status': 'healthy'
                })
                
            except Exception as e:
                health_status['unhealthy_instances'] += 1
                health_status['instances'].append({
                    'id': service['id'],
                    'address': f"{service['address']}:{service['port']}",
                    'status': 'unhealthy',
                    'error': str(e)
                })
        
        return health_status

# Пример использования Service Discovery
sd = ServiceDiscovery()

# Регистрируем микросервисы
services_to_register = [
    {
        'name': 'user-service',
        'id': 'user-service-1',
        'address': '10.0.1.10',
        'port': 8001,
        'tags': ['api', 'users', 'v1']
    },
    {
        'name': 'user-service',
        'id': 'user-service-2', 
        'address': '10.0.1.11',
        'port': 8001,
        'tags': ['api', 'users', 'v1']
    },
    {
        'name': 'order-service',
        'id': 'order-service-1',
        'address': '10.0.1.20',
        'port': 8002,
        'tags': ['api', 'orders', 'v1']
    },
    {
        'name': 'payment-service',
        'id': 'payment-service-1',
        'address': '10.0.1.30',
        'port': 8003,
        'tags': ['api', 'payments', 'v1', 'critical']
    }
]

print("Registering microservices...")
for service in services_to_register:
    sd.register_service(**service)

# Тестирование service discovery
print("\nDiscovering services...")

for service_name in ['user-service', 'order-service', 'payment-service']:
    print(f"\n{service_name}:")
    
    # Через Consul API
    services = sd.discover_service(service_name)
    print(f"  Found {len(services)} instances via Consul API")
    
    for service in services:
        print(f"    {service['id']}: {service['address']}:{service['port']} {service['tags']}")
    
    # Через DNS
    dns_services = sd.discover_service_dns(service_name)
    print(f"  Found {len(dns_services)} instances via DNS")
    
    # Получение endpoint с балансировкой
    endpoint = sd.get_service_endpoint(service_name, 'round_robin')
    print(f"  Load balanced endpoint: {endpoint}")
    
    # Health check
    health = sd.health_check_service(service_name)
    print(f"  Health: {health['healthy_instances']}/{health['total_instances']} healthy")
```

### 📊 DNS Security и Monitoring

**DNS over HTTPS (DoH) и мониторинг:**

```python
import requests
import base64
import json
import time
from urllib.parse import urlencode

class SecureDNSResolver:
    def __init__(self):
        self.doh_servers = {
            'cloudflare': 'https://cloudflare-dns.com/dns-query',
            'google': 'https://dns.google/dns-query',
            'quad9': 'https://dns.quad9.net/dns-query'
        }
        
        self.metrics = {
            'queries_total': 0,
            'queries_by_server': {},
            'response_times': {},
            'cache_hits': 0,
            'cache_misses': 0,
            'security_blocks': 0
        }
        
        # Простой кеш для DoH запросов
        self.cache = {}
    
    def query_doh(self, hostname: str, record_type: str = 'A', 
                  server: str = 'cloudflare') -> Dict:
        """DNS over HTTPS запрос"""
        
        if server not in self.doh_servers:
            raise ValueError(f"Unknown DoH server: {server}")
        
        # Проверяем кеш
        cache_key = f"{hostname}:{record_type}:{server}"
        if cache_key in self.cache:
            cached_data = self.cache[cache_key]
            if time.time() - cached_data['cached_at'] < cached_data.get('ttl', 300):
                self.metrics['cache_hits'] += 1
                return {
                    'hostname': hostname,
                    'record_type': record_type,
                    'answers': cached_data['answers'],
                    'cached': True,
                    'server': server,
                    'response_time_ms': 0
                }
        
        self.metrics['cache_misses'] += 1
        
        doh_url = self.doh_servers[server]
        
        # Параметры для DoH запроса
        params = {
            'name': hostname,
            'type': record_type,
            'do': 'false',  # DNSSEC не требуется для этого примера
            'cd': 'false'   # Checking disabled
        }
        
        headers = {
            'Accept': 'application/dns-json',
            'User-Agent': 'SecureDNSResolver/1.0'
        }
        
        start_time = time.time()
        
        try:
            response = requests.get(
                doh_url,
                params=params,
                headers=headers,
                timeout=10
            )
            
            response_time = (time.time() - start_time) * 1000
            
            if response.status_code == 200:
                dns_response = response.json()
                
                # Обновляем метрики
                self.metrics['queries_total'] += 1
                if server not in self.metrics['queries_by_server']:
                    self.metrics['queries_by_server'][server] = 0
                self.metrics['queries_by_server'][server] += 1
                
                if server not in self.metrics['response_times']:
                    self.metrics['response_times'][server] = []
                self.metrics['response_times'][server].append(response_time)
                
                # Парсим ответы
                answers = []
                if 'Answer' in dns_response:
                    for answer in dns_response['Answer']:
                        answers.append({
                            'type': answer['type'],
                            'data': answer['data'],
                            'ttl': answer['TTL']
                        })
                
                # Кешируем результат
                if answers:
                    min_ttl = min(answer['ttl'] for answer in answers)
                    self.cache[cache_key] = {
                        'answers': answers,
                        'ttl': min_ttl,
                        'cached_at': time.time()
                    }
                
                # Проверяем на security блокировки
                if dns_response.get('Status') == 3:  # NXDOMAIN от security фильтра
                    self.metrics['security_blocks'] += 1
                
                return {
                    'hostname': hostname,
                    'record_type': record_type,
                    'answers': answers,
                    'status': dns_response.get('Status', 0),
                    'cached': False,
                    'server': server,
                    'response_time_ms': response_time,
                    'truncated': dns_response.get('TC', False),
                    'authentic_data': dns_response.get('AD', False)
                }
            
            else:
                return {
                    'hostname': hostname,
                    'record_type': record_type,
                    'error': f"HTTP {response.status_code}: {response.text}",
                    'server': server,
                    'response_time_ms': response_time
                }
                
        except Exception as e:
            response_time = (time.time() - start_time) * 1000
            return {
                'hostname': hostname,
                'record_type': record_type,
                'error': str(e),
                'server': server,
                'response_time_ms': response_time
            }
    
    def security_scan(self, hostnames: List[str]) -> Dict:
        """Проверка доменов на malware/phishing через secure DNS"""
        
        results = {
            'total_checked': len(hostnames),
            'clean': 0,
            'blocked': 0,
            'errors': 0,
            'details': []
        }
        
        for hostname in hostnames:
            # Используем Quad9 (блокирует вредоносные домены)
            result = self.query_doh(hostname, 'A', 'quad9')
            
            detail = {
                'hostname': hostname,
                'status': 'unknown'
            }
            
            if 'error' in result:
                results['errors'] += 1
                detail['status'] = 'error'
                detail['error'] = result['error']
            
            elif result.get('status') == 3:  # NXDOMAIN (может быть блокировка)
                results['blocked'] += 1
                detail['status'] = 'blocked'
                detail['reason'] = 'Blocked by security filter'
            
            elif result.get('answers'):
                results['clean'] += 1
                detail['status'] = 'clean'
                detail['ip_addresses'] = [a['data'] for a in result['answers'] if a['type'] == 1]
            
            else:
                results['errors'] += 1
                detail['status'] = 'no_answer'
            
            results['details'].append(detail)
        
        return results
    
    def get_metrics(self) -> Dict:
        """Получение метрик DNS resolver'а"""
        
        # Вычисляем статистику response times
        avg_response_times = {}
        for server, times in self.metrics['response_times'].items():
            if times:
                avg_response_times[server] = {
                    'avg_ms': sum(times) / len(times),
                    'min_ms': min(times),
                    'max_ms': max(times),
                    'queries': len(times)
                }
        
        cache_total = self.metrics['cache_hits'] + self.metrics['cache_misses']
        cache_hit_rate = (self.metrics['cache_hits'] / cache_total * 100) if cache_total > 0 else 0
        
        return {
            'total_queries': self.metrics['queries_total'],
            'queries_by_server': self.metrics['queries_by_server'],
            'avg_response_times': avg_response_times,
            'cache_hit_rate_percent': round(cache_hit_rate, 2),
            'cache_hits': self.metrics['cache_hits'],
            'cache_misses': self.metrics['cache_misses'],
            'security_blocks': self.metrics['security_blocks'],
            'cached_entries': len(self.cache)
        }

# Использование Secure DNS Resolver
resolver = SecureDNSResolver()

# Тестирование разных DoH серверов
test_domains = [
    'api.example.com',
    'google.com',
    'github.com',
    'stackoverflow.com'
]

print("Testing DNS over HTTPS...")
print("=" * 40)

for domain in test_domains:
    print(f"\nResolving {domain}:")
    
    for server in ['cloudflare', 'google', 'quad9']:
        result = resolver.query_doh(domain, 'A', server)
        
        if 'error' not in result:
            ips = [a['data'] for a in result['answers'] if a['type'] == 1]
            cache_status = " [CACHED]" if result['cached'] else ""
            print(f"  {server}: {', '.join(ips)} ({result['response_time_ms']:.1f}ms){cache_status}")
        else:
            print(f"  {server}: ERROR - {result['error']}")

# Тестирование security функций
print("\n\nTesting security scanning...")
suspicious_domains = [
    'google.com',           # Чистый домен
    'malware.testing.google.test',  # Тестовый malware домен
    'phishing.testing.google.test'  # Тестовый phishing домен
]

security_results = resolver.security_scan(suspicious_domains)
print(f"Security scan results:")
print(f"  Total: {security_results['total_checked']}")
print(f"  Clean: {security_results['clean']}")
print(f"  Blocked: {security_results['blocked']}")
print(f"  Errors: {security_results['errors']}")

for detail in security_results['details']:
    print(f"  {detail['hostname']}: {detail['status']}")

# Показываем финальные метрики
print("\n\nDNS Resolver Metrics:")
metrics = resolver.get_metrics()
print(json.dumps(metrics, indent=2))
```

### 📝 Практическое задание

1. Настройте DNS кеширование для вашего приложения с Redis
2. Реализуйте DNS load balancing для ваших API endpoints
3. Настройте Service Discovery с Consul для микросервисов
4. Добавьте мониторинг DNS производительности
5. Протестируйте DNS over HTTPS для повышения безопасности

### ✅ Контрольные вопросы

- [ ] Понимаете влияние DNS на производительность API?
- [ ] Можете настроить эффективное DNS кеширование?
- [ ] Знаете принципы DNS load balancing?
- [ ] Умеете использовать DNS для service discovery?
- [ ] Понимаете важность DNS security и DoH?

---

# Модуль 5: Производительность и оптимизация {#module-5}
*Недели 9-10 | Время изучения: 16-20 часов*

## Неделя 9: Сетевая производительность

### 🧠 Концепция: Latency vs Throughput - ключевое понимание

**Latency** (задержка) - время от отправки запроса до получения ответа
**Throughput** (пропускная способность) - количество данных в единицу времени

```
Аналогия с трубой:
┌─────────────────────────────────────────────────────────────┐
│ Latency = время прохождения капли от начала до конца трубы │
│ Throughput = объем воды, проходящий через трубу в секунду  │
├─────────────────────────────────────────────────────────────┤
│ Можно иметь:                                                │
│ • Высокий throughput, высокий latency (толстая длинная)    │
│ • Низкий throughput, низкий latency (тонкая короткая)      │
│ • Высокий throughput, низкий latency (толстая короткая)    │
│ • Низкий throughput, высокий latency (тонкая длинная)      │
└─────────────────────────────────────────────────────────────┘
```

### 📊 Измерение сетевых метрик

**Comprehensive Network Performance Monitor:**

```python
import time
import socket
import subprocess
import threading
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil
import requests
from dataclasses import dataclass
from typing import List, Dict, Optional
import json

@dataclass
class NetworkMetrics:
    timestamp: float
    latency_ms: float
    throughput_mbps: float
    packet_loss_percent: float
    jitter_ms: float
    bandwidth_utilization_percent: float

class NetworkPerformanceMonitor:
    def __init__(self):
        self.metrics_history = []
        self.baseline_metrics = None
        
    def measure_latency(self, host: str, port: int = 80, samples: int = 10) -> Dict:
        """Измерение latency с множественными пробами"""
        latencies = []
        successful_connections = 0
        
        for i in range(samples):
            start_time = time.time()
            try:
                sock = socket.create_connection((host, port), timeout=10)
                sock.close()
                
                latency = (time.time() - start_time) * 1000  # в мс
                latencies.append(latency)
                successful_connections += 1
                
            except Exception as e:
                # Считаем неудачные соединения как очень большую latency
                latencies.append(10000)  # 10 секунд timeout
            
            # Небольшая пауза между пробами
            time.sleep(0.1)
        
        if latencies:
            return {
                'min_ms': min(latencies),
                'max_ms': max(latencies),
                'avg_ms': statistics.mean(latencies),
                'median_ms': statistics.median(latencies),
                'p95_ms': statistics.quantiles(latencies, n=20)[18] if len(latencies) > 1 else latencies[0],
                'jitter_ms': statistics.stdev(latencies) if len(latencies) > 1 else 0,
                'success_rate': (successful_connections / samples) * 100,
                'samples': samples
            }
        else:
            return None
    
    def measure_throughput(self, url: str, duration_seconds: int = 30) -> Dict:
        """Измерение throughput через HTTP download"""
        
        start_time = time.time()
        total_bytes = 0
        chunk_times = []
        
        try:
            response = requests.get(url, stream=True, timeout=duration_seconds + 10)
            response.raise_for_status()
            
            chunk_start = time.time()
            
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    chunk_end = time.time()
                    chunk_time = chunk_end - chunk_start
                    
                    total_bytes += len(chunk)
                    chunk_times.append(chunk_time)
                    
                    chunk_start = chunk_end
                    
                    # Прерываем после заданного времени
                    if time.time() - start_time >= duration_seconds:
                        break
            
            total_time = time.time() - start_time
            
            if total_time > 0:
                throughput_bps = total_bytes / total_time
                throughput_mbps = throughput_bps / (1024 * 1024)
                
                return {
                    'throughput_mbps': throughput_mbps,
                    'throughput_bps': throughput_bps,
                    'total_bytes': total_bytes,
                    'duration_seconds': total_time,
                    'avg_chunk_time_ms': statistics.mean(chunk_times) * 1000 if chunk_times else 0,
                    'chunks_received': len(chunk_times)
                }
            
        except Exception as e:
            return {
                'error': str(e),
                'throughput_mbps': 0
            }
        
        return {'throughput_mbps': 0}
    
    def measure_packet_loss(self, host: str, count: int = 100) -> float:
        """Измерение packet loss через ping"""
        try:
            result = subprocess.run(
                ['ping', '-c', str(count), host],
                capture_output=True,
                text=True,
                timeout=count + 30
            )
            
            output = result.stdout
            
            # Парсим вывод ping для packet loss
            for line in output.split('\n'):
                if 'packet loss' in line:
                    # Ищем процент потерь
                    parts = line.split()
                    for i, part in enumerate(parts):
                        if '%' in part and 'loss' in parts[i+1:i+2]:
                            return float(part.replace('%', ''))
            
            return 0.0
            
        except Exception as e:
            print(f"Packet loss measurement error: {e}")
            return 100.0  # Максимальные потери при ошибке
    
    def measure_bandwidth_utilization(self) -> Dict:
        """Измерение использования bandwidth сетевого интерфейса"""
        
        # Получаем статистику до измерения
        net_stats_before = psutil.net_io_counters()
        time_before = time.time()
        
        # Ждем секунду для измерения
        time.sleep(1)
        
        # Получаем статистику после измерения
        net_stats_after = psutil.net_io_counters()
        time_after = time.time()
        
        time_delta = time_after - time_before
        
        # Вычисляем скорости
        bytes_sent_per_sec = (net_stats_after.bytes_sent - net_stats_before.bytes_sent) / time_delta
        bytes_recv_per_sec = (net_stats_after.bytes_recv - net_stats_before.bytes_recv) / time_delta
        
        # Конвертируем в Mbps
        upload_mbps = (bytes_sent_per_sec * 8) / (1024 * 1024)
        download_mbps = (bytes_recv_per_sec * 8) / (1024 * 1024)
        
        return {
            'upload_mbps': upload_mbps,
            'download_mbps': download_mbps,
            'total_mbps': upload_mbps + download_mbps,
            'bytes_sent_per_sec': bytes_sent_per_sec,
            'bytes_recv_per_sec': bytes_recv_per_sec,
            'packets_sent': net_stats_after.packets_sent - net_stats_before.packets_sent,
            'packets_recv': net_stats_after.packets_recv - net_stats_before.packets_recv
        }
    
    def comprehensive_network_test(self, target_host: str, test_url: str = None) -> Dict:
        """Комплексное тестирование сетевой производительности"""
        
        print(f"Starting comprehensive network test for {target_host}...")
        
        results = {
            'target_host': target_host,
            'timestamp': time.time(),
            'tests': {}
        }
        
        # 1. Latency тест
        print("  Testing latency...")
        latency_results = self.measure_latency(target_host)
        if latency_results:
            results['tests']['latency'] = latency_results
            print(f"    Avg latency: {latency_results['avg_ms']:.1f}ms")
            print(f"    P95 latency: {latency_results['p95_ms']:.1f}ms")
            print(f"    Jitter: {latency_results['jitter_ms']:.1f}ms")
        
        # 2. Packet loss тест
        print("  Testing packet loss...")
        packet_loss = self.measure_packet_loss(target_host)
        results['tests']['packet_loss'] = {
            'loss_percent': packet_loss
        }
        print(f"    Packet loss: {packet_loss}%")
        
        # 3. Throughput тест (если URL предоставлен)
        if test_url:
            print("  Testing throughput...")
            throughput_results = self.measure_throughput(test_url, 10)
            results['tests']['throughput'] = throughput_results
            if 'throughput_mbps' in throughput_results:
                print(f"    Throughput: {throughput_results['throughput_mbps']:.2f} Mbps")
        
        # 4. Bandwidth utilization
        print("  Measuring bandwidth utilization...")
        bandwidth_results = self.measure_bandwidth_utilization()
        results['tests']['bandwidth'] = bandwidth_results
        print(f"    Current usage: {bandwidth_results['total_mbps']:.2f} Mbps")
        
        # 5. Анализ результатов
        analysis = self.analyze_performance(results)
        results['analysis'] = analysis
        
        return results
    
    def analyze_performance(self, test_results: Dict) -> Dict:
        """Анализ результатов производительности"""
        
        analysis = {
            'performance_score': 100,  # Начинаем со 100%
            'issues': [],
            'recommendations': []
        }
        
        tests = test_results.get('tests', {})
        
        # Анализ latency
        if 'latency' in tests:
            latency = tests['latency']
            avg_latency = latency.get('avg_ms', 0)
            jitter = latency.get('jitter_ms', 0)
            
            if avg_latency > 200:
                analysis['performance_score'] -= 30
                analysis['issues'].append(f"High latency: {avg_latency:.1f}ms (>200ms)")
                analysis['recommendations'].append("Consider using CDN or closer servers")
            
            elif avg_latency > 100:
                analysis['performance_score'] -= 15
                analysis['issues'].append(f"Moderate latency: {avg_latency:.1f}ms (>100ms)")
                analysis['recommendations'].append("Optimize server response time")
            
            if jitter > 50:
                analysis['performance_score'] -= 20
                analysis['issues'].append(f"High jitter: {jitter:.1f}ms (>50ms)")
                analysis['recommendations'].append("Check network stability")
        
        # Анализ packet loss
        if 'packet_loss' in tests:
            packet_loss = tests['packet_loss'].get('loss_percent', 0)
            
            if packet_loss > 5:
                analysis['performance_score'] -= 40
                analysis['issues'].append(f"High packet loss: {packet_loss}% (>5%)")
                analysis['recommendations'].append("Check network infrastructure")
            
            elif packet_loss > 1:
                analysis['performance_score'] -= 20
                analysis['issues'].append(f"Moderate packet loss: {packet_loss}% (>1%)")
                analysis['recommendations'].append("Monitor network quality")
        
        # Анализ throughput
        if 'throughput' in tests:
            throughput = tests['throughput'].get('throughput_mbps', 0)
            
            if throughput < 1:
                analysis['performance_score'] -= 25
                analysis['issues'].append(f"Low throughput: {throughput:.2f} Mbps (<1 Mbps)")
                analysis['recommendations'].append("Check bandwidth limitations")
            
            elif throughput < 10:
                analysis['performance_score'] -= 10
                analysis['issues'].append(f"Moderate throughput: {throughput:.2f} Mbps (<10 Mbps)")
                analysis['recommendations'].append("Consider bandwidth upgrade")
        
        # Общая оценка
        if analysis['performance_score'] >= 90:
            analysis['overall_rating'] = 'Excellent'
        elif analysis['performance_score'] >= 70:
            analysis['overall_rating'] = 'Good'
        elif analysis['performance_score'] >= 50:
            analysis['overall_rating'] = 'Fair'
        else:
            analysis['overall_rating'] = 'Poor'
        
        return analysis
    
    def continuous_monitoring(self, targets: List[str], interval_seconds: int = 60):
        """Непрерывный мониторинг сетевой производительности"""
        
        print(f"Starting continuous monitoring of {len(targets)} targets...")
        print(f"Monitoring interval: {interval_seconds} seconds")
        
        while True:
            timestamp = time.time()
            print(f"\n--- Monitoring cycle at {time.strftime('%Y-%m-%d %H:%M:%S')} ---")
            
            for target in targets:
                try:
                    # Быстрый тест latency
                    latency_result = self.measure_latency(target, samples=5)
                    
                    if latency_result:
                        avg_latency = latency_result['avg_ms']
                        jitter = latency_result['jitter_ms']
                        success_rate = latency_result['success_rate']
                        
                        status = "OK"
                        if avg_latency > 200 or jitter > 50 or success_rate < 90:
                            status = "DEGRADED"
                        if avg_latency > 500 or success_rate < 50:
                            status = "CRITICAL"
                        
                        print(f"{target}: {avg_latency:.1f}ms (±{jitter:.1f}ms) [{status}]")
                        
                        # Сохраняем метрики
                        metric = NetworkMetrics(
                            timestamp=timestamp,
                            latency_ms=avg_latency,
                            throughput_mbps=0,  # Не измеряем throughput в continuous режиме
                            packet_loss_percent=0,
                            jitter_ms=jitter,
                            bandwidth_utilization_percent=0
                        )
                        self.metrics_history.append(metric)
                    
                except Exception as e:
                    print(f"{target}: ERROR - {e}")
            
            # Ограничиваем историю последними 1000 записями
            if len(self.metrics_history) > 1000:
                self.metrics_history = self.metrics_history[-1000:]
            
            time.sleep(interval_seconds)

# Практическое использование
monitor = NetworkPerformanceMonitor()

# Тестирование ключевых сервисов
critical_services = [
    {
        'name': 'API Server',
        'host': 'api.example.com',
        'test_url': 'https://api.example.com/health'
    },
    {
        'name': 'Database',
        'host': 'db.example.com',
        'test_url': None
    },
    {
        'name': 'CDN',
        'host': 'cdn.example.com',
        'test_url': 'https://cdn.example.com/test.jpg'
    }
]

print("Network Performance Testing Suite")
print("=" * 50)

for service in critical_services:
    print(f"\n🔍 Testing {service['name']} ({service['host']}):")
    
    results = monitor.comprehensive_network_test(
        service['host'], 
        service['test_url']
    )
    
    # Выводим анализ
    analysis = results['analysis']
    print(f"\n📊 Performance Analysis:")
    print(f"  Overall Rating: {analysis['overall_rating']}")
    print(f"  Performance Score: {analysis['performance_score']}/100")
    
    if analysis['issues']:
        print(f"  Issues Found:")
        for issue in analysis['issues']:
            print(f"    ⚠️ {issue}")
    
    if analysis['recommendations']:
        print(f"  Recommendations:")
        for rec in analysis['recommendations']:
            print(f"    💡 {rec}")
    
    print("-" * 50)

# Сохранение результатов для дальнейшего анализа
results_file = f"network_performance_{int(time.time())}.json"
with open(results_file, 'w') as f:
    json.dump({
        'timestamp': time.time(),
        'services_tested': len(critical_services),
        'test_results': [
            monitor.comprehensive_network_test(service['host'], service['test_url']) 
            for service in critical_services
        ]
    }, f, indent=2)

print(f"\n📁 Results saved to: {results_file}")
```

### ⚡ TCP Tuning для высокой производительности

**Оптимизация TCP параметров на production серверах:**

```bash
#!/bin/bash
# tcp_optimization.sh - Скрипт для оптимизации TCP параметров

echo "🔧 Optimizing TCP parameters for high-performance backend servers"

# Создаем backup текущих настроек
echo "📋 Creating backup of current settings..."
sysctl -a | grep -E "(tcp|net\.core)" > /tmp/tcp_settings_backup_$(date +%s).txt

# TCP Buffer настройки
echo "🚀 Optimizing TCP buffers..."

# Увеличиваем максимальные размеры буферов (128MB)
sysctl -w net.core.rmem_max=134217728
sysctl -w net.core.wmem_max=134217728

# TCP socket буферы: min default max (в байтах)
# min: 64KB, default: 1MB, max: 128MB
sysctl -w net.ipv4.tcp_rmem="65536 1048576 134217728"
sysctl -w net.ipv4.tcp_wmem="65536 1048576 134217728"

# Увеличиваем буферы для сетевых устройств
sysctl -w net.core.netdev_max_backlog=30000
sysctl -w net.core.netdev_budget=600

echo "⚡ Optimizing TCP congestion control..."

# Используем BBR congestion control (если доступен)
if sysctl net.ipv4.tcp_available_congestion_control | grep -q bbr; then
    sysctl -w net.ipv4.tcp_congestion_control=bbr
    echo "✅ BBR congestion control enabled"
else
    # Fallback к cubic
    sysctl -w net.ipv4.tcp_congestion_control=cubic
    echo "⚠️ BBR not available, using cubic"
fi

echo "🔄 Optimizing connection handling..."

# Увеличиваем количество соединений в очереди
sysctl -w net.core.somaxconn=65535

# Оптимизация TIME_WAIT сокетов
sysctl -w net.ipv4.tcp_tw_reuse=1
sysctl -w net.ipv4.tcp_fin_timeout=15

# Быстрое обнаружение мертвых соединений
sysctl -w net.ipv4.tcp_keepalive_time=600
sysctl -w net.ipv4.tcp_keepalive_intvl=60
sysctl -w net.ipv4.tcp_keepalive_probes=3

echo "🛡️ Optimizing security and stability..."

# Защита от SYN flood атак
sysctl -w net.ipv4.tcp_max_syn_backlog=65535
sysctl -w net.ipv4.tcp_syncookies=1

# Оптимизация для высоконагруженных серверов
sysctl -w net.ipv4.ip_local_port_range="1024 65535"
sysctl -w net.ipv4.tcp_max_tw_buckets=1440000

echo "📊 Optimizing for high throughput..."

# Disable slow start after idle
sysctl -w net.ipv4.tcp_slow_start_after_idle=0

# Enable window scaling
sysctl -w net.ipv4.tcp_window_scaling=1

# Enable timestamps
sysctl -w net.ipv4.tcp_timestamps=1

# Enable SACK
sysctl -w net.ipv4.tcp_sack=1

echo "💾 Making settings persistent..."

# Добавляем в /etc/sysctl.conf для постоянного применения
cat >> /etc/sysctl.conf << 'EOF'

# High-performance TCP settings for backend servers
# Generated by tcp_optimization.sh

# TCP Buffer optimization
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 65536 1048576 134217728
net.ipv4.tcp_wmem = 65536 1048576 134217728

# Network device buffers
net.core.netdev_max_backlog = 30000
net.core.netdev_budget = 600

# Connection handling
net.core.somaxconn = 65535
net.ipv4.tcp_max_syn_backlog = 65535

# TIME_WAIT optimization
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_fin_timeout = 15

# Keep-alive optimization
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_intvl = 60
net.ipv4.tcp_keepalive_probes = 3

# Security
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_tw_buckets = 1440000

# Performance
net.ipv4.ip_local_port_range = 1024 65535
net.ipv4.tcp_slow_start_after_idle = 0
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_timestamps = 1
net.ipv4.tcp_sack = 1

EOF

echo "✅ TCP optimization completed!"
echo "📊 Current TCP settings summary:"

# Показываем текущие настройки
echo "Buffer sizes:"
echo "  Max receive buffer: $(sysctl -n net.core.rmem_max) bytes"
echo "  Max send buffer: $(sysctl -n net.core.wmem_max) bytes"
echo "  TCP receive buffers: $(sysctl -n net.ipv4.tcp_rmem)"
echo "  TCP send buffers: $(sysctl -n net.ipv4.tcp_wmem)"

echo "Connection limits:"
echo "  Max connections queue: $(sysctl -n net.core.somaxconn)"
echo "  Max SYN backlog: $(sysctl -n net.ipv4.tcp_max_syn_backlog)"

echo "Congestion control:"
echo "  Algorithm: $(sysctl -n net.ipv4.tcp_congestion_control)"

echo ""
echo "🔄 Reboot required for some changes to take full effect"
echo "📁 Backup of previous settings saved to /tmp/tcp_settings_backup_*.txt"
```

**Мониторинг эффективности TCP оптимизации:**

```python
import subprocess
import re
import time
import json
from typing import Dict, List

class TCPPerformanceMonitor:
    def __init__(self):
        self.metrics_history = []
    
    def get_tcp_stats(self) -> Dict:
        """Получение детальной статистики TCP"""
        
        stats = {}
        
        try:
            # Статистика TCP соединений
            ss_output = subprocess.check_output(['ss', '-s'], text=True)
            
            # Парсим вывод ss -s
            for line in ss_output.split('\n'):
                if 'TCP:' in line:
                    # TCP: 1234 (estab 856, closed 234, orphaned 12, synrecv 0, timewait 132/0)
                    numbers = re.findall(r'\d+', line)
                    if len(numbers) >= 6:
                        stats['tcp_total'] = int(numbers[0])
                        stats['tcp_established'] = int(numbers[1])
                        stats['tcp_closed'] = int(numbers[2])
                        stats['tcp_orphaned'] = int(numbers[3])
                        stats['tcp_synrecv'] = int(numbers[4])
                        stats['tcp_timewait'] = int(numbers[5])
        
        except Exception as e:
            print(f"Error getting TCP stats: {e}")
        
        try:
            # Статистика netstat
            netstat_output = subprocess.check_output(['netstat', '-s'], text=True)
            
            tcp_section = False
            for line in netstat_output.split('\n'):
                line = line.strip()
                
                if line.startswith('Tcp:'):
                    tcp_section = True
                    continue
                elif line.startswith(('Udp:', 'Icmp:', 'Ip:')):
                    tcp_section = False
                    continue
                
                if tcp_section and line:
                    # Парсим различные TCP метрики
                    if 'segments received' in line:
                        stats['tcp_segments_received'] = int(re.search(r'(\d+)', line).group(1))
                    elif 'segments sent out' in line:
                        stats['tcp_segments_sent'] = int(re.search(r'(\d+)', line).group(1))
                    elif 'bad segments received' in line:
                        stats['tcp_bad_segments'] = int(re.search(r'(\d+)', line).group(1))
                    elif 'segments retransmitted' in line:
                        stats['tcp_retransmissions'] = int(re.search(r'(\d+)', line).group(1))
                    elif 'connections established' in line:
                        stats['tcp_connections_established'] = int(re.search(r'(\d+)', line).group(1))
                    elif 'failed connection attempts' in line:
                        stats['tcp_failed_connections'] = int(re.search(r'(\d+)', line).group(1))
        
        except Exception as e:
            print(f"Error getting netstat data: {e}")
        
        # Добавляем системные метрики
        try:
            # Информация о буферах
            with open('/proc/sys/net/core/rmem_max', 'r') as f:
                stats['rmem_max'] = int(f.read().strip())
            
            with open('/proc/sys/net/core/wmem_max', 'r') as f:
                stats['wmem_max'] = int(f.read().strip())
            
            # TCP congestion control
            with open('/proc/sys/net/ipv4/tcp_congestion_control', 'r') as f:
                stats['congestion_control'] = f.read().strip()
        
        except Exception as e:
            print(f"Error reading system TCP settings: {e}")
        
        stats['timestamp'] = time.time()
        return stats
    
    def calculate_tcp_efficiency(self, stats: Dict) -> Dict:
        """Вычисление эффективности TCP"""
        
        efficiency = {}
        
        # Коэффициент ретрансмиссии
        if 'tcp_segments_sent' in stats and 'tcp_retransmissions' in stats:
            if stats['tcp_segments_sent'] > 0:
                retrans_rate = (stats['tcp_retransmissions'] / stats['tcp_segments_sent']) * 100
                efficiency['retransmission_rate_percent'] = retrans_rate
                
                # Оценка качества сети
                if retrans_rate < 0.1:
                    efficiency['network_quality'] = 'Excellent'
                elif retrans_rate < 0.5:
                    efficiency['network_quality'] = 'Good'
                elif retrans_rate < 1.0:
                    efficiency['network_quality'] = 'Fair'
                else:
                    efficiency['network_quality'] = 'Poor'
        
        # Коэффициент успешных соединений
        if 'tcp_connections_established' in stats and 'tcp_failed_connections' in stats:
            total_attempts = stats['tcp_connections_established'] + stats['tcp_failed_connections']
            if total_attempts > 0:
                success_rate = (stats['tcp_connections_established'] / total_attempts) * 100
                efficiency['connection_success_rate_percent'] = success_rate
        
        # Использование портов
        if 'tcp_established' in stats:
            # Предполагаем максимум 65535 портов
            port_utilization = (stats['tcp_established'] / 65535) * 100
            efficiency['port_utilization_percent'] = port_utilization
            
            if port_utilization > 80:
                efficiency['port_warning'] = 'High port utilization - consider port reuse optimization'
        
        # TIME_WAIT анализ
        if 'tcp_timewait' in stats and 'tcp_established' in stats:
            if stats['tcp_established'] > 0:
                timewait_ratio = stats['tcp_timewait'] / stats['tcp_established']
                efficiency['timewait_ratio'] = timewait_ratio
                
                if timewait_ratio > 0.5:
                    efficiency['timewait_warning'] = 'High TIME_WAIT ratio - consider tcp_tw_reuse'
        
        return efficiency
    
    def monitor_tcp_performance(self, duration_minutes: int = 5):
        """Мониторинг TCP производительности в течение времени"""
        
        print(f"Starting TCP performance monitoring for {duration_minutes} minutes...")
        
        start_time = time.time()
        end_time = start_time + (duration_minutes * 60)
        
        baseline_stats = self.get_tcp_stats()
        print("Baseline TCP stats collected")
        
        while time.time() < end_time:
            current_stats = self.get_tcp_stats()
            efficiency = self.calculate_tcp_efficiency(current_stats)
            
            # Вычисляем дельту с baseline
            delta_stats = {}
            for key in current_stats:
                if key in baseline_stats and isinstance(current_stats[key], (int, float)):
                    delta_stats[f'delta_{key}'] = current_stats[key] - baseline_stats[key]
            
            monitoring_record = {
                'timestamp': current_stats['timestamp'],
                'current_stats': current_stats,
                'efficiency': efficiency,
                'delta_from_baseline': delta_stats
            }
            
            self.metrics_history.append(monitoring_record)
            
            # Выводим ключевые метрики
            print(f"\n--- {time.strftime('%H:%M:%S')} ---")
            print(f"Established connections: {current_stats.get('tcp_established', 'N/A')}")
            print(f"TIME_WAIT connections: {current_stats.get('tcp_timewait', 'N/A')}")
            
            if 'retransmission_rate_percent' in efficiency:
                print(f"Retransmission rate: {efficiency['retransmission_rate_percent']:.3f}%")
            
            if 'connection_success_rate_percent' in efficiency:
                print(f"Connection success rate: {efficiency['connection_success_rate_percent']:.1f}%")
            
            # Предупреждения
            for key, value in efficiency.items():
                if 'warning' in key:
                    print(f"⚠️ {value}")
            
            time.sleep(30)  # Проверяем каждые 30 секунд
        
        return self.metrics_history
    
    def generate_tcp_report(self) -> Dict:
        """Генерация отчета по TCP производительности"""
        
        if not self.metrics_history:
            return {'error': 'No metrics data available'}
        
        # Анализируем тренды
        report = {
            'monitoring_period': {
                'start': min(record['timestamp'] for record in self.metrics_history),
                'end': max(record['timestamp'] for record in self.metrics_history),
                'duration_minutes': len(self.metrics_history) * 0.5  # 30 сек интервалы
            },
            'performance_summary': {},
            'trends': {},
            'recommendations': []
        }
        
        # Извлекаем метрики для анализа
        established_counts = [r['current_stats'].get('tcp_established', 0) for r in self.metrics_history]
        timewait_counts = [r['current_stats'].get('tcp_timewait', 0) for r in self.metrics_history]
        retrans_rates = [r['efficiency'].get('retransmission_rate_percent', 0) for r in self.metrics_history if 'retransmission_rate_percent' in r['efficiency']]
        
        # Статистика по соединениям
        if established_counts:
            report['performance_summary']['connections'] = {
                'avg_established': statistics.mean(established_counts),
                'max_established': max(established_counts),
                'min_established': min(established_counts)
            }
        
        if timewait_counts:
            report['performance_summary']['timewait'] = {
                'avg_timewait': statistics.mean(timewait_counts),
                'max_timewait': max(timewait_counts)
            }
        
        if retrans_rates:
            avg_retrans = statistics.mean(retrans_rates)
            report['performance_summary']['retransmission_rate_percent'] = avg_retrans
            
            # Рекомендации по ретрансмиссиям
            if avg_retrans > 1.0:
                report['recommendations'].append("High retransmission rate detected. Check network quality and congestion control settings.")
            elif avg_retrans > 0.5:
                report['recommendations'].append("Moderate retransmission rate. Monitor network conditions.")
        
        # Тренды
        if len(established_counts) > 1:
            # Простой тренд анализ
            first_half = established_counts[:len(established_counts)//2]
            second_half = established_counts[len(established_counts)//2:]
            
            avg_first = statistics.mean(first_half)
            avg_second = statistics.mean(second_half)
            
            if avg_second > avg_first * 1.2:
                report['trends']['connections'] = 'Increasing significantly'
                report['recommendations'].append("Connection count is growing. Monitor resource usage.")
            elif avg_second > avg_first * 1.05:
                report['trends']['connections'] = 'Slightly increasing'
            else:
                report['trends']['connections'] = 'Stable'
        
        # Общие рекомендации
        last_stats = self.metrics_history[-1]['current_stats']
        
        if last_stats.get('tcp_timewait', 0) > 1000:
            report['recommendations'].append("High TIME_WAIT count. Consider enabling tcp_tw_reuse.")
        
        if last_stats.get('tcp_established', 0) > 10000:
            report['recommendations'].append("High connection count. Ensure proper connection pooling.")
        
        return report

# Практическое использование TCP мониторинга
tcp_monitor = TCPPerformanceMonitor()

print("🔍 TCP Performance Analysis Suite")
print("=" * 50)

# Получаем текущую статистику
current_stats = tcp_monitor.get_tcp_stats()
efficiency = tcp_monitor.calculate_tcp_efficiency(current_stats)

print("📊 Current TCP Statistics:")
print(f"  Established connections: {current_stats.get('tcp_established', 'N/A')}")
print(f"  TIME_WAIT connections: {current_stats.get('tcp_timewait', 'N/A')}")
print(f"  Orphaned connections: {current_stats.get('tcp_orphaned', 'N/A')}")
print(f"  Total segments sent: {current_stats.get('tcp_segments_sent', 'N/A')}")
print(f"  Retransmissions: {current_stats.get('tcp_retransmissions', 'N/A')}")

print("\n⚡ TCP Efficiency Metrics:")
for key, value in efficiency.items():
    if 'warning' not in key:
        print(f"  {key}: {value}")

print("\n⚠️ Warnings:")
for key, value in efficiency.items():
    if 'warning' in key:
        print(f"  {value}")

print("\n🔧 Current TCP Configuration:")
print(f"  Max receive buffer: {current_stats.get('rmem_max', 'N/A')} bytes")
print(f"  Max send buffer: {current_stats.get('wmem_max', 'N/A')} bytes")
print(f"  Congestion control: {current_stats.get('congestion_control', 'N/A')}")

# Запуск мониторинга на 2 минуты для демонстрации
print("\n🕐 Starting 2-minute TCP monitoring...")
metrics_data = tcp_monitor.monitor_tcp_performance(2)

# Генерация отчета
print("\n📋 Generating TCP Performance Report...")
report = tcp_monitor.generate_tcp_report()

print("\n📊 TCP Performance Report:")
print(json.dumps(report, indent=2, default=str))

# Сохранение данных для анализа
report_file = f"tcp_performance_report_{int(time.time())}.json"
with open(report_file, 'w') as f:
    json.dump({
        'report': report,
        'raw_metrics': metrics_data
    }, f, indent=2, default=str)

print(f"\n💾 Detailed report saved to: {report_file}")
```

### 🔧 Application-Level Performance Optimization

**HTTP Client оптимизация с connection pooling:**

```python
import aiohttp
import asyncio
import time
import ssl
from aiohttp import TCPConnector
from typing import List, Dict, Optional
import statistics

class OptimizedHTTPClient:
    def __init__(self, max_connections: int = 100, max_connections_per_host: int = 30):
        """
        Оптимизированный HTTP клиент для высокой производительности
        
        Args:
            max_connections: Общий лимит соединений
            max_connections_per_host: Лимит соединений на хост
        """
        
        # SSL контекст для HTTPS соединений
        ssl_context = ssl.create_default_context()
        ssl_context.set_ciphers('ECDHE+AESGCM:ECDHE+CHACHA20:DHE+AESGCM:DHE+CHACHA20:!aNULL:!SHA1:!WEAK')
        
        # TCP коннектор с оптимизациями
        self.connector = TCPConnector(
            limit=max_connections,              # Общий лимит соединений
            limit_per_host=max_connections_per_host,  # Лимит на хост
            ttl_dns_cache=300,                  # DNS кеш на 5 минут
            use_dns_cache=True,                 # Использовать DNS кеш
            ssl=ssl_context,                    # SSL контекст
            keepalive_timeout=30,               # Keep-alive таймаут
            enable_cleanup_closed=True,         # Автоочистка закрытых соединений
            force_close=False,                  # Не форсировать закрытие
            resolver=aiohttp.AsyncResolver()    # Асинхронный DNS resolver
        )
        
        # Таймауты для разных операций
        self.timeout = aiohttp.ClientTimeout(
            total=30,           # Общий таймаут запроса
            connect=10,         # Таймаут установки соединения
            sock_read=10,       # Таймаут чтения из сокета
            sock_connect=10     # Таймаут подключения сокета
        )
        
        # Метрики производительности
        self.metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'response_times': [],
            'connection_reuse_count': 0,
            'dns_lookup_times': [],
            'connection_times': [],
            'ssl_handshake_times': []
        }
        
        self.session = None
    
    async def __aenter__(self):
        """Async context manager entry"""
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=self.timeout,
            headers={
                'User-Agent': 'OptimizedHTTPClient/1.0',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        if self.session:
            await self.session.close()
    
    async def get(self, url: str, headers: Dict = None, **kwargs) -> Dict:
        """Оптимизированный GET запрос с метриками"""
        return await self._request('GET', url, headers=headers, **kwargs)
    
    async def post(self, url: str, json_data: Dict = None, headers: Dict = None, **kwargs) -> Dict:
        """Оптимизированный POST запрос с метриками"""
        return await self._request('POST', url, json=json_data, headers=headers, **kwargs)
    
    async def _request(self, method: str, url: str, **kwargs) -> Dict:
        """Внутренний метод для выполнения запросов с детальными метриками"""
        
        start_time = time.time()
        
        # Подготовка заголовков
        headers = kwargs.get('headers', {})
        
        # Добавляем заголовки для оптимизации
        default_headers = {
            'Accept': 'application/json',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }
        
        for key, value in default_headers.items():
            if key not in headers:
                headers[key] = value
        
        kwargs['headers'] = headers
        
        try:
            # Выполняем запрос
            async with self.session.request(method, url, **kwargs) as response:
                
                # Читаем тело ответа
                if response.content_type == 'application/json':
                    response_data = await response.json()
                else:
                    response_data = await response.text()
                
                end_time = time.time()
                response_time = (end_time - start_time) * 1000  # в мс
                
                # Обновляем метрики
                self.metrics['total_requests'] += 1
                self.metrics['response_times'].append(response_time)
                
                if 200 <= response.status < 300:
                    self.metrics['successful_requests'] += 1
                else:
                    self.metrics['failed_requests'] += 1
                
                return {
                    'status': response.status,
                    'headers': dict(response.headers),
                    'data': response_data,
                    'response_time_ms': response_time,
                    'url': str(response.url),
                    'method': method,
                    'success': 200 <= response.status < 300
                }
        
        except asyncio.TimeoutError:
            end_time = time.time()
            response_time = (end_time - start_time) * 1000
            
            self.metrics['total_requests'] += 1
            self.metrics['failed_requests'] += 1
            
            return {
                'status': 0,
                'error': 'Request timeout',
                'response_time_ms': response_time,
                'url': url,
                'method': method,
                'success': False
            }
        
        except Exception as e:
            end_time = time.time()
            response_time = (end_time - start_time) * 1000
            
            self.metrics['total_requests'] += 1
            self.metrics['failed_requests'] += 1
            
            return {
                'status': 0,
                'error': str(e),
                'response_time_ms': response_time,
                'url': url,
                'method': method,
                'success': False
            }
    
    async def batch_requests(self, requests: List[Dict], max_concurrent: int = 50) -> List[Dict]:
        """Выполнение пакета запросов с ограничением конкурентности"""
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def bounded_request(request_config):
            async with semaphore:
                method = request_config.get('method', 'GET')
                url = request_config['url']
                kwargs = {k: v for k, v in request_config.items() if k not in ['method', 'url']}
                
                return await self._request(method, url, **kwargs)
        
        # Выполняем все запросы параллельно
        tasks = [bounded_request(req) for req in requests]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Обрабатываем исключения
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                processed_results.append({
                    'status': 0,
                    'error': str(result),
                    'success': False
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    def get_performance_stats(self) -> Dict:
        """Получение статистики производительности"""
        
        if not self.metrics['response_times']:
            return {'error': 'No requests made yet'}
        
        response_times = self.metrics['response_times']
        
        stats = {
            'total_requests': self.metrics['total_requests'],
            'successful_requests': self.metrics['successful_requests'],
            'failed_requests': self.metrics['failed_requests'],
            'success_rate_percent': (self.metrics['successful_requests'] / self.metrics['total_requests'] * 100) if self.metrics['total_requests'] > 0 else 0,
            'response_time_stats': {
                'min_ms': min(response_times),
                'max_ms': max(response_times),
                'avg_ms': statistics.mean(response_times),
                'median_ms': statistics.median(response_times),
                'p95_ms': statistics.quantiles(response_times, n=20)[18] if len(response_times) > 1 else response_times[0],
                'p99_ms': statistics.quantiles(response_times, n=100)[98] if len(response_times) > 1 else response_times[0]
            },
            'connection_pool_stats': {
                'total_connections': self.connector._limit,
                'connections_per_host': self.connector._limit_per_host,
                'dns_cache_enabled': self.connector._use_dns_cache,
                'keepalive_timeout': self.connector._keepalive_timeout
            }
        }
        
        return stats

# Практический пример использования
async def performance_benchmark():
    """Бенчмарк производительности HTTP клиента"""
    
    # URLs для тестирования
    test_urls = [
        'https://httpbin.org/get',
        'https://httpbin.org/delay/1',
        'https://httpbin.org/status/200',
        'https://httpbin.org/json',
        'https://httpbin.org/gzip'
    ] * 20  # 100 запросов всего
    
    print("🚀 Starting HTTP Client Performance Benchmark")
    print("=" * 60)
    
    # Тест с оптимизированным клиентом
    print("Testing optimized HTTP client...")
    
    async with OptimizedHTTPClient(max_connections=50, max_connections_per_host=10) as client:
        
        start_time = time.time()
        
        # Подготавливаем запросы
        requests = [{'method': 'GET', 'url': url} for url in test_urls]
        
        # Выполняем пакет запросов
        results = await client.batch_requests(requests, max_concurrent=25)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # Анализируем результаты
        successful_results = [r for r in results if r.get('success', False)]
        failed_results = [r for r in results if not r.get('success', False)]
        
        print(f"\n📊 Benchmark Results:")
        print(f"  Total time: {total_time:.2f} seconds")
        print(f"  Total requests: {len(results)}")
        print(f"  Successful: {len(successful_results)}")
        print(f"  Failed: {len(failed_results)}")
        print(f"  Requests per second: {len(results) / total_time:.1f}")
        
        # Статистика клиента
        stats = client.get_performance_stats()
        print(f"\n⚡ Performance Statistics:")
        print(f"  Success rate: {stats['success_rate_percent']:.1f}%")
        print(f"  Average response time: {stats['response_time_stats']['avg_ms']:.1f}ms")
        print(f"  P95 response time: {stats['response_time_stats']['p95_ms']:.1f}ms")
        print(f"  P99 response time: {stats['response_time_stats']['p99_ms']:.1f}ms")
        
        # Показываем ошибки если есть
        if failed_results:
            print(f"\n❌ Failed Requests:")
            error_types = {}
            for result in failed_results:
                error = result.get('error', 'Unknown error')
                error_types[error] = error_types.get(error, 0) + 1
            
            for error, count in error_types.items():
                print(f"  {error}: {count} times")
    
    print("\n✅ Benchmark completed!")

# Запуск бенчмарка
if __name__ == "__main__":
    asyncio.run(performance_benchmark())
```

### 📝 Практическое задание Неделя 9

1. Проведите comprehensive анализ сетевой производительности ваших ключевых сервисов
2. Оптимизируйте TCP параметры на production серверах
3. Реализуйте мониторинг TCP эффективности с алертами
4. Оптимизируйте HTTP клиенты с connection pooling
5. Создайте бенчмарк тесты для измерения improvement

---

## Неделя 10: Оптимизация соединений

### 🧠 Концепция: Connection Lifecycle Management

Управление жизненным циклом соединений - ключ к масштабируемости:

```
Connection Lifecycle:
┌─────────────────────────────────────────────────────────────┐
│ 1. Creation (Expensive)                                     │
│    ├─ DNS Resolution (50-200ms)                            │
│    ├─ TCP Handshake (RTT)                                  │
│    ├─ TLS Handshake (2-3 RTT)                             │
│    └─ Application Protocol Setup                           │
├─────────────────────────────────────────────────────────────┤
│ 2. Usage (Cheap)                                           │
│    ├─ Request/Response cycles                              │
│    ├─ Keep-alive maintenance                               │
│    └─ Connection validation                                │
├─────────────────────────────────────────────────────────────┤
│ 3. Management (Critical)                                   │
│    ├─ Pool size optimization                               │
│    ├─ Health checking                                      │
│    ├─ Load balancing                                       │
│    └─ Timeout handling                                     │
├─────────────────────────────────────────────────────────────┤
│ 4. Destruction (Controlled)                                │
│    ├─ Graceful shutdown                                    │
│    ├─ Resource cleanup                                     │
│    └─ Connection draining                                  │
└─────────────────────────────────────────────────────────────┘
```

### 🏊‍♂️ Advanced Connection Pooling

**Enterprise-grade Connection Pool Implementation:**

```python
import asyncio
import time
import logging
import weakref
from typing import Dict, List, Optional, Set, Callable, Any
from dataclasses import dataclass, field
from enum import Enum
import aiohttp
import statistics
from contextlib import asynccontextmanager

class ConnectionState(Enum):
    CREATING = "creating"
    IDLE = "idle"
    BUSY = "busy"
    STALE = "stale"
    UNHEALTHY = "unhealthy"
    CLOSING = "closing"

@dataclass
class ConnectionMetrics:
    created_at: float
    last_used_at: float
    total_requests: int = 0
    failed_requests: int = 0
    avg_response_time: float = 0.0
    state: ConnectionState = ConnectionState.IDLE
    health_score: float = 1.0  # 1.0 = полностью здоров, 0.0 = нездоров

@dataclass
class PoolConfig:
    min_size: int = 5
    max_size: int = 50
    max_idle_time: float = 300.0  # 5 минут
    max_lifetime: float = 3600.0  # 1 час
    health_check_interval: float = 60.0  # 1 минута
    max_requests_per_connection: int = 1000
    connection_timeout: float = 10.0
    request_timeout: float = 30.0
    retry_attempts: int = 3
    backoff_factor: float = 0.3

class AdvancedConnectionPool:
    def __init__(self, 
                 target_host: str, 
                 target_port: int = 443,
                 config: PoolConfig = None,
                 health_check_callback: Callable = None):
        
        self.target_host = target_host
        self.target_port = target_port
        self.config = config or PoolConfig()
        self.health_check_callback = health_check_callback
        
        # Connection storage
        self._connections: Dict[str, aiohttp.ClientSession] = {}
        self._connection_metrics: Dict[str, ConnectionMetrics] = {}
        self._connection_semaphore = asyncio.Semaphore(self.config.max_size)
        
        # State management
        self._creating_connections: Set[str] = set()
        self._busy_connections: Set[str] = set()
        self._idle_connections: Set[str] = set()
        
        # Background tasks
        self._maintenance_task: Optional[asyncio.Task] = None
        self._health_check_task: Optional[asyncio.Task] = None
        self._closed = False
        
        # Metrics
        self.pool_metrics = {
            'total_connections_created': 0,
            'total_connections_destroyed': 0,
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'pool_exhaustions': 0,
            'health_check_failures': 0
        }
        
        # Logger
        self.logger = logging.getLogger(f'ConnectionPool.{target_host}')
        
    async def start(self):
        """Запуск пула соединений"""
        if self._maintenance_task is not None:
            return
        
        self.logger.info(f"Starting connection pool for {self.target_host}:{self.target_port}")
        
        # Создаем минимальное количество соединений
        await self._ensure_min_connections()
        
        # Запускаем фоновые задачи
        self._maintenance_task = asyncio.create_task(self._maintenance_loop())
        self._health_check_task = asyncio.create_task(self._health_check_loop())
        
        self.logger.info(f"Connection pool started with {len(self._connections)} connections")
    
    async def stop(self):
        """Остановка пула соединений"""
        if self._closed:
            return
        
        self._closed = True
        self.logger.info("Shutting down connection pool...")
        
        # Останавливаем фоновые задачи
        if self._maintenance_task:
            self._maintenance_task.cancel()
            try:
                await self._maintenance_task
            except asyncio.CancelledError:
                pass
        
        if self._health_check_task:
            self._health_check_task.cancel()
            try:
                await self._health_check_task
            except asyncio.CancelledError:
                pass
        
        # Закрываем все соединения
        for conn_id in list(self._connections.keys()):
            await self._destroy_connection(conn_id)
        
        self.logger.info("Connection pool shut down complete")
    
    @asynccontextmanager
    async def get_connection(self):
        """Получение соединения из пула с автоматическим возвратом"""
        if self._closed:
            raise RuntimeError("Connection pool is closed")
        
        connection = await self._acquire_connection()
        try:
            yield connection
        finally:
            await self._release_connection(connection)
    
    async def _acquire_connection(self) -> aiohttp.ClientSession:
        """Получение соединения из пула"""
        
        # Ждем доступности слота в пуле
        await self._connection_semaphore.acquire()
        
        try:
            # Пытаемся получить существующее idle соединение
            if self._idle_connections:
                conn_id = self._idle_connections.pop()
                connection = self._connections[conn_id]
                
                # Проверяем здоровье соединения
                if await self._is_connection_healthy(conn_id):
                    self._busy_connections.add(conn_id)
                    self._connection_metrics[conn_id].last_used_at = time.time()
                    self.logger.debug(f"Reusing existing connection {conn_id}")
                    return connection
                else:
                    # Соединение нездорово, уничтожаем его
                    await self._destroy_connection(conn_id)
            
            # Создаем новое соединение если нет здоровых idle
            if len(self._connections) < self.config.max_size:
                conn_id = await self._create_connection()
                self._busy_connections.add(conn_id)
                return self._connections[conn_id]
            
            # Пул исчерпан, ждем освобождения соединения
            self.pool_metrics['pool_exhaustions'] += 1
            self.logger.warning("Connection pool exhausted, waiting for available connection")
            
            # Можно реализовать ожидание или выбросить исключение
            raise RuntimeError("Connection pool exhausted")
            
        except Exception:
            self._connection_semaphore.release()
            raise
    
    async def _release_connection(self, connection: aiohttp.ClientSession):
        """Возврат соединения в пул"""
        
        # Находим connection ID
        conn_id = None
        for cid, conn in self._connections.items():
            if conn is connection:
                conn_id = cid
                break
        
        if conn_id is None:
            self.logger.error("Attempted to release unknown connection")
            self._connection_semaphore.release()
            return
        
        # Перемещаем из busy в idle
        if conn_id in self._busy_connections:
            self._busy_connections.remove(conn_id)
            
            # Проверяем не превысил ли лимит запросов
            metrics = self._connection_metrics[conn_id]
            if metrics.total_requests >= self.config.max_requests_per_connection:
                self.logger.info(f"Connection {conn_id} reached request limit, destroying")
                await self._destroy_connection(conn_id)
            else:
                self._idle_connections.add(conn_id)
                metrics.state = ConnectionState.IDLE
        
        self._connection_semaphore.release()
    
    async def _create_connection(self) -> str:
        """Создание нового соединения"""
        
        conn_id = f"{self.target_host}:{self.target_port}:{time.time()}"
        self._creating_connections.add(conn_id)
        
        try:
            self.logger.debug(f"Creating new connection {conn_id}")
            
            # Создаем SSL контекст
            import ssl
            ssl_context = ssl.create_default_context()
            
            # Создаем TCP коннектор
            connector = aiohttp.TCPConnector(
                limit=1,  # Одно соединение на коннектор
                ssl=ssl_context,
                keepalive_timeout=self.config.max_idle_time,
                enable_cleanup_closed=True
            )
            
            # Создаем сессию
            timeout = aiohttp.ClientTimeout(
                total=self.config.request_timeout,
                connect=self.config.connection_timeout
            )
            
            session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={
                    'User-Agent': 'AdvancedConnectionPool/1.0',
                    'Connection': 'keep-alive'
                }
            )
            
            # Тестируем соединение
            test_url = f"https://{self.target_host}:{self.target_port}/health"
            try:
                async with session.get(test_url) as response:
                    if response.status < 500:  # Считаем успешным любой не-server error
                        pass
            except Exception as e:
                self.logger.debug(f"Health check during creation failed (normal): {e}")
            
            # Сохраняем соединение
            self._connections[conn_id] = session
            self._connection_metrics[conn_id] = ConnectionMetrics(
                created_at=time.time(),
                last_used_at=time.time(),
                state=ConnectionState.IDLE
            )
            
            self.pool_metrics['total_connections_created'] += 1
            self.logger.info(f"Created new connection {conn_id}")
            
            return conn_id
            
        except Exception as e:
            self.logger.error(f"Failed to create connection {conn_id}: {e}")
            raise
        finally:
            self._creating_connections.discard(conn_id)
    
    async def _destroy_connection(self, conn_id: str):
        """Уничтожение соединения"""
        
        if conn_id not in self._connections:
            return
        
        self.logger.debug(f"Destroying connection {conn_id}")
        
        # Закрываем сессию
        session = self._connections[conn_id]
        if not session.closed:
            await session.close()
        
        # Удаляем из всех коллекций
        self._connections.pop(conn_id, None)
        self._connection_metrics.pop(conn_id, None)
        self._busy_connections.discard(conn_id)
        self._idle_connections.discard(conn_id)
        self._creating_connections.discard(conn_id)
        
        self.pool_metrics['total_connections_destroyed'] += 1
        self.logger.debug(f"Destroyed connection {conn_id}")
    
    async def _is_connection_healthy(self, conn_id: str) -> bool:
        """Проверка здоровья соединения"""
        
        if conn_id not in self._connections:
            return False
        
        metrics = self._connection_metrics[conn_id]
        session = self._connections[conn_id]
        
        # Проверяем базовые условия
        if session.closed:
            return False
        
        # Проверяем возраст соединения
        if time.time() - metrics.created_at > self.config.max_lifetime:
            self.logger.debug(f"Connection {conn_id} exceeded max lifetime")
            return False
        
        # Проверяем время бездействия
        if time.time() - metrics.last_used_at > self.config.max_idle_time:
            self.logger.debug(f"Connection {conn_id} exceeded max idle time")
            return False
        
        # Проверяем health score
        if metrics.health_score < 0.5:
            self.logger.debug(f"Connection {conn_id} has low health score: {metrics.health_score}")
            return False
        
        # Дополнительная проверка через callback
        if self.health_check_callback:
            try:
                is_healthy = await self.health_check_callback(session)
                if not is_healthy:
                    metrics.health_score *= 0.8  # Снижаем health score
                    return False
            except Exception as e:
                self.logger.debug(f"Health check callback failed for {conn_id}: {e}")
                metrics.health_score *= 0.7
                return False
        
        return True
    
    async def _ensure_min_connections(self):
        """Обеспечение минимального количества соединений"""
        
        current_count = len(self._connections)
        if current_count < self.config.min_size:
            needed = self.config.min_size - current_count
            
            self.logger.info(f"Creating {needed} connections to reach minimum pool size")
            
            # Создаем соединения параллельно
            tasks = []
            for _ in range(needed):
                if len(self._connections) + len(tasks) < self.config.max_size:
                    task = asyncio.create_task(self._create_connection())
                    tasks.append(task)
            
            if tasks:
                created_conn_ids = await asyncio.gather(*tasks, return_exceptions=True)
                
                # Добавляем успешно созданные соединения в idle пул
                for conn_id in created_conn_ids:
                    if isinstance(conn_id, str) and conn_id in self._connections:
                        self._idle_connections.add(conn_id)
    
    async def _maintenance_loop(self):
        """Фоновая задача обслуживания пула"""
        
        self.logger.info("Started maintenance loop")
        
        while not self._closed:
            try:
                await asyncio.sleep(30)  # Проверяем каждые 30 секунд
                
                if self._closed:
                    break
                
                # Удаляем устаревшие соединения
                stale_connections = []
                current_time = time.time()
                
                for conn_id, metrics in self._connection_metrics.items():
                    if (current_time - metrics.last_used_at > self.config.max_idle_time or
                        current_time - metrics.created_at > self.config.max_lifetime or
                        metrics.health_score < 0.3):
                        stale_connections.append(conn_id)
                
                # Удаляем stale соединения (но не ниже минимума)
                for conn_id in stale_connections:
                    if len(self._connections) > self.config.min_size:
                        await self._destroy_connection(conn_id)
                
                # Обеспечиваем минимальное количество соединений
                await self._ensure_min_connections()
                
                self.logger.debug(f"Maintenance: {len(self._connections)} total, "
                                f"{len(self._idle_connections)} idle, "
                                f"{len(self._busy_connections)} busy")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Error in maintenance loop: {e}")
        
        self.logger.info("Maintenance loop stopped")
    
    async def _health_check_loop(self):
        """Фоновая задача проверки здоровья соединений"""
        
        self.logger.info("Started health check loop")
        
        while not self._closed:
            try:
                await asyncio.sleep(self.config.health_check_interval)
                
                if self._closed:
                    break
                
                # Проверяем здоровье idle соединений
                unhealthy_connections = []
                
                for conn_id in list(self._idle_connections):
                    if not await self._is_connection_healthy(conn_id):
                        unhealthy_connections.append(conn_id)
                        self.pool_metrics['health_check_failures'] += 1
                
                # Удаляем нездоровые соединения
                for conn_id in unhealthy_connections:
                    await self._destroy_connection(conn_id)
                
                if unhealthy_connections:
                    self.logger.info(f"Health check removed {len(unhealthy_connections)} unhealthy connections")
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"Error in health check loop: {e}")
        
        self.logger.info("Health check loop stopped")
    
    def get_pool_stats(self) -> Dict:
        """Получение статистики пула"""
        
        return {
            'pool_config': {
                'min_size': self.config.min_size,
                'max_size': self.config.max_size,
                'max_idle_time': self.config.max_idle_time,
                'max_lifetime': self.config.max_lifetime
            },
            'current_state': {
                'total_connections': len(self._connections),
                'idle_connections': len(self._idle_connections),
                'busy_connections': len(self._busy_connections),
                'creating_connections': len(self._creating_connections)
            },
            'lifetime_metrics': dict(self.pool_metrics),
            'connection_details': {
                conn_id: {
                    'created_at': metrics.created_at,
                    'last_used_at': metrics.last_used_at,
                    'total_requests': metrics.total_requests,
                    'failed_requests': metrics.failed_requests,
                    'health_score': metrics.health_score,
                    'state': metrics.state.value,
                    'age_seconds': time.time() - metrics.created_at,
                    'idle_seconds': time.time() - metrics.last_used_at
                }
                for conn_id, metrics in self._connection_metrics.items()
            }
        }

# Пример использования Advanced Connection Pool
async def demonstrate_advanced_pool():
    """Демонстрация работы продвинутого пула соединений"""
    
    print("🏊‍♂️ Advanced Connection Pool Demonstration")
    print("=" * 60)
    
    # Создаем конфигурацию пула
    config = PoolConfig(
        min_size=3,
        max_size=10,
        max_idle_time=60.0,  # 1 минута для демонстрации
        max_lifetime=300.0,  # 5 минут для демонстрации
        health_check_interval=30.0,
        max_requests_per_connection=50
    )
    
    # Health check callback
    async def health_check(session: aiohttp.ClientSession) -> bool:
        try:
            async with session.get('https://httpbin.org/status/200', timeout=aiohttp.ClientTimeout(total=5)) as response:
                return response.status == 200
        except:
            return False
    
    # Создаем пул
    pool = AdvancedConnectionPool(
        target_host='httpbin.org',
        target_port=443,
        config=config,
        health_check_callback=health_check
    )
    
    try:
        # Запускаем пул
        await pool.start()
        print("✅ Connection pool started")
        
        # Демонстрируем использование пула
        print("\n🔄 Making requests through connection pool...")
        
        successful_requests = 0
        failed_requests = 0
        
        # Делаем серию запросов
        for i in range(20):
            try:
                async with pool.get_connection() as session:
                    async with session.get('https://httpbin.org/delay/0.5') as response:
                        if response.status == 200:
                            successful_requests += 1
                        else:
                            failed_requests += 1
                        
                        print(f"Request {i+1}: {response.status} ({response.content_length} bytes)")
                
            except Exception as e:
                failed_requests += 1
                print(f"Request {i+1}: ERROR - {e}")
            
            # Небольшая задержка между запросами
            if i % 5 == 4:
                await asyncio.sleep(1)
        
        print(f"\n📊 Request Results:")
        print(f"  Successful: {successful_requests}")
        print(f"  Failed: {failed_requests}")
        print(f"  Success rate: {successful_requests / (successful_requests + failed_requests) * 100:.1f}%")
        
        # Показываем статистику пула
        stats = pool.get_pool_stats()
        print(f"\n🏊‍♂️ Pool Statistics:")
        print(f"  Total connections: {stats['current_state']['total_connections']}")
        print(f"  Idle connections: {stats['current_state']['idle_connections']}")
        print(f"  Busy connections: {stats['current_state']['busy_connections']}")
        print(f"  Connections created: {stats['lifetime_metrics']['total_connections_created']}")
        print(f"  Connections destroyed: {stats['lifetime_metrics']['total_connections_destroyed']}")
        print(f"  Pool exhaustions: {stats['lifetime_metrics']['pool_exhaustions']}")
        
        print(f"\n🔍 Connection Details:")
        for conn_id, details in stats['connection_details'].items():
            print(f"  {conn_id[-20:]}: {details['state']} (age: {details['age_seconds']:.1f}s, "
                  f"requests: {details['total_requests']}, health: {details['health_score']:.2f})")
        
        # Тестируем поведение при высокой нагрузке
        print(f"\n⚡ High Load Test (50 concurrent requests)...")
        
        async def make_request(request_id):
            try:
                async with pool.get_connection() as session:
                    async with session.get('https://httpbin.org/delay/1') as response:
                        return {'id': request_id, 'status': response.status, 'success': True}
            except Exception as e:
                return {'id': request_id, 'error': str(e), 'success': False}
        
        # Запускаем 50 параллельных запросов
        start_time = time.time()
        tasks = [make_request(i) for i in range(50)]
        results = await asyncio.gather(*tasks)
        end_time = time.time()
        
        successful_concurrent = len([r for r in results if r.get('success')])
        failed_concurrent = len([r for r in results if not r.get('success')])
        
        print(f"  Total time: {end_time - start_time:.2f} seconds")
        print(f"  Successful: {successful_concurrent}")
        print(f"  Failed: {failed_concurrent}")
        print(f"  Requests per second: {len(results) / (end_time - start_time):.1f}")
        
        # Финальная статистика пула
        final_stats = pool.get_pool_stats()
        print(f"\n📈 Final Pool Metrics:")
        print(f"  Pool exhaustions: {final_stats['lifetime_metrics']['pool_exhaustions']}")
        print(f"  Health check failures: {final_stats['lifetime_metrics']['health_check_failures']}")
        print(f"  Total connections ever created: {final_stats['lifetime_metrics']['total_connections_created']}")
        
    finally:
        # Останавливаем пул
        await pool.stop()
        print("\n🛑 Connection pool stopped")

# Запуск демонстрации
if __name__ == "__main__":
    asyncio.run(demonstrate_advanced_pool())
```

### 🔄 Circuit Breaker Pattern для Network Resilience

**Реализация Circuit Breaker для сетевых вызовов:**

```python
import asyncio
import time
import logging
from enum import Enum
from typing import Callable, Any, Dict, Optional
from dataclasses import dataclass
import statistics

class CircuitState(Enum):
    CLOSED = "closed"        # Нормальная работа
    OPEN = "open"           # Блокирует вызовы
    HALF_OPEN = "half_open" # Тестирует восстановление

@dataclass
class CircuitBreakerConfig:
    failure_threshold: int = 5          # Количество ошибок для открытия
    recovery_timeout: float = 60.0      # Время до попытки восстановления
    success_threshold: int = 3          # Успехов для закрытия в half-open
    timeout: float = 30.0               # Таймаут вызова
    expected_exception: type = Exception # Тип исключений для учета

class CircuitBreakerOpenException(Exception):
    """Исключение когда Circuit Breaker открыт"""
    pass

class NetworkCircuitBreaker:
    def __init__(self, name: str, config: CircuitBreakerConfig = None):
        self.name = name
        self.config = config or CircuitBreakerConfig()
        
        # State management
        self._state = CircuitState.CLOSED
        self._failure_count = 0
        self._success_count = 0
        self._last_failure_time = 0
        self._last_success_time = 0
        
        # Metrics
        self._metrics = {
            'total_calls': 0,
            'successful_calls': 0,
            'failed_calls': 0,
            'circuit_opened_count': 0,
            'circuit_closed_count': 0,
            'calls_rejected': 0,
            'response_times': []
        }
        
        # Logger
        self.logger = logging.getLogger(f'CircuitBreaker.{name}')
    
    async def __call__(self, func: Callable, *args, **kwargs) -> Any:
        """Выполнение функции через Circuit Breaker"""
        
        self._metrics['total_calls'] += 1
        
        # Проверяем состояние Circuit Breaker
        if self._state == CircuitState.OPEN:
            if self._should_attempt_reset():
                self._state = CircuitState.HALF_OPEN
                self.logger.info(f"Circuit breaker {self.name} entering HALF_OPEN state")
            else:
                self._metrics['calls_rejected'] += 1
                raise CircuitBreakerOpenException(
                    f"Circuit breaker {self.name} is OPEN. "
                    f"Last failure: {time.time() - self._last_failure_time:.1f}s ago"
                )
        
        # Выполняем функцию с измерением времени
        start_time = time.time()
        
        try:
            # Выполняем с таймаутом
            result = await asyncio.wait_for(
                func(*args, **kwargs),
                timeout=self.config.timeout
            )
            
            # Успешный вызов
            end_time = time.time()
            response_time = (end_time - start_time) * 1000  # в мс
            
            self._on_success(response_time)
            return result
            
        except asyncio.TimeoutError as e:
            end_time = time.time()
            response_time = (end_time - start_time) * 1000
            self._on_failure(e, response_time)
            raise
            
        except self.config.expected_exception as e:
            end_time = time.time()
            response_time = (end_time - start_time) * 1000
            self._on_failure(e, response_time)
            raise
    
    def _should_attempt_reset(self) -> bool:
        """Проверка, стоит ли пытаться сбросить Circuit Breaker"""
        return (time.time() - self._last_failure_time) >= self.config.recovery_timeout
    
    def _on_success(self, response_time: float):
        """Обработка успешного вызова"""
        
        self._metrics['successful_calls'] += 1
        self._metrics['response_times'].append(response_time)
        self._last_success_time = time.time()
        
        if self._state == CircuitState.HALF_OPEN:
            self._success_count += 1
            
            if self._success_count >= self.config.success_threshold:
                self._reset()
        
        elif self._state == CircuitState.CLOSED:
            # Сбрасываем счетчик ошибок при успехе
            self._failure_count = 0
    
    def _on_failure(self, exception: Exception, response_time: float):
        """Обработка неудачного вызова"""
        
        self._metrics['failed_calls'] += 1
        self._metrics['response_times'].append(response_time)
        self._last_failure_time = time.time()
        
        self.logger.warning(f"Call failed in {self.name}: {exception}")
        
        if self._state == CircuitState.HALF_OPEN:
            # В half-open состоянии любая ошибка открывает circuit
            self._trip()
        
        elif self._state == CircuitState.CLOSED:
            self._failure_count += 1
            
            if self._failure_count >= self.config.failure_threshold:
                self._trip()
    
    def _trip(self):
        """Открытие Circuit Breaker"""
        self._state = CircuitState.OPEN
        self._success_count = 0
        self._metrics['circuit_opened_count'] += 1
        
        self.logger.error(f"Circuit breaker {self.name} OPENED after {self._failure_count} failures")
    
    def _reset(self):
        """Закрытие Circuit Breaker"""
        self._state = CircuitState.CLOSED
        self._failure_count = 0
        self._success_count = 0
        self._metrics['circuit_closed_count'] += 1
        
        self.logger.info(f"Circuit breaker {self.name} CLOSED after successful recovery")
    
    def force_open(self):
        """Принудительное открытие Circuit Breaker"""
        self._trip()
        self.logger.warning(f"Circuit breaker {self.name} force opened")
    
    def force_close(self):
        """Принудительное закрытие Circuit Breaker"""
        self._reset()
        self.logger.info(f"Circuit breaker {self.name} force closed")
    
    @property
    def state(self) -> CircuitState:
        """Текущее состояние Circuit Breaker"""
        return self._state
    
    @property
    def failure_rate(self) -> float:
        """Коэффициент ошибок"""
        total = self._metrics['total_calls']
        if total == 0:
            return 0.0
        return (self._metrics['failed_calls'] / total) * 100
    
    def get_metrics(self) -> Dict:
        """Получение метрик Circuit Breaker"""
        
        response_times = self._metrics['response_times']
        
        metrics = {
            'name': self.name,
            'state': self._state.value,
            'config': {
                'failure_threshold': self.config.failure_threshold,
                'recovery_timeout': self.config.recovery_timeout,
                'success_threshold': self.config.success_threshold,
                'timeout': self.config.timeout
            },
            'current_counts': {
                'failure_count': self._failure_count,
                'success_count': self._success_count
            },
            'lifetime_metrics': dict(self._metrics),
            'failure_rate_percent': self.failure_rate,
            'last_failure_ago_seconds': time.time() - self._last_failure_time if self._last_failure_time > 0 else None,
            'last_success_ago_seconds': time.time() - self._last_success_time if self._last_success_time > 0 else None
        }
        
        # Статистика по времени ответа
        if response_times:
            metrics['response_time_stats'] = {
                'avg_ms': statistics.mean(response_times),
                'min_ms': min(response_times),
                'max_ms': max(response_times),
                'p95_ms': statistics.quantiles(response_times, n=20)[18] if len(response_times) > 1 else response_times[0]
            }
        
        return metrics

# Пример использования Circuit Breaker с сетевыми вызовами
class ResilientAPIClient:
    def __init__(self):
        # Создаем Circuit Breakers для разных сервисов
        self.circuit_breakers = {
            'user_service': NetworkCircuitBreaker(
                'user_service',
                CircuitBreakerConfig(
                    failure_threshold=3,
                    recovery_timeout=30.0,
                    timeout=10.0
                )
            ),
            'order_service': NetworkCircuitBreaker(
                'order_service', 
                CircuitBreakerConfig(
                    failure_threshold=5,
                    recovery_timeout=60.0,
                    timeout=15.0
                )
            ),
            'payment_service': NetworkCircuitBreaker(
                'payment_service',
                CircuitBreakerConfig(
                    failure_threshold=2,  # Более чувствительный для критичного сервиса
                    recovery_timeout=120.0,
                    timeout=20.0
                )
            )
        }
        
        # HTTP клиент
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def get_user(self, user_id: int) -> Dict:
        """Получение пользователя через Circuit Breaker"""
        
        async def api_call():
            url = f'https://jsonplaceholder.typicode.com/users/{user_id}'
            async with self.session.get(url) as response:
                if response.status >= 500:
                    raise aiohttp.ClientError(f"Server error: {response.status}")
                return await response.json()
        
        cb = self.circuit_breakers['user_service']
        return await cb(api_call)
    
    async def create_order(self, order_data: Dict) -> Dict:
        """Создание заказа через Circuit Breaker"""
        
        async def api_call():
            url = 'https://jsonplaceholder.typicode.com/posts'
            async with self.session.post(url, json=order_data) as response:
                if
            